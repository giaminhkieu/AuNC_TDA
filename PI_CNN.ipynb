{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giaminhkieu/AuNC_TDA/blob/main/PI_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0CdEEiyG2GJ"
      },
      "source": [
        "# Generating Persistence Images and Simplical complex features to predict AuNC properties\n",
        "\n",
        "1. This notebook focuses on generating persistence images and simplicial complex count and using them to build a Kernel Ridge Regression and Random Forest Regression model to predict the HOMO-LUMO gap, internal energies, and dipole moment of AuNCs.\n",
        "2. Persistence Images were generated using the code in Townsend *et. al.* 2020 publication, publicly available at: https://gitlab.com/voglab/PersistentImages_Chemistry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki4_nk4hFq0F"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXKRJ4SdVsQP",
        "outputId": "6bd72d8a-c542-4b7b-b830-bee7827ad53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gudhi\n",
            "  Downloading gudhi-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from gudhi) (1.23.5)\n",
            "Installing collected packages: gudhi\n",
            "Successfully installed gudhi-3.8.0\n",
            "Collecting ripser\n",
            "  Downloading ripser-0.6.4.tar.gz (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from ripser) (0.29.36)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ripser) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ripser) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ripser) (1.2.2)\n",
            "Collecting persim (from ripser)\n",
            "  Downloading persim-0.3.1-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from persim->ripser) (3.7.1)\n",
            "Collecting hopcroftkarp (from persim->ripser)\n",
            "  Downloading hopcroftkarp-1.2.5.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from persim->ripser)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from persim->ripser) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ripser) (3.2.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->persim->ripser) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->persim->ripser) (1.16.0)\n",
            "Building wheels for collected packages: ripser, hopcroftkarp\n",
            "  Building wheel for ripser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ripser: filename=ripser-0.6.4-cp310-cp310-linux_x86_64.whl size=752978 sha256=c316e2cfaad2d0074a37a702b974119b905c4de95176f8fb7dc9cb2c37ea5620\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/f5/66/f41f708b049057431155934f74e20ca6001a085fcd2e615150\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18103 sha256=a7cec3079b30d592b12cf002653e0b49cccab92db8d54b13fd72310d52630f17\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/0f/3b/0f931844eecc34addd90e72d54cd39c08b7066c5f25c00b9a4\n",
            "Successfully built ripser hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, deprecated, persim, ripser\n",
            "Successfully installed deprecated-1.2.14 hopcroftkarp-1.2.5 persim-0.3.1 ripser-0.6.4\n",
            "Collecting elements\n",
            "  Downloading elements-1.0.0.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from elements) (1.23.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from elements) (2.31.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->elements) (9.4.0)\n",
            "Building wheels for collected packages: elements\n",
            "  Building wheel for elements (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elements: filename=elements-1.0.0-py3-none-any.whl size=21892 sha256=5707a5424e52e7b9db154911a204448f1c74fdee4a77374261a18b8f25513d99\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b2/1d/b417ced7c50363975f73c922903470bb743dfe21f97051353f\n",
            "Successfully built elements\n",
            "Installing collected packages: elements\n",
            "Successfully installed elements-1.0.0\n",
            "Collecting qml\n",
            "  Downloading qml-0.4.0.27.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: qml\n",
            "  Building wheel for qml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qml: filename=qml-0.4.0.27-cp310-cp310-linux_x86_64.whl size=1211678 sha256=1578e910d3a447223f156204ced69156cd725aa4664a22d863c954316812ad2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/57/2f/cdd885bb28ee050be4f41882d74d716e1fc0ff0745a657b39a\n",
            "Successfully built qml\n",
            "Installing collected packages: qml\n",
            "Successfully installed qml-0.4.0.27\n",
            "Collecting dscribe\n",
            "  Downloading dscribe-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.4 (from dscribe)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.10.1)\n",
            "Collecting ase>=3.19.0 (from dscribe)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.3.2)\n",
            "Collecting sparse (from dscribe)\n",
            "  Downloading sparse-0.14.0-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase>=3.19.0->dscribe) (3.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dscribe) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from sparse->dscribe) (0.56.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse->dscribe) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse->dscribe) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.16.0)\n",
            "Installing collected packages: pybind11, sparse, ase, dscribe\n",
            "Successfully installed ase-3.22.1 dscribe-2.0.1 pybind11-2.11.1 sparse-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gudhi\n",
        "!pip install ripser\n",
        "!pip install elements\n",
        "!pip install qml\n",
        "!pip install dscribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i6jvju4aUDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599ebe04-19d2-48d2-f732-0107c2b9dc7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZB4QckCd5bA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/URECA_22')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import qml\n",
        "from qml.representations import *\n",
        "from natsort import natsorted\n",
        "from dscribe.descriptors import SOAP\n",
        "from ase.io import read\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGcP4hneFh24"
      },
      "source": [
        "# Process data and store them to pickle files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "dlkMTF4K1eyZ",
        "outputId": "01689b48-cf0b-4739-c662-cecc689b713b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-4a60f256-b723-423c-85d3-a1da71a3c4c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clusters</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Charge</th>\n",
              "      <th>Ligands1</th>\n",
              "      <th>Type1</th>\n",
              "      <th>Quantity1</th>\n",
              "      <th>Ligands2</th>\n",
              "      <th>Type2</th>\n",
              "      <th>Quantity2</th>\n",
              "      <th>Ligands3</th>\n",
              "      <th>Type3</th>\n",
              "      <th>Quantity3</th>\n",
              "      <th>u298</th>\n",
              "      <th>gap</th>\n",
              "      <th>dipX</th>\n",
              "      <th>dipY</th>\n",
              "      <th>dipZ</th>\n",
              "      <th>mu</th>\n",
              "      <th>core</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Au(PPh3)(CH3)</td>\n",
              "      <td>Au_PPh3_CH3_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>CH3</td>\n",
              "      <td>others</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.400818</td>\n",
              "      <td>1.946276</td>\n",
              "      <td>-0.4391</td>\n",
              "      <td>2.4436</td>\n",
              "      <td>-1.3932</td>\n",
              "      <td>7.236</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Au(SbPh3)4</td>\n",
              "      <td>Au_SbPh3_4_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-198.398732</td>\n",
              "      <td>2.058591</td>\n",
              "      <td>0.5488</td>\n",
              "      <td>0.0287</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>1.469</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Au(SbPh3)Cl</td>\n",
              "      <td>Au_SbPh3_Cl_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>Cl</td>\n",
              "      <td>Halogens</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.829549</td>\n",
              "      <td>3.519847</td>\n",
              "      <td>0.3156</td>\n",
              "      <td>3.6059</td>\n",
              "      <td>-1.8031</td>\n",
              "      <td>10.279</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Au(SbTol3)3(SbCl2Tol2)</td>\n",
              "      <td>Au_SbTol3_3_SbCl2Tol2_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbTol3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>3</td>\n",
              "      <td>SbCl2Tol2</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-226.946163</td>\n",
              "      <td>3.277166</td>\n",
              "      <td>-0.6295</td>\n",
              "      <td>-2.4385</td>\n",
              "      <td>1.0070</td>\n",
              "      <td>6.894</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Au2(PET)(PPh3)2]+</td>\n",
              "      <td>Au2_PET_PPh3_2_c1</td>\n",
              "      <td>1</td>\n",
              "      <td>PET</td>\n",
              "      <td>SX</td>\n",
              "      <td>1</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-130.543475</td>\n",
              "      <td>3.157827</td>\n",
              "      <td>2.3911</td>\n",
              "      <td>0.2794</td>\n",
              "      <td>32.0597</td>\n",
              "      <td>81.717</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>Au103S2(S-Nap)41</td>\n",
              "      <td>Au103_S2_SNap_41_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>S-Nap</td>\n",
              "      <td>SX</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1594.470972</td>\n",
              "      <td>0.324921</td>\n",
              "      <td>-0.1763</td>\n",
              "      <td>-1.6961</td>\n",
              "      <td>-0.2783</td>\n",
              "      <td>4.392</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>Au108S24(PPh3)16</td>\n",
              "      <td>Au108_S24_PPh3_16_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>16</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1302.446940</td>\n",
              "      <td>0.485519</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>-0.0230</td>\n",
              "      <td>-0.0275</td>\n",
              "      <td>0.103</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>[Au110(p-CF3C6H4CC)48]2-</td>\n",
              "      <td>Au110_CCPhCF3_48_c-2</td>\n",
              "      <td>-2</td>\n",
              "      <td>p-CF3C6H4CC</td>\n",
              "      <td>CCR</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-2195.283841</td>\n",
              "      <td>0.153487</td>\n",
              "      <td>-57.1284</td>\n",
              "      <td>-66.4725</td>\n",
              "      <td>-44.5142</td>\n",
              "      <td>249.865</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>Au133(TBBT)52</td>\n",
              "      <td>Au133_TBBT_52_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>TBBT</td>\n",
              "      <td>SX</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-2192.276830</td>\n",
              "      <td>0.068608</td>\n",
              "      <td>0.0956</td>\n",
              "      <td>-1.9440</td>\n",
              "      <td>-0.0162</td>\n",
              "      <td>4.947</td>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>Au144(PET)60</td>\n",
              "      <td>Au144_PET_60_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PET</td>\n",
              "      <td>SX</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1914.637625</td>\n",
              "      <td>0.095082</td>\n",
              "      <td>-0.9166</td>\n",
              "      <td>-0.5340</td>\n",
              "      <td>-0.0097</td>\n",
              "      <td>2.696</td>\n",
              "      <td>144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>212 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a60f256-b723-423c-85d3-a1da71a3c4c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-fe8c437d-19ed-4205-929f-7099b7072852\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe8c437d-19ed-4205-929f-7099b7072852')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-fe8c437d-19ed-4205-929f-7099b7072852 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a60f256-b723-423c-85d3-a1da71a3c4c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a60f256-b723-423c-85d3-a1da71a3c4c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                     Clusters                  Filename  Charge     Ligands1  \\\n",
              "0               Au(PPh3)(CH3)            Au_PPh3_CH3_c0       0         PPh3   \n",
              "1                  Au(SbPh3)4             Au_SbPh3_4_c0       0        SbPh3   \n",
              "2                 Au(SbPh3)Cl            Au_SbPh3_Cl_c0       0        SbPh3   \n",
              "3      Au(SbTol3)3(SbCl2Tol2)  Au_SbTol3_3_SbCl2Tol2_c0       0       SbTol3   \n",
              "4          [Au2(PET)(PPh3)2]+         Au2_PET_PPh3_2_c1       1          PET   \n",
              "..                        ...                       ...     ...          ...   \n",
              "207          Au103S2(S-Nap)41       Au103_S2_SNap_41_c0       0            S   \n",
              "208          Au108S24(PPh3)16      Au108_S24_PPh3_16_c0       0         PPh3   \n",
              "209  [Au110(p-CF3C6H4CC)48]2-      Au110_CCPhCF3_48_c-2      -2  p-CF3C6H4CC   \n",
              "210             Au133(TBBT)52          Au133_TBBT_52_c0       0         TBBT   \n",
              "211              Au144(PET)60           Au144_PET_60_c0       0          PET   \n",
              "\n",
              "    Type1  Quantity1   Ligands2     Type2  Quantity2 Ligands3   Type3  \\\n",
              "0    PPh3          1        CH3    others          1        0  others   \n",
              "1    PPh3          4          0    others          0        0  others   \n",
              "2    PPh3          1         Cl  Halogens          1        0  others   \n",
              "3    PPh3          3  SbCl2Tol2      PPh3          1        0  others   \n",
              "4      SX          1       PPh3      PPh3          2        0  others   \n",
              "..    ...        ...        ...       ...        ...      ...     ...   \n",
              "207     S          2      S-Nap        SX         41        0  others   \n",
              "208  PPh3         16          S         S         24        0  others   \n",
              "209   CCR         48          0    others          0        0  others   \n",
              "210    SX         52          0    others          0        0  others   \n",
              "211    SX         60          0    others          0        0  others   \n",
              "\n",
              "     Quantity3         u298       gap     dipX     dipY     dipZ       mu  \\\n",
              "0            0   -56.400818  1.946276  -0.4391   2.4436  -1.3932    7.236   \n",
              "1            0  -198.398732  2.058591   0.5488   0.0287   0.1793    1.469   \n",
              "2            0   -56.829549  3.519847   0.3156   3.6059  -1.8031   10.279   \n",
              "3            0  -226.946163  3.277166  -0.6295  -2.4385   1.0070    6.894   \n",
              "4            0  -130.543475  3.157827   2.3911   0.2794  32.0597   81.717   \n",
              "..         ...          ...       ...      ...      ...      ...      ...   \n",
              "207          0 -1594.470972  0.324921  -0.1763  -1.6961  -0.2783    4.392   \n",
              "208          0 -1302.446940  0.485519   0.0193  -0.0230  -0.0275    0.103   \n",
              "209          0 -2195.283841  0.153487 -57.1284 -66.4725 -44.5142  249.865   \n",
              "210          0 -2192.276830  0.068608   0.0956  -1.9440  -0.0162    4.947   \n",
              "211          0 -1914.637625  0.095082  -0.9166  -0.5340  -0.0097    2.696   \n",
              "\n",
              "     core  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       2  \n",
              "..    ...  \n",
              "207   103  \n",
              "208   108  \n",
              "209   110  \n",
              "210   133  \n",
              "211   144  \n",
              "\n",
              "[212 rows x 19 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import dataframe\n",
        "os.chdir('/content/drive/MyDrive/AuNC database')\n",
        "DatasetAuNC = pd.read_excel(\"DatasetAuNC_XTB_290723.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3zc5WFR2X8B",
        "outputId": "d2e00d97-7cc7-4161-a022-e72a6bc614ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 212/212 [00:00<00:00, 376.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# Appending xyz\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/AuNC database/Optimized_geometries')\n",
        "\n",
        "xyz_list = []\n",
        "files = []\n",
        "for i in tqdm(range(len(DatasetAuNC))):\n",
        "    try:\n",
        "        xyz_filename = DatasetAuNC[\"Filename\"][i] + \".xyz\"\n",
        "        with open(xyz_filename) as f:\n",
        "            lines = f.readlines()\n",
        "            files.append(xyz_filename)\n",
        "            xyz_list.append(lines)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File '{xyz_filename}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error at index {i}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "DatasetAuNC[\"xyz\"] = xyz_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JQVzTqV_UX9"
      },
      "outputs": [],
      "source": [
        "# Calculating Betti feature count for overview\n",
        "\n",
        "Error = []\n",
        "B0_list = []\n",
        "B1_list = []\n",
        "B2_list = []\n",
        "\n",
        "for i in range(len(DatasetAuNC)):\n",
        "  try:\n",
        "    D, elements = Makexyzdistance(DatasetAuNC[\"xyz\"][i])\n",
        "    persistent_homology_features = ripser(D,distance_matrix=True, maxdim = 2)\n",
        "    B0_list.append(len(persistent_homology_features['dgms'][0])-1)\n",
        "    B1_list.append(len(persistent_homology_features['dgms'][1]))\n",
        "    B2_list.append(len(persistent_homology_features['dgms'][2]))\n",
        "\n",
        "  except:\n",
        "    Error.append(DatasetAuNC[\"Number\"][i])\n",
        "    continue\n",
        "\n",
        "DatasetAuNC[\"Betti0count\"] = B0_list\n",
        "DatasetAuNC[\"Betti1count\"] = B1_list\n",
        "DatasetAuNC[\"Betti2count\"] = B2_list\n",
        "\n",
        "print(\"Finished appending Betti feature count!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1T4Cxhe7QYq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Calculating and appending VariancePersist arrays to pandas dataframe\n",
        "VariancePersistv1 is a modified PI function with added buffer values\n",
        "(0.5, 0.05) for Betti 1 and 2 features.\n",
        "Except block is added due to some xyz file having values written in different notations (e.g. 1e-4)\n",
        "and cannot be read as a float. (current AuNC database does not have this issue)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def append_persistence_image(Dataframe):\n",
        "  Error = []\n",
        "  PersImgArr = []\n",
        "\n",
        "  for idx in range(len(Dataframe)):\n",
        "    try:\n",
        "        persistent_image_matrix = VariancePersistv1(\n",
        "                              Dataframe[\"xyz\"][idx],\n",
        "                              pixelx=resolution,\n",
        "                              pixely=resolution,\n",
        "                              myspread=myspread ,\n",
        "                              myspecs={\"maxBD\": max_bound, \"minBD\":min_bound},\n",
        "                              electroneg_addition=electroneg_addition,\n",
        "                              electroneg_division=electroneg_division,\n",
        "                              B1_buffer=B1_buffer,\n",
        "                              B2_buffer=B2_buffer,\n",
        "                              showplot = False\n",
        "                              )\n",
        "\n",
        "        PersImgArr.append(persistent_image_matrix)\n",
        "    except:\n",
        "        Error.append(idx)\n",
        "        print(\"Error at index: \", idx)\n",
        "        PersImgArr.append(np.zeros(resolution*resolution,))\n",
        "\n",
        "  Dataframe[\"PersImg\"] = PersImgArr\n",
        "  print(\"Finished appending Persistence Images!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Persistence Image Parameters\n",
        "resolution=100\n",
        "myspread=0.3\n",
        "min_bound=-0.3\n",
        "max_bound=7\n",
        "electroneg_addition=+0.4\n",
        "electroneg_division=10\n",
        "B1_buffer=0.5\n",
        "B2_buffer=0.05\n",
        "\n",
        "append_persistence_image(DatasetAuNC)"
      ],
      "metadata": {
        "id": "19TJqyHm6n6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGVd8emwtP1Y"
      },
      "outputs": [],
      "source": [
        "# Simplcial complex analysis\n",
        "def Simplex_analyze(Core_coordinates, bond_length_limit):\n",
        "  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)\n",
        "  simplex_tree = rips.create_simplex_tree(max_dimension=3)\n",
        "  simplex_generator = simplex_tree.get_skeleton(3)\n",
        "\n",
        "  Tetrahedra_list = []\n",
        "  Vertices_of_tetrahedra = []\n",
        "  Overlapping_triangles = []\n",
        "  Unconnected_triangles = []\n",
        "  Triangles_with_1_shared_vertex = []\n",
        "  Triangles_with_2_shared_vertices = []\n",
        "\n",
        "  for simplex in simplex_generator:\n",
        "    if simplex[1] >= bond_length_limit: continue\n",
        "    if len(simplex[0]) == 4: #simplex[0]: list of vertices, simplex[1]: birth filtration\n",
        "      Tetrahedra_list.append(simplex)\n",
        "      for vertex in simplex[0]:\n",
        "        if vertex not in Vertices_of_tetrahedra:\n",
        "          Vertices_of_tetrahedra.append(vertex)\n",
        "\n",
        "  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)\n",
        "  simplex_tree = rips.create_simplex_tree(max_dimension=3)\n",
        "  simplex_generator = simplex_tree.get_skeleton(3)\n",
        "\n",
        "  for simplex in simplex_generator:\n",
        "    Shared_count_arr = []\n",
        "\n",
        "    if simplex[1] >= bond_length_limit: continue\n",
        "    if len(simplex[0]) != 3: continue\n",
        "\n",
        "    if len(Tetrahedra_list) == 0: Unconnected_triangles.append(simplex[0])\n",
        "    else:\n",
        "      for tetrahedra in Tetrahedra_list:\n",
        "        Shared_count_arr.append(len(set(simplex[0]) & set(tetrahedra[0])))\n",
        "\n",
        "      if max(Shared_count_arr) == 3: Overlapping_triangles.append(simplex)\n",
        "      elif max(Shared_count_arr) == 2: Triangles_with_2_shared_vertices.append(simplex)\n",
        "      elif max(Shared_count_arr) == 1: Triangles_with_1_shared_vertex.append(simplex)\n",
        "      else: Unconnected_triangles.append(simplex)\n",
        "\n",
        "  return {\"Tetrahedral_count\": len(Tetrahedra_list), \"Unconnected_triangles_count\": len(Unconnected_triangles),\n",
        "          \"Triangles_with_1_shared_vertex_count\":len(Triangles_with_1_shared_vertex),\n",
        "          \"Triangles_with_2_shared_vertices_count\": len(Triangles_with_2_shared_vertices)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RZhjH8ethJo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Simplex analysis\n",
        "\"\"\"\n",
        "\n",
        "Tetrahedral_count_arr = []\n",
        "Unconnected_triangles_count_arr = []\n",
        "Triangles_with_1_shared_vertex_count_arr = []\n",
        "Triangles_with_2_shared_vertices_count_arr = []\n",
        "files = []\n",
        "\n",
        "for idx in range(len(DatasetAuNC)):\n",
        "  try:\n",
        "    filename = DatasetAuNC[\"Filename\"][idx] + \".xyz\"\n",
        "    df=pd.read_table(filename, delim_whitespace=True, names=['a','b','c','d'],skiprows = 2) # skip the first 2 lines of xyz files\n",
        "    mat = df[['b','c','d']].to_numpy()\n",
        "    ElementArr = df['a'].to_numpy()\n",
        "\n",
        "    Core_coordinates = []\n",
        "\n",
        "    for index in range(len(ElementArr)):\n",
        "      if ElementArr[index] != \"Au\": continue\n",
        "      Core_coordinates.append(mat[index])\n",
        "\n",
        "    Dict = Simplex_analyze(Core_coordinates, bond_length_limit = 4.0)\n",
        "\n",
        "    Tetrahedral_count_arr.append(Dict[\"Tetrahedral_count\"])\n",
        "    Unconnected_triangles_count_arr.append(Dict[\"Unconnected_triangles_count\"])\n",
        "    Triangles_with_1_shared_vertex_count_arr.append(Dict[\"Triangles_with_1_shared_vertex_count\"])\n",
        "    Triangles_with_2_shared_vertices_count_arr.append(Dict[\"Triangles_with_2_shared_vertices_count\"])\n",
        "\n",
        "    print(\"Done: \", i)\n",
        "    print(len(Tetrahedral_count_arr))\n",
        "\n",
        "\n",
        "  except:\n",
        "    print(\"error at: \", i)\n",
        "    continue\n",
        "\n",
        "DatasetAuNC[\"Tetrahedral_count\"] = Tetrahedral_count_arr\n",
        "DatasetAuNC[\"Unconnected_triangles_count\"] = Unconnected_triangles_count_arr\n",
        "DatasetAuNC[\"Triangles_with_1_shared_vertex_count\"] = Triangles_with_1_shared_vertex_count_arr\n",
        "DatasetAuNC[\"Triangles_with_2_shared_vertices_count\"] = Triangles_with_2_shared_vertices_count_arr\n",
        "\n",
        "print(\"Finished appending Simplex count!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculating and appending Coulomb Matrix arrays to pandas dataframe\n",
        "\"\"\"\n",
        "def append_cm(dataframe, xyz_root):\n",
        "  CoulMatArr = []\n",
        "  for f in natsorted(os.listdir(xyz_root)):\n",
        "    try:\n",
        "      mol = qml.Compound(xyz=f)\n",
        "      mol.generate_coulomb_matrix(size=2000, sorting=\"row-norm\")\n",
        "      A = mol.representation\n",
        "      CoulMatArr.append([f,A])\n",
        "    except:\n",
        "      print(\"Error at: \", f)\n",
        "\n",
        "  # Sample array containing the format [\"Filename.xyz\", property array]\n",
        "  property_arrays = np.empty(len(dataframe), dtype=object)\n",
        "\n",
        "  for filename, property_array in CoulMatArr:\n",
        "      filename_without_extension = filename.split('.xyz')[0]\n",
        "      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]\n",
        "\n",
        "      if len(row_index) == 1:\n",
        "          property_arrays[row_index[0]] = property_array\n",
        "      else:\n",
        "          print(f\"Filename '{filename_without_extension}' not found in the DataFrame.\")\n",
        "\n",
        "  dataframe['Coulomb_Matrix'] = property_arrays\n",
        "  print(\"Finished appending Coulomb Matrices!\")\n",
        "\n",
        "append_cm(DatasetAuNC, \"/content/drive/MyDrive/AuNC database/Optimized_geometries\")"
      ],
      "metadata": {
        "id": "qBW1TrJRKmos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculating and appending SOAP arrays to pandas dataframe\n",
        "\"\"\"\n",
        "def append_soap(dataframe, xyz_root):\n",
        "  species = [\"Au\", \"P\", \"S\", \"Sb\", \"Se\", \"Cl\", \"Br\", \"F\", \"C\", \"N\", \"O\", \"H\", \"I\", \"Fe\"]\n",
        "  r_cut = 6.0\n",
        "  n_max = 8\n",
        "  l_max = 6\n",
        "\n",
        "  soap = SOAP(\n",
        "      species=species,\n",
        "      periodic=False,\n",
        "      r_cut=r_cut,\n",
        "      n_max=n_max,\n",
        "      l_max=l_max,\n",
        "  )\n",
        "\n",
        "  SOAPArr = []\n",
        "  for f in natsorted(os.listdir(xyz_root)):\n",
        "    try:\n",
        "      mol = read(f)\n",
        "      a = soap.create(system=mol,centers=[0])\n",
        "      A = a.flatten()\n",
        "      SOAPArr.append([f,A])\n",
        "    except:\n",
        "      print(\"error at: \", f)\n",
        "\n",
        "  property_arrays = np.empty(len(dataframe), dtype=object)\n",
        "\n",
        "  for filename, property_array in SOAPArr:\n",
        "      filename_without_extension = filename.split('.xyz')[0]\n",
        "      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]\n",
        "\n",
        "      if len(row_index) == 1:\n",
        "          property_arrays[row_index[0]] = property_array\n",
        "      else:\n",
        "          print(f\"Filename '{filename_without_extension}' not found in the DataFrame.\")\n",
        "\n",
        "  dataframe['SOAP'] = property_arrays\n",
        "  print(\"Finished appending SOAP!\")\n",
        "\n",
        "append_soap(DatasetAuNC, \"/content/drive/MyDrive/AuNC database/Optimized_geometries\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unxheweybkcO",
        "outputId": "bd22f611-e31f-43e4-b77e-c9d55c70a00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done:  Au2_PET_PPh3_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au2_SbTol3_2_Cl2_c0.xyz shape:  (44296,)\n",
            "Done:  Au3_iylidene_3_c1.xyz shape:  (44296,)\n",
            "Done:  Au3_pylidene_2_PPh3_c1.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh2an_4_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_Br2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_I2_c0.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_I2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_STol_2_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_dppm_3_I2_c0.xyz shape:  (44296,)\n",
            "Done:  Au5_P_PPh3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au5_dppm_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_C_PPh2pyr_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_C_bylidene_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PPh3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PXant_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PhDP_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_S2Tol_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au6_Se2_dppNPh_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_Se2_dppNR4_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_dppp_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au7_PPh3_7_c1.xyz shape:  (44296,)\n",
            "Done:  Au7_dppp_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au8_PPh3_7_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_Pmet3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_S2_dppm_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppf_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCBu_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCCCPh_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCPh_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_SPyH_2_c4.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_SPy_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au9_PNC_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_C4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_D2h_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_Pan3_8_C4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_Pan3_8_D2h_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_R-BINAP_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_S-BINAP_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_dpph_4_db_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_dpph_4_dc_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_PCy2Ph_6_Cl3_c1.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_5_C6F5_4_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_7_S2C2CN2_2_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_8_NCO_Cl_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_R-BINAP_4_CCPhCF3_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR1_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR2_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNTol_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S-BINAP_4_CCPhCF3_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_SC4_10_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_Se4_dppNR4_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_TBBT_10_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_bylidene_4_Br2_c2.xyz shape:  (44296,)\n",
            "Done:  Au11_PDPE_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PMePh2_10_C3v_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_PMePh2_10_D4d_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_PNC_6_PPh3_2_c5.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Br3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_CNPr_2_I_c2.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Cl3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_I3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Spyr_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_p-MBA_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_8_CCPhCF3_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_8_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PPhCF3_7_Cl3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPhF3_7_I3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PXant_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_bylidene_5_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dppe_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_Br2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_I2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_SCN_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppp_5_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dpppen_4_SePh_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au12_SC4_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au13_PMe2Ph_10_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_PPh3_8_p-MBA_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au13_SAdm_8_dppb_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_SbPh3_8_Cl4_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_5_Br2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_5_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_8_CCPhF_4_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_9_Cl3_c2.xyz shape:  (44296,)\n",
            "Done:  Au13_dppe_5_CCPh_2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppe_5_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppm_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppm_6_c5.xyz shape:  (44296,)\n",
            "Done:  Au14_SCy_10_dppb_c0.xyz shape:  (44296,)\n",
            "Done:  Au16_SAdm_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_S2_STipb_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_S8_dppe_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au18_SCy_14_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_dppm_6_Br4_c2.xyz shape:  (44296,)\n",
            "Done:  Au18_dppm_6_Cl4_c4.xyz shape:  (44296,)\n",
            "Done:  Au19_dppNH_3_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au20_PP3_4_c4.xyz shape:  (44296,)\n",
            "Done:  Au20_PPhpy2_10_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au20_PTBu3_8_c0.xyz shape:  (44296,)\n",
            "Done:  Au20_TBBT_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au20_dppm_6_CN_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_SAdm_15_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_SCy_12_dppm_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au21_STBu_15_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_S_SAdm_15_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_CCR_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_CCTBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_SAdm_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_dppo_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_CCTBu_15_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_CCTBu_15_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_PPh3_6_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au23_PPh3_10_dpa_2_Cl_c2.xyz shape:  (44296,)\n",
            "Done:  Au23_SCy_16_c-1.xyz shape:  (44296,)\n",
            "Done:  Au23_bylidene_6_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_PPh3_4_CCPh_14_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_PPh3_10_PET_5_Br2_c1.xyz shape:  (44296,)\n",
            "Done:  Au24_R-dppb_6_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_S-dppb_6_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_SAdm_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SCH2Ph_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SPh_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_STBu_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SePh_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_TBTT_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_CCAr_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-mN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-oN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-pN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_16_SPhBr_2_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c1.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SCy_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SEt_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_SNap_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SPhF_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SPh_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_SPh_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SPr_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SePh_5_PPh3_10_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au25_SePh_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_bylidene_10_Br7_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_bylidene_10_Br8_c1.xyz shape:  (44296,)\n",
            "Done:  Au28_SCy_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au28_TBBT_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au29_SAdm_19_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_SAdm_18_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_SAdm_18_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_STBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_S_STBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PBu3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PEt3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PPh3_8_dpa_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au32_PPr3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au34_DMBT_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au34_SCy_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_CCPh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_DMBT_24_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_DMBT_24_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SCyp_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SPh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_STol_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SePh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_TBBT_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_CCPh_20_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au38_DMBT_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_PET_24_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_PET_24_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_S2_SAdm_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_STol_20_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au40_SAdm_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au40_STol_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au40_dppm_4_CCPh_20_c4.xyz shape:  (44296,)\n",
            "Done:  Au42_SCH2Ph_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_SCy_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_TBBT_26_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_TBBT_26_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au43_SCy_25_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_CCPh_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_DMBT_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_TBBT_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_TBBT_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au48_TBBT_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au49_DMBT_27_c0.xyz shape:  (44296,)\n",
            "Done:  Au52_PET_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au52_TBBT_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au54_PEt3_18_Cl12_c0.xyz shape:  (44296,)\n",
            "Done:  Au56_TBBT_34_c0.xyz shape:  (44296,)\n",
            "Done:  Au60_S6_SCH2Ph_36_c0.xyz shape:  (44296,)\n",
            "Done:  Au60_S7_SCH2Ph_36_c0.xyz shape:  (44296,)\n",
            "Done:  Au67_CCR_32_Cl4_c-3.xyz shape:  (44296,)\n",
            "Done:  Au67_SCH2Ph_35_c0.xyz shape:  (44296,)\n",
            "Done:  Au70_S20_PPh3_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au92_TBBT_44_c0.xyz shape:  (44296,)\n",
            "Done:  Au103_S2_SNap_41_c0.xyz shape:  (44296,)\n",
            "Done:  Au108_S24_PPh3_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au110_CCPhCF3_48_c-2.xyz shape:  (44296,)\n",
            "Done:  Au133_TBBT_52_c0.xyz shape:  (44296,)\n",
            "Done:  Au144_PET_60_c0.xyz shape:  (44296,)\n",
            "Done:  Au_PPh3_CH3_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbPh3_4_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbPh3_Cl_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbTol3_3_SbCl2Tol2_c0.xyz shape:  (44296,)\n",
            "error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "rziN2BWXwncJ",
        "outputId": "e73606a3-8ee4-4f48-eab7-844893386e03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-740eec0d-951d-42cf-8050-a56b8961525c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>names</th>\n",
              "      <th>imag+zpve</th>\n",
              "      <th>u298</th>\n",
              "      <th>h298</th>\n",
              "      <th>g298</th>\n",
              "      <th>gap</th>\n",
              "      <th>dipX</th>\n",
              "      <th>dipY</th>\n",
              "      <th>dipZ</th>\n",
              "      <th>dipTotal</th>\n",
              "      <th>core</th>\n",
              "      <th>Tetrahedral_count</th>\n",
              "      <th>Unconnected_triangles_count</th>\n",
              "      <th>Triangles_with_1_shared_vertex_count</th>\n",
              "      <th>Triangles_with_2_shared_vertices_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AuTM-1.xyz.1315600.hpc-mn1.out</td>\n",
              "      <td>-4.54      9.35       0.361254792113</td>\n",
              "      <td>-126.516193</td>\n",
              "      <td>-126.114631</td>\n",
              "      <td>-126.226368</td>\n",
              "      <td>1.947794</td>\n",
              "      <td>-1.8152</td>\n",
              "      <td>2.4689</td>\n",
              "      <td>-3.5907</td>\n",
              "      <td>11.998</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AuTM-2.xyz.1315601.hpc-mn1.out</td>\n",
              "      <td>5.26       5.70       0.300816191820</td>\n",
              "      <td>-75.059447</td>\n",
              "      <td>-74.731351</td>\n",
              "      <td>-74.816078</td>\n",
              "      <td>0.536785</td>\n",
              "      <td>1.7070</td>\n",
              "      <td>-12.6958</td>\n",
              "      <td>-6.9454</td>\n",
              "      <td>37.038</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AuTM-3.xyz.1315602.hpc-mn1.out</td>\n",
              "      <td>5.72       7.27       0.414537168724</td>\n",
              "      <td>-220.342502</td>\n",
              "      <td>-219.860514</td>\n",
              "      <td>-220.026097</td>\n",
              "      <td>1.281813</td>\n",
              "      <td>1.0570</td>\n",
              "      <td>0.0231</td>\n",
              "      <td>-1.4388</td>\n",
              "      <td>4.538</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AuTM-4.xyz.1315603.hpc-mn1.out</td>\n",
              "      <td>11.56      27.95      0.330145801891</td>\n",
              "      <td>-78.203268</td>\n",
              "      <td>-77.844378</td>\n",
              "      <td>-77.931539</td>\n",
              "      <td>2.252962</td>\n",
              "      <td>6.8291</td>\n",
              "      <td>3.2187</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>19.189</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AuTM-5.xyz.1315604.hpc-mn1.out</td>\n",
              "      <td>1.77       38.84      0.244731836526</td>\n",
              "      <td>-32.384417</td>\n",
              "      <td>-32.124641</td>\n",
              "      <td>-32.182588</td>\n",
              "      <td>4.939549</td>\n",
              "      <td>1.6536</td>\n",
              "      <td>9.0627</td>\n",
              "      <td>20.8189</td>\n",
              "      <td>57.866</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3154</th>\n",
              "      <td>AuTM-3155.xyz.1323291.hpc-mn1.out</td>\n",
              "      <td>14.72      21.95      0.405623835398</td>\n",
              "      <td>-85.628117</td>\n",
              "      <td>-85.190559</td>\n",
              "      <td>-85.282873</td>\n",
              "      <td>2.599206</td>\n",
              "      <td>3.8788</td>\n",
              "      <td>3.9315</td>\n",
              "      <td>1.8642</td>\n",
              "      <td>14.816</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3155</th>\n",
              "      <td>AuTM-3156.xyz.1323292.hpc-mn1.out</td>\n",
              "      <td>7.50       11.77      0.300199763344</td>\n",
              "      <td>-56.400818</td>\n",
              "      <td>-56.077525</td>\n",
              "      <td>-56.153166</td>\n",
              "      <td>1.946276</td>\n",
              "      <td>-0.4391</td>\n",
              "      <td>2.4436</td>\n",
              "      <td>-1.3932</td>\n",
              "      <td>7.236</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3156</th>\n",
              "      <td>AuTM-3157.xyz.1323293.hpc-mn1.out</td>\n",
              "      <td>9.52       16.36      0.262733823975</td>\n",
              "      <td>-56.829549</td>\n",
              "      <td>-56.543338</td>\n",
              "      <td>-56.620601</td>\n",
              "      <td>3.519847</td>\n",
              "      <td>0.2095</td>\n",
              "      <td>3.4129</td>\n",
              "      <td>-1.9723</td>\n",
              "      <td>10.033</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3157</th>\n",
              "      <td>AuTM-3158.xyz.1323294.hpc-mn1.out</td>\n",
              "      <td>7.27       8.38       1.048572866833</td>\n",
              "      <td>-198.407509</td>\n",
              "      <td>-197.277608</td>\n",
              "      <td>-197.473715</td>\n",
              "      <td>1.877934</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>-0.1856</td>\n",
              "      <td>-1.2711</td>\n",
              "      <td>3.680</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3158</th>\n",
              "      <td>AuTM-3159.xyz.1323295.hpc-mn1.out</td>\n",
              "      <td>4.90       5.90       1.261562859735</td>\n",
              "      <td>-226.946163</td>\n",
              "      <td>-225.583462</td>\n",
              "      <td>-225.823290</td>\n",
              "      <td>3.277171</td>\n",
              "      <td>-0.6295</td>\n",
              "      <td>-2.4385</td>\n",
              "      <td>1.0070</td>\n",
              "      <td>6.894</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3159 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740eec0d-951d-42cf-8050-a56b8961525c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-740eec0d-951d-42cf-8050-a56b8961525c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-740eec0d-951d-42cf-8050-a56b8961525c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   names  \\\n",
              "0        AuTM-1.xyz.1315600.hpc-mn1.out    \n",
              "1        AuTM-2.xyz.1315601.hpc-mn1.out    \n",
              "2        AuTM-3.xyz.1315602.hpc-mn1.out    \n",
              "3        AuTM-4.xyz.1315603.hpc-mn1.out    \n",
              "4        AuTM-5.xyz.1315604.hpc-mn1.out    \n",
              "...                                  ...   \n",
              "3154  AuTM-3155.xyz.1323291.hpc-mn1.out    \n",
              "3155  AuTM-3156.xyz.1323292.hpc-mn1.out    \n",
              "3156  AuTM-3157.xyz.1323293.hpc-mn1.out    \n",
              "3157  AuTM-3158.xyz.1323294.hpc-mn1.out    \n",
              "3158  AuTM-3159.xyz.1323295.hpc-mn1.out    \n",
              "\n",
              "                                   imag+zpve        u298        h298  \\\n",
              "0      -4.54      9.35       0.361254792113  -126.516193 -126.114631   \n",
              "1      5.26       5.70       0.300816191820   -75.059447  -74.731351   \n",
              "2      5.72       7.27       0.414537168724  -220.342502 -219.860514   \n",
              "3      11.56      27.95      0.330145801891   -78.203268  -77.844378   \n",
              "4      1.77       38.84      0.244731836526   -32.384417  -32.124641   \n",
              "...                                      ...         ...         ...   \n",
              "3154   14.72      21.95      0.405623835398   -85.628117  -85.190559   \n",
              "3155   7.50       11.77      0.300199763344   -56.400818  -56.077525   \n",
              "3156   9.52       16.36      0.262733823975   -56.829549  -56.543338   \n",
              "3157   7.27       8.38       1.048572866833  -198.407509 -197.277608   \n",
              "3158   4.90       5.90       1.261562859735  -226.946163 -225.583462   \n",
              "\n",
              "            g298       gap    dipX     dipY     dipZ  dipTotal  core  \\\n",
              "0    -126.226368  1.947794 -1.8152   2.4689  -3.5907    11.998     1   \n",
              "1     -74.816078  0.536785  1.7070 -12.6958  -6.9454    37.038     1   \n",
              "2    -220.026097  1.281813  1.0570   0.0231  -1.4388     4.538     1   \n",
              "3     -77.931539  2.252962  6.8291   3.2187   0.0003    19.189     1   \n",
              "4     -32.182588  4.939549  1.6536   9.0627  20.8189    57.866     1   \n",
              "...          ...       ...     ...      ...      ...       ...   ...   \n",
              "3154  -85.282873  2.599206  3.8788   3.9315   1.8642    14.816     1   \n",
              "3155  -56.153166  1.946276 -0.4391   2.4436  -1.3932     7.236     1   \n",
              "3156  -56.620601  3.519847  0.2095   3.4129  -1.9723    10.033     1   \n",
              "3157 -197.473715  1.877934  0.6674  -0.1856  -1.2711     3.680     1   \n",
              "3158 -225.823290  3.277171 -0.6295  -2.4385   1.0070     6.894     1   \n",
              "\n",
              "      Tetrahedral_count  Unconnected_triangles_count  \\\n",
              "0                     0                            0   \n",
              "1                     0                            0   \n",
              "2                     0                            0   \n",
              "3                     0                            0   \n",
              "4                     0                            0   \n",
              "...                 ...                          ...   \n",
              "3154                  0                            0   \n",
              "3155                  0                            0   \n",
              "3156                  0                            0   \n",
              "3157                  0                            0   \n",
              "3158                  0                            0   \n",
              "\n",
              "      Triangles_with_1_shared_vertex_count  \\\n",
              "0                                        0   \n",
              "1                                        0   \n",
              "2                                        0   \n",
              "3                                        0   \n",
              "4                                        0   \n",
              "...                                    ...   \n",
              "3154                                     0   \n",
              "3155                                     0   \n",
              "3156                                     0   \n",
              "3157                                     0   \n",
              "3158                                     0   \n",
              "\n",
              "      Triangles_with_2_shared_vertices_count  \n",
              "0                                          0  \n",
              "1                                          0  \n",
              "2                                          0  \n",
              "3                                          0  \n",
              "4                                          0  \n",
              "...                                      ...  \n",
              "3154                                       0  \n",
              "3155                                       0  \n",
              "3156                                       0  \n",
              "3157                                       0  \n",
              "3158                                       0  \n",
              "\n",
              "[3159 rows x 15 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "DatabaseTM16Jan includes the XTB calculated properties for the AuTM molecules, along with their simplex counts already calculated at 4.0 filter via the function above\n",
        "\"\"\"\n",
        "\n",
        "DatasetTM = pd.read_csv(\"DatabaseTM16Jan.csv\",\n",
        "                  sep=',',\n",
        "                  header = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R5PX6CcoOZp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Appending xyz texts of AuTMs to dataframe\n",
        "\"\"\"\n",
        "os.chdir(\"/content/drive/MyDrive/TM_dataset\")\n",
        "xyz_list = []\n",
        "files = []\n",
        "for i in range(len(DatasetTM)):\n",
        "  print(\"Datapoint: \",i)\n",
        "  filename = \"AuTM-\" + str(i+1) + \".xyz\"\n",
        "  with open(filename) as f:\n",
        "      lines = f.readlines()\n",
        "      files.append(filename)\n",
        "      xyz_list.append(lines)\n",
        "\n",
        "DatasetTM[\"xyz\"]=xyz_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS-kt-1FvwCD"
      },
      "outputs": [],
      "source": [
        "#Persistence Image Parameters\n",
        "resolution=100\n",
        "myspread=0.3\n",
        "min_bound=-0.3\n",
        "max_bound=7\n",
        "electroneg_addition=+0.4\n",
        "electroneg_division=10\n",
        "B1_buffer=0.5\n",
        "B2_buffer=0.05\n",
        "\n",
        "append_persistence_image(DatasetAuTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA8sidF0mGw7"
      },
      "source": [
        "# Read from pre-calculated dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgGD26OnmTv2"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database\")\n",
        "DatasetAuNC = pd.read_pickle('DatasetAuNC_290723_withCM_SOAP.pkl')\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/URECA_22\")\n",
        "DatasetTM = pd.read_pickle('DatasetTM100.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J82C1fy_PFD"
      },
      "source": [
        "## Result plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcVVD96c6KT6"
      },
      "outputs": [],
      "source": [
        "def analyze_regression_performance(true_values, predicted_values, parameters, duration, text_position='top_left', save_image=False, image_name=\"regression_performance.png\"):\n",
        "    # Calculate evaluation metrics\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    # Create a blank figure for text-only display\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.axis('off')  # Turn off axis for text display only\n",
        "\n",
        "    # Determine text position\n",
        "    if text_position == 'top_left':\n",
        "        text_x, text_y = 0.02, 0.98\n",
        "        ha, va = 'left', 'top'\n",
        "    elif text_position == 'top_right':\n",
        "        text_x, text_y = 0.98, 0.98\n",
        "        ha, va = 'right', 'top'\n",
        "    else:\n",
        "        raise ValueError(\"Invalid text_position. Choose 'top_left' or 'top_right'.\")\n",
        "\n",
        "    # Add text with model parameters and evaluation metrics\n",
        "    parameters_text = '\\n'.join(f'{key}: {value}' for key, value in parameters.items())\n",
        "    metrics_text = f\"Mean Absolute Error (MAE): {mae:.4f}\\n\" \\\n",
        "                   f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\" \\\n",
        "                   f\"R-squared (R²): {r2:.4f}\\n\" \\\n",
        "                   f\"Training duration (s): {duration:.1f}\\n\"\n",
        "    text = f\"{parameters_text}\\n\\n{metrics_text}\"\n",
        "    ax.text(text_x, text_y, text, transform=ax.transAxes,\n",
        "            bbox=dict(facecolor='white', edgecolor='black', alpha=0.8),\n",
        "            horizontalalignment=ha, verticalalignment=va)\n",
        "\n",
        "    if save_image:\n",
        "        plt.savefig(image_name, bbox_inches='tight')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFoUzP3B6LnN"
      },
      "outputs": [],
      "source": [
        "def plot_ML_results(true_values, predicted_values, x_tick_sep=1.0, y_tick_sep=1.0, dpi=900, save_image=False, image_name=\"plot_ML_results.png\"):\n",
        "    \"\"\"\n",
        "    Plot the true values against predicted values for machine learning model evaluation.\n",
        "\n",
        "    Parameters:\n",
        "        true_values (array-like): True target values.\n",
        "        predicted_values (array-like): Predicted target values.\n",
        "        x_tick_sep (float): Separation between x-axis tick marks. Default is 1.0.\n",
        "        y_tick_sep (float): Separation between y-axis tick marks. Default is 1.0.\n",
        "        dpi (int): Dots per inch for the image resolution. Default is 900.\n",
        "        save_image (bool): If True, save the image. Default is False.\n",
        "        image_name (str): Name of the saved image file. Default is \"plot_ML_results.png\".\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(true_values, predicted_values, c='teal', s=40)\n",
        "\n",
        "    max_value = max(max(predicted_values), max(true_values))\n",
        "    min_value = min(min(predicted_values), min(true_values))\n",
        "\n",
        "    # Ensure the axes always start at 0.0\n",
        "    max_value = max(max_value, 0.0)\n",
        "    min_value = min(min_value, 0.0)\n",
        "\n",
        "    plt.plot([min_value, max_value], [min_value, max_value], 'black')\n",
        "\n",
        "    plt.xlabel('Calculated HOMO-LUMO gap (eV)', fontsize=25)\n",
        "    plt.ylabel('Predicted HOMO-LUMO gap (eV)', fontsize=25)\n",
        "\n",
        "    plt.tick_params(axis='both', labelsize=20, pad=8)\n",
        "    x_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, x_tick_sep)\n",
        "    y_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, y_tick_sep)\n",
        "\n",
        "    plt.xticks(x_ticks)\n",
        "    plt.yticks(y_ticks)\n",
        "\n",
        "    plt.axis('equal')\n",
        "\n",
        "    if save_image:\n",
        "        plt.savefig(image_name, dpi=dpi)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have 'true_values' and 'predicted_values' arrays containing the data.\n",
        "# plot_ML_results(true_values, predicted_values, x_tick_sep=0.5, y_tick_sep=0.5, dpi=900, save_image=True, image_name=\"my_plot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnucUzOaGOMG"
      },
      "source": [
        "## Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(use_simplex=True, use_charge=True):\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Define the input layers\n",
        "    input_1 = Input(shape=(5,))\n",
        "    input_2 = Input(shape=(100, 100, 1))\n",
        "    input_3 = Input(shape=(1,))\n",
        "\n",
        "    # Model 1 - Simplexes\n",
        "    if use_simplex:\n",
        "        dense_1 = Dense(5, activation='relu')(input_1)\n",
        "        dense_1_extra = Dense(3, activation='relu')(dense_1)\n",
        "        input_concat = [dense_1_extra]\n",
        "    else:\n",
        "        input_concat = []\n",
        "\n",
        "    # Model 2 - Convolutional Neural Network\n",
        "    conv_1 = Conv2D(16, (3, 3), activation='relu')(input_2)\n",
        "    maxpool_1 = MaxPooling2D((2, 2))(conv_1)\n",
        "    conv_2 = Conv2D(16, (3, 3), activation='relu')(maxpool_1)\n",
        "    maxpool_2 = MaxPooling2D((2, 2))(conv_2)\n",
        "    flatten = Flatten()(maxpool_2)\n",
        "    dense_2 = Dense(32, activation='relu')(flatten)\n",
        "    dense_3 = Dense(16, activation='relu')(dense_2)\n",
        "    input_concat.append(dense_3)\n",
        "\n",
        "    # Model 3 - Charge\n",
        "    if use_charge:\n",
        "        input_concat.append(input_3)\n",
        "\n",
        "    # Concatenate input layers based on whether simplex and/or charge are used\n",
        "    if len(input_concat) > 1:\n",
        "        x = Concatenate()(input_concat)\n",
        "    else:\n",
        "        x = input_concat[0]\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    output = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_cnn_leave_one_out(Dataset, sample_size, target_variable, batch_size, learning_rate, patience, model_save_name, use_simplex=True, use_charge=True, random_state=None):\n",
        "    random.seed(random_state)\n",
        "    np.random.seed(random_state)\n",
        "    tf.random.set_seed(random_state)\n",
        "\n",
        "    # Initialize the lists for input data and target variable\n",
        "    x1, x2, x3, y = [], [], [], []\n",
        "\n",
        "    # Append data to the lists\n",
        "    for i in range(len(Dataset)):\n",
        "        x1.append(np.asarray([Dataset[\"core\"][i],\n",
        "                              Dataset[\"Tetrahedral_count\"][i],\n",
        "                              Dataset[\"Unconnected_triangles_count\"][i],\n",
        "                              Dataset[\"Triangles_with_1_shared_vertex_count\"][i],\n",
        "                              Dataset[\"Triangles_with_2_shared_vertices_count\"][i]]))\n",
        "\n",
        "        x2.append(np.asarray(Dataset[\"PersImg\"][i]).reshape(100, 100, 1))\n",
        "        x3.append([Dataset[\"Charge\"][i]])\n",
        "        y.append(float(Dataset[\"gap\"][i]))\n",
        "\n",
        "    y = np.array(y)\n",
        "    x1 = np.array(x1)\n",
        "    x2 = np.array(x2)\n",
        "    x3 = np.array(x3)\n",
        "\n",
        "    predicted_arr = []\n",
        "    true_arr = []\n",
        "    MAE_arr = []\n",
        "    RMSE_arr = []\n",
        "    TotalAccuracy = 0\n",
        "    TotalError = 0\n",
        "    MSE = 0\n",
        "\n",
        "    x1_train_full = x1.tolist()\n",
        "    x2_train_full = x2.tolist()\n",
        "    x3_train_full = x3.tolist()\n",
        "    y_train_full = y.tolist()\n",
        "\n",
        "    for test_index in range(len(x1)):\n",
        "        print(\"Cycle: \", test_index)\n",
        "\n",
        "        x1_train = deepcopy(x1_train_full)\n",
        "        x2_train = deepcopy(x2_train_full)\n",
        "        x3_train = deepcopy(x3_train_full)\n",
        "        y_train = deepcopy(y_train_full)\n",
        "\n",
        "        x1_test = x1[test_index]\n",
        "        x2_test = x2[test_index]\n",
        "        x3_test = x3[test_index]\n",
        "        y_test = y[test_index]\n",
        "\n",
        "        x1_train.pop(test_index)\n",
        "        x2_train.pop(test_index)\n",
        "        x3_train.pop(test_index)\n",
        "        y_train.pop(test_index)\n",
        "\n",
        "        x1_train = np.asarray(x1_train)\n",
        "        x2_train = np.asarray(x2_train)\n",
        "        x3_train = np.asarray(x3_train)\n",
        "        y_train = np.asarray(y_train)\n",
        "\n",
        "\n",
        "        # Split training data into train and validation sets\n",
        "        x1_train, x1_val, x2_train, x2_val, x3_train, x3_val, y_train, y_val = train_test_split(\n",
        "            x1_train, x2_train, x3_train, y_train, test_size=10, random_state=random_state)\n",
        "\n",
        "        print(\"start compiling\")\n",
        "\n",
        "        model = create_model(use_simplex=use_simplex, use_charge=use_charge)\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='mean_absolute_error',\n",
        "                      metrics=['mean_squared_error'])\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "        model_checkpoint = ModelCheckpoint(filepath=model_save_name,\n",
        "                                           save_best_only=True,\n",
        "                                           save_weights_only=False,\n",
        "                                           monitor='val_loss',\n",
        "                                           mode='min',\n",
        "                                           verbose=1)\n",
        "\n",
        "        start_time = time.time()  # Start the timer\n",
        "        print(\"start fitting\")\n",
        "        model.fit([x1_train, x2_train, x3_train],\n",
        "                  y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=500,\n",
        "                  verbose=1,\n",
        "                  validation_data=([x1_val, x2_val, x3_val], y_val),\n",
        "                  callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        end_time = time.time()  # Stop the timer\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        model = load_model(model_save_name)\n",
        "\n",
        "        y_pred = model.predict([np.asarray([x1_test]), np.asarray([x2_test]), np.asarray([x3_test])])\n",
        "        predicted_arr.append(y_pred)\n",
        "        true_arr.append(float(y_test))\n",
        "\n",
        "    return predicted_arr, true_arr, duration\n"
      ],
      "metadata": {
        "id": "HLN20pVS9pGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database/fig4_plots\")\n",
        "\n",
        "target_variable = \"gap\"\n",
        "batch_size = 24\n",
        "learning_rate=0.001\n",
        "patience = 40\n",
        "Filename = \"CNN\" + \"_\" + target_variable\n",
        "seed = 42\n",
        "model_save_name = \"best_cnn_gap.h5\"\n",
        "if target_variable == \"u298\": tick_sep=150\n",
        "elif target_variable == \"gap\": tick_sep=0.5\n",
        "else: tick_sep=50\n",
        "\n",
        "parameters = {'target_variable': target_variable, 'batch_size': batch_size, \"lr\": learning_rate, \"seed\": seed}\n",
        "\n",
        "predicted_values, true_values, duration = run_cnn_leave_one_out(Dataset=DatasetAuNC,\n",
        "                                                                sample_size=len(DatasetAuNC),\n",
        "                                                                target_variable=target_variable,\n",
        "                                                                batch_size=batch_size, learning_rate=learning_rate, patience=patience,\n",
        "                                                                model_save_name=model_save_name,\n",
        "                                                                random_state=seed,\n",
        "                                                                use_simplex=True, use_charge=True,\n",
        "                                                                )\n",
        "analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=False, image_name=Filename+\"_data.png\")\n",
        "plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=False, image_name=Filename+\".pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RWZJkXicAMIr",
        "outputId": "ec846ea9-fdf5-44ea-cd53-c2ec4ba83d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Start fitting - Fold: 3\n",
            "Epoch 1/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.2238 - mean_squared_error: 1.8551 \n",
            "Epoch 1: val_loss improved from inf to 1.11833, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 4s 90ms/step - loss: 1.2238 - mean_squared_error: 1.8551 - val_loss: 1.1183 - val_mean_squared_error: 1.6029\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1027 - mean_squared_error: 1.5719\n",
            "Epoch 2: val_loss improved from 1.11833 to 1.00059, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.1027 - mean_squared_error: 1.5719 - val_loss: 1.0006 - val_mean_squared_error: 1.3662\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.9837 - mean_squared_error: 1.3106\n",
            "Epoch 3: val_loss improved from 1.00059 to 0.87880, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.9837 - mean_squared_error: 1.3106 - val_loss: 0.8788 - val_mean_squared_error: 1.1617\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.8716 - mean_squared_error: 1.0879\n",
            "Epoch 4: val_loss improved from 0.87880 to 0.78122, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.8716 - mean_squared_error: 1.0879 - val_loss: 0.7812 - val_mean_squared_error: 0.9970\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7758 - mean_squared_error: 0.9162\n",
            "Epoch 5: val_loss improved from 0.78122 to 0.69373, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.7758 - mean_squared_error: 0.9162 - val_loss: 0.6937 - val_mean_squared_error: 0.8712\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7037 - mean_squared_error: 0.7954\n",
            "Epoch 6: val_loss improved from 0.69373 to 0.63552, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.7037 - mean_squared_error: 0.7954 - val_loss: 0.6355 - val_mean_squared_error: 0.7798\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6599 - mean_squared_error: 0.7088\n",
            "Epoch 7: val_loss improved from 0.63552 to 0.59576, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.6599 - mean_squared_error: 0.7088 - val_loss: 0.5958 - val_mean_squared_error: 0.7181\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6275 - mean_squared_error: 0.6503\n",
            "Epoch 8: val_loss improved from 0.59576 to 0.56069, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.6275 - mean_squared_error: 0.6503 - val_loss: 0.5607 - val_mean_squared_error: 0.6677\n",
            "Epoch 9/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6243 - mean_squared_error: 0.6497\n",
            "Epoch 9: val_loss improved from 0.56069 to 0.52589, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.6001 - mean_squared_error: 0.6052 - val_loss: 0.5259 - val_mean_squared_error: 0.6208\n",
            "Epoch 10/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5870 - mean_squared_error: 0.5725\n",
            "Epoch 10: val_loss improved from 0.52589 to 0.49668, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.5739 - mean_squared_error: 0.5629 - val_loss: 0.4967 - val_mean_squared_error: 0.5770\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5479 - mean_squared_error: 0.5233\n",
            "Epoch 11: val_loss improved from 0.49668 to 0.46939, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.5479 - mean_squared_error: 0.5233 - val_loss: 0.4694 - val_mean_squared_error: 0.5380\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5237 - mean_squared_error: 0.4732\n",
            "Epoch 12: val_loss improved from 0.46939 to 0.44279, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.5267 - mean_squared_error: 0.4920 - val_loss: 0.4428 - val_mean_squared_error: 0.5032\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5055 - mean_squared_error: 0.4648\n",
            "Epoch 13: val_loss improved from 0.44279 to 0.42472, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.5055 - mean_squared_error: 0.4648 - val_loss: 0.4247 - val_mean_squared_error: 0.4726\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4887 - mean_squared_error: 0.4432\n",
            "Epoch 14: val_loss improved from 0.42472 to 0.41374, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4887 - mean_squared_error: 0.4432 - val_loss: 0.4137 - val_mean_squared_error: 0.4485\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4768 - mean_squared_error: 0.4259\n",
            "Epoch 15: val_loss improved from 0.41374 to 0.40370, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.4768 - mean_squared_error: 0.4259 - val_loss: 0.4037 - val_mean_squared_error: 0.4291\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4687 - mean_squared_error: 0.4116\n",
            "Epoch 16: val_loss improved from 0.40370 to 0.39282, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4687 - mean_squared_error: 0.4116 - val_loss: 0.3928 - val_mean_squared_error: 0.4136\n",
            "Epoch 17/500\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.4840 - mean_squared_error: 0.4245\n",
            "Epoch 17: val_loss improved from 0.39282 to 0.38613, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4597 - mean_squared_error: 0.3993 - val_loss: 0.3861 - val_mean_squared_error: 0.4027\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4546 - mean_squared_error: 0.3929\n",
            "Epoch 18: val_loss improved from 0.38613 to 0.38356, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4546 - mean_squared_error: 0.3929 - val_loss: 0.3836 - val_mean_squared_error: 0.3942\n",
            "Epoch 19/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4549 - mean_squared_error: 0.3933\n",
            "Epoch 19: val_loss improved from 0.38356 to 0.38032, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4531 - mean_squared_error: 0.3912 - val_loss: 0.3803 - val_mean_squared_error: 0.3862\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4501 - mean_squared_error: 0.3861\n",
            "Epoch 20: val_loss improved from 0.38032 to 0.37867, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.4501 - mean_squared_error: 0.3861 - val_loss: 0.3787 - val_mean_squared_error: 0.3823\n",
            "Epoch 21/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4435 - mean_squared_error: 0.3680\n",
            "Epoch 21: val_loss improved from 0.37867 to 0.37699, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.4474 - mean_squared_error: 0.3841 - val_loss: 0.3770 - val_mean_squared_error: 0.3807\n",
            "Epoch 22/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4619 - mean_squared_error: 0.3935\n",
            "Epoch 22: val_loss improved from 0.37699 to 0.37586, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.4463 - mean_squared_error: 0.3835 - val_loss: 0.3759 - val_mean_squared_error: 0.3790\n",
            "Epoch 23/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4413 - mean_squared_error: 0.3822\n",
            "Epoch 23: val_loss did not improve from 0.37586\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4452 - mean_squared_error: 0.3793 - val_loss: 0.3762 - val_mean_squared_error: 0.3787\n",
            "Epoch 24/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4488 - mean_squared_error: 0.3590\n",
            "Epoch 24: val_loss improved from 0.37586 to 0.37435, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.4442 - mean_squared_error: 0.3768 - val_loss: 0.3743 - val_mean_squared_error: 0.3774\n",
            "Epoch 25/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4307 - mean_squared_error: 0.3483\n",
            "Epoch 25: val_loss improved from 0.37435 to 0.37275, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.4422 - mean_squared_error: 0.3761 - val_loss: 0.3728 - val_mean_squared_error: 0.3758\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4424 - mean_squared_error: 0.3855\n",
            "Epoch 26: val_loss did not improve from 0.37275\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4406 - mean_squared_error: 0.3732 - val_loss: 0.3732 - val_mean_squared_error: 0.3744\n",
            "Epoch 27/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4442 - mean_squared_error: 0.3857\n",
            "Epoch 27: val_loss did not improve from 0.37275\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4393 - mean_squared_error: 0.3694 - val_loss: 0.3731 - val_mean_squared_error: 0.3724\n",
            "Epoch 28/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4762 - mean_squared_error: 0.4262\n",
            "Epoch 28: val_loss did not improve from 0.37275\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4398 - mean_squared_error: 0.3668 - val_loss: 0.3737 - val_mean_squared_error: 0.3715\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4572 - mean_squared_error: 0.4131\n",
            "Epoch 29: val_loss improved from 0.37275 to 0.37023, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.4366 - mean_squared_error: 0.3655 - val_loss: 0.3702 - val_mean_squared_error: 0.3691\n",
            "Epoch 30/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4444 - mean_squared_error: 0.3881\n",
            "Epoch 30: val_loss did not improve from 0.37023\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4363 - mean_squared_error: 0.3654 - val_loss: 0.3715 - val_mean_squared_error: 0.3694\n",
            "Epoch 31/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4333 - mean_squared_error: 0.3800\n",
            "Epoch 31: val_loss did not improve from 0.37023\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4352 - mean_squared_error: 0.3625 - val_loss: 0.3703 - val_mean_squared_error: 0.3690\n",
            "Epoch 32/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4060 - mean_squared_error: 0.2981\n",
            "Epoch 32: val_loss did not improve from 0.37023\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4346 - mean_squared_error: 0.3608 - val_loss: 0.3728 - val_mean_squared_error: 0.3708\n",
            "Epoch 33/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4184 - mean_squared_error: 0.3642\n",
            "Epoch 33: val_loss did not improve from 0.37023\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4335 - mean_squared_error: 0.3586 - val_loss: 0.3706 - val_mean_squared_error: 0.3681\n",
            "Epoch 34/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4031 - mean_squared_error: 0.2946\n",
            "Epoch 34: val_loss improved from 0.37023 to 0.36871, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.4333 - mean_squared_error: 0.3608 - val_loss: 0.3687 - val_mean_squared_error: 0.3672\n",
            "Epoch 35/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4316 - mean_squared_error: 0.3602\n",
            "Epoch 35: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4325 - mean_squared_error: 0.3595 - val_loss: 0.3707 - val_mean_squared_error: 0.3681\n",
            "Epoch 36/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4252 - mean_squared_error: 0.3615\n",
            "Epoch 36: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4313 - mean_squared_error: 0.3589 - val_loss: 0.3694 - val_mean_squared_error: 0.3673\n",
            "Epoch 37/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4471 - mean_squared_error: 0.3813\n",
            "Epoch 37: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4307 - mean_squared_error: 0.3577 - val_loss: 0.3691 - val_mean_squared_error: 0.3664\n",
            "Epoch 38/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4113 - mean_squared_error: 0.3241\n",
            "Epoch 38: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4305 - mean_squared_error: 0.3575 - val_loss: 0.3693 - val_mean_squared_error: 0.3661\n",
            "Epoch 39/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4466 - mean_squared_error: 0.3813\n",
            "Epoch 39: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4299 - mean_squared_error: 0.3558 - val_loss: 0.3696 - val_mean_squared_error: 0.3663\n",
            "Epoch 40/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4353 - mean_squared_error: 0.3761\n",
            "Epoch 40: val_loss did not improve from 0.36871\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4301 - mean_squared_error: 0.3548 - val_loss: 0.3697 - val_mean_squared_error: 0.3660\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3964 - mean_squared_error: 0.3147\n",
            "Epoch 41: val_loss improved from 0.36871 to 0.36768, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.4285 - mean_squared_error: 0.3538 - val_loss: 0.3677 - val_mean_squared_error: 0.3645\n",
            "Epoch 42/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4236 - mean_squared_error: 0.3539\n",
            "Epoch 42: val_loss improved from 0.36768 to 0.36739, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.4300 - mean_squared_error: 0.3568 - val_loss: 0.3674 - val_mean_squared_error: 0.3650\n",
            "Epoch 43/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4486 - mean_squared_error: 0.3780\n",
            "Epoch 43: val_loss did not improve from 0.36739\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4280 - mean_squared_error: 0.3541 - val_loss: 0.3689 - val_mean_squared_error: 0.3651\n",
            "Epoch 44/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4093 - mean_squared_error: 0.2979\n",
            "Epoch 44: val_loss did not improve from 0.36739\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4285 - mean_squared_error: 0.3512 - val_loss: 0.3706 - val_mean_squared_error: 0.3664\n",
            "Epoch 45/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4048 - mean_squared_error: 0.3022\n",
            "Epoch 45: val_loss did not improve from 0.36739\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4278 - mean_squared_error: 0.3514 - val_loss: 0.3678 - val_mean_squared_error: 0.3648\n",
            "Epoch 46/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4089 - mean_squared_error: 0.2964\n",
            "Epoch 46: val_loss did not improve from 0.36739\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4267 - mean_squared_error: 0.3512 - val_loss: 0.3693 - val_mean_squared_error: 0.3658\n",
            "Epoch 47/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4297 - mean_squared_error: 0.3514\n",
            "Epoch 47: val_loss did not improve from 0.36739\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4274 - mean_squared_error: 0.3509 - val_loss: 0.3676 - val_mean_squared_error: 0.3653\n",
            "Epoch 48/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4119 - mean_squared_error: 0.3287\n",
            "Epoch 48: val_loss improved from 0.36739 to 0.36650, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.4271 - mean_squared_error: 0.3523 - val_loss: 0.3665 - val_mean_squared_error: 0.3637\n",
            "Epoch 49/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4224 - mean_squared_error: 0.3553\n",
            "Epoch 49: val_loss did not improve from 0.36650\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4255 - mean_squared_error: 0.3496 - val_loss: 0.3689 - val_mean_squared_error: 0.3656\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4449 - mean_squared_error: 0.3749\n",
            "Epoch 50: val_loss did not improve from 0.36650\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4253 - mean_squared_error: 0.3478 - val_loss: 0.3684 - val_mean_squared_error: 0.3651\n",
            "Epoch 51/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4094 - mean_squared_error: 0.3222\n",
            "Epoch 51: val_loss improved from 0.36650 to 0.36562, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.4249 - mean_squared_error: 0.3484 - val_loss: 0.3656 - val_mean_squared_error: 0.3630\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4251 - mean_squared_error: 0.3493\n",
            "Epoch 52: val_loss did not improve from 0.36562\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4251 - mean_squared_error: 0.3493 - val_loss: 0.3660 - val_mean_squared_error: 0.3633\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4242 - mean_squared_error: 0.3470\n",
            "Epoch 53: val_loss did not improve from 0.36562\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4242 - mean_squared_error: 0.3470 - val_loss: 0.3702 - val_mean_squared_error: 0.3657\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4239 - mean_squared_error: 0.3446\n",
            "Epoch 54: val_loss did not improve from 0.36562\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4239 - mean_squared_error: 0.3446 - val_loss: 0.3666 - val_mean_squared_error: 0.3633\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4234 - mean_squared_error: 0.3462\n",
            "Epoch 55: val_loss improved from 0.36562 to 0.36551, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4234 - mean_squared_error: 0.3462 - val_loss: 0.3655 - val_mean_squared_error: 0.3629\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4234 - mean_squared_error: 0.3457\n",
            "Epoch 56: val_loss did not improve from 0.36551\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4234 - mean_squared_error: 0.3457 - val_loss: 0.3679 - val_mean_squared_error: 0.3643\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4236 - mean_squared_error: 0.3447\n",
            "Epoch 57: val_loss did not improve from 0.36551\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4236 - mean_squared_error: 0.3447 - val_loss: 0.3665 - val_mean_squared_error: 0.3644\n",
            "Epoch 58/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4130 - mean_squared_error: 0.3595\n",
            "Epoch 58: val_loss did not improve from 0.36551\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4240 - mean_squared_error: 0.3450 - val_loss: 0.3696 - val_mean_squared_error: 0.3656\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4223 - mean_squared_error: 0.3434\n",
            "Epoch 59: val_loss did not improve from 0.36551\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4223 - mean_squared_error: 0.3434 - val_loss: 0.3656 - val_mean_squared_error: 0.3628\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4233 - mean_squared_error: 0.3451\n",
            "Epoch 60: val_loss improved from 0.36551 to 0.36311, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4233 - mean_squared_error: 0.3451 - val_loss: 0.3631 - val_mean_squared_error: 0.3608\n",
            "Epoch 61/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4235 - mean_squared_error: 0.3505\n",
            "Epoch 61: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4228 - mean_squared_error: 0.3445 - val_loss: 0.3655 - val_mean_squared_error: 0.3629\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4213 - mean_squared_error: 0.3422\n",
            "Epoch 62: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4213 - mean_squared_error: 0.3422 - val_loss: 0.3689 - val_mean_squared_error: 0.3653\n",
            "Epoch 63/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4276 - mean_squared_error: 0.3569\n",
            "Epoch 63: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4211 - mean_squared_error: 0.3416 - val_loss: 0.3664 - val_mean_squared_error: 0.3637\n",
            "Epoch 64/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4022 - mean_squared_error: 0.2956\n",
            "Epoch 64: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4210 - mean_squared_error: 0.3410 - val_loss: 0.3696 - val_mean_squared_error: 0.3650\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4210 - mean_squared_error: 0.3415\n",
            "Epoch 65: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4210 - mean_squared_error: 0.3415 - val_loss: 0.3649 - val_mean_squared_error: 0.3634\n",
            "Epoch 66/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4218 - mean_squared_error: 0.3468\n",
            "Epoch 66: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4207 - mean_squared_error: 0.3406 - val_loss: 0.3670 - val_mean_squared_error: 0.3637\n",
            "Epoch 67/500\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.3278 - mean_squared_error: 0.2643\n",
            "Epoch 67: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.4196 - mean_squared_error: 0.3393 - val_loss: 0.3659 - val_mean_squared_error: 0.3624\n",
            "Epoch 68/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4270 - mean_squared_error: 0.3564\n",
            "Epoch 68: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4194 - mean_squared_error: 0.3383 - val_loss: 0.3666 - val_mean_squared_error: 0.3624\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4195 - mean_squared_error: 0.3389\n",
            "Epoch 69: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4195 - mean_squared_error: 0.3389 - val_loss: 0.3639 - val_mean_squared_error: 0.3622\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4194 - mean_squared_error: 0.3382\n",
            "Epoch 70: val_loss did not improve from 0.36311\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4194 - mean_squared_error: 0.3382 - val_loss: 0.3685 - val_mean_squared_error: 0.3640\n",
            "Epoch 71/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4382 - mean_squared_error: 0.3617\n",
            "Epoch 71: val_loss improved from 0.36311 to 0.36292, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4200 - mean_squared_error: 0.3393 - val_loss: 0.3629 - val_mean_squared_error: 0.3618\n",
            "Epoch 72/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4344 - mean_squared_error: 0.3626\n",
            "Epoch 72: val_loss did not improve from 0.36292\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4181 - mean_squared_error: 0.3376 - val_loss: 0.3676 - val_mean_squared_error: 0.3637\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4176 - mean_squared_error: 0.3359\n",
            "Epoch 73: val_loss did not improve from 0.36292\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4176 - mean_squared_error: 0.3359 - val_loss: 0.3684 - val_mean_squared_error: 0.3645\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4175 - mean_squared_error: 0.3357\n",
            "Epoch 74: val_loss did not improve from 0.36292\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4175 - mean_squared_error: 0.3357 - val_loss: 0.3641 - val_mean_squared_error: 0.3624\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4191 - mean_squared_error: 0.3389\n",
            "Epoch 75: val_loss improved from 0.36292 to 0.35950, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4191 - mean_squared_error: 0.3389 - val_loss: 0.3595 - val_mean_squared_error: 0.3606\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4168 - mean_squared_error: 0.3350\n",
            "Epoch 76: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4168 - mean_squared_error: 0.3350 - val_loss: 0.3667 - val_mean_squared_error: 0.3628\n",
            "Epoch 77/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4177 - mean_squared_error: 0.3279\n",
            "Epoch 77: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4179 - mean_squared_error: 0.3339 - val_loss: 0.3700 - val_mean_squared_error: 0.3644\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4165 - mean_squared_error: 0.3331\n",
            "Epoch 78: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4165 - mean_squared_error: 0.3331 - val_loss: 0.3648 - val_mean_squared_error: 0.3621\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4160 - mean_squared_error: 0.3342\n",
            "Epoch 79: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4160 - mean_squared_error: 0.3342 - val_loss: 0.3636 - val_mean_squared_error: 0.3617\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4152 - mean_squared_error: 0.3338\n",
            "Epoch 80: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4152 - mean_squared_error: 0.3338 - val_loss: 0.3670 - val_mean_squared_error: 0.3639\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4159 - mean_squared_error: 0.3332\n",
            "Epoch 81: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4159 - mean_squared_error: 0.3332 - val_loss: 0.3694 - val_mean_squared_error: 0.3653\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4144 - mean_squared_error: 0.3317\n",
            "Epoch 82: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4144 - mean_squared_error: 0.3317 - val_loss: 0.3657 - val_mean_squared_error: 0.3628\n",
            "Epoch 83/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4251 - mean_squared_error: 0.3497\n",
            "Epoch 83: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4144 - mean_squared_error: 0.3321 - val_loss: 0.3653 - val_mean_squared_error: 0.3629\n",
            "Epoch 84/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4159 - mean_squared_error: 0.3437\n",
            "Epoch 84: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4139 - mean_squared_error: 0.3314 - val_loss: 0.3657 - val_mean_squared_error: 0.3632\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4142 - mean_squared_error: 0.3317\n",
            "Epoch 85: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4142 - mean_squared_error: 0.3317 - val_loss: 0.3658 - val_mean_squared_error: 0.3633\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4133 - mean_squared_error: 0.3306\n",
            "Epoch 86: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4133 - mean_squared_error: 0.3306 - val_loss: 0.3684 - val_mean_squared_error: 0.3655\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4133 - mean_squared_error: 0.3303\n",
            "Epoch 87: val_loss did not improve from 0.35950\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4133 - mean_squared_error: 0.3303 - val_loss: 0.3671 - val_mean_squared_error: 0.3645\n",
            "Epoch 88/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4065 - mean_squared_error: 0.3241\n",
            "Epoch 88: val_loss improved from 0.35950 to 0.35791, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.4168 - mean_squared_error: 0.3353 - val_loss: 0.3579 - val_mean_squared_error: 0.3613\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4135 - mean_squared_error: 0.3326\n",
            "Epoch 89: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4135 - mean_squared_error: 0.3326 - val_loss: 0.3671 - val_mean_squared_error: 0.3655\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4128 - mean_squared_error: 0.3300\n",
            "Epoch 90: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4128 - mean_squared_error: 0.3300 - val_loss: 0.3711 - val_mean_squared_error: 0.3671\n",
            "Epoch 91/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4257 - mean_squared_error: 0.3495\n",
            "Epoch 91: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4124 - mean_squared_error: 0.3293 - val_loss: 0.3636 - val_mean_squared_error: 0.3626\n",
            "Epoch 92/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4104 - mean_squared_error: 0.3348\n",
            "Epoch 92: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.4118 - mean_squared_error: 0.3287 - val_loss: 0.3678 - val_mean_squared_error: 0.3641\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4117 - mean_squared_error: 0.3285\n",
            "Epoch 93: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.4117 - mean_squared_error: 0.3285 - val_loss: 0.3681 - val_mean_squared_error: 0.3653\n",
            "Epoch 94/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4328 - mean_squared_error: 0.3611\n",
            "Epoch 94: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4149 - mean_squared_error: 0.3298 - val_loss: 0.3729 - val_mean_squared_error: 0.3680\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4126 - mean_squared_error: 0.3281\n",
            "Epoch 95: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4126 - mean_squared_error: 0.3281 - val_loss: 0.3589 - val_mean_squared_error: 0.3610\n",
            "Epoch 96/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4012 - mean_squared_error: 0.3158\n",
            "Epoch 96: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4115 - mean_squared_error: 0.3287 - val_loss: 0.3608 - val_mean_squared_error: 0.3611\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4110 - mean_squared_error: 0.3268\n",
            "Epoch 97: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4110 - mean_squared_error: 0.3268 - val_loss: 0.3683 - val_mean_squared_error: 0.3638\n",
            "Epoch 98/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3992 - mean_squared_error: 0.3168\n",
            "Epoch 98: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.4103 - mean_squared_error: 0.3256 - val_loss: 0.3693 - val_mean_squared_error: 0.3651\n",
            "Epoch 99/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4122 - mean_squared_error: 0.3264\n",
            "Epoch 99: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.4099 - mean_squared_error: 0.3264 - val_loss: 0.3620 - val_mean_squared_error: 0.3628\n",
            "Epoch 100/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4084 - mean_squared_error: 0.3393\n",
            "Epoch 100: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4118 - mean_squared_error: 0.3286 - val_loss: 0.3616 - val_mean_squared_error: 0.3624\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4103 - mean_squared_error: 0.3259\n",
            "Epoch 101: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4103 - mean_squared_error: 0.3259 - val_loss: 0.3691 - val_mean_squared_error: 0.3646\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4100 - mean_squared_error: 0.3252\n",
            "Epoch 102: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4100 - mean_squared_error: 0.3252 - val_loss: 0.3646 - val_mean_squared_error: 0.3625\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4096 - mean_squared_error: 0.3257\n",
            "Epoch 103: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4096 - mean_squared_error: 0.3257 - val_loss: 0.3632 - val_mean_squared_error: 0.3624\n",
            "Epoch 104/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4054 - mean_squared_error: 0.3245\n",
            "Epoch 104: val_loss did not improve from 0.35791\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4119 - mean_squared_error: 0.3266 - val_loss: 0.3716 - val_mean_squared_error: 0.3676\n",
            "Epoch 105/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3774 - mean_squared_error: 0.3067\n",
            "Epoch 105: val_loss improved from 0.35791 to 0.35501, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 66ms/step - loss: 0.4100 - mean_squared_error: 0.3266 - val_loss: 0.3550 - val_mean_squared_error: 0.3606\n",
            "Epoch 106/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3840 - mean_squared_error: 0.2898\n",
            "Epoch 106: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4105 - mean_squared_error: 0.3275 - val_loss: 0.3648 - val_mean_squared_error: 0.3640\n",
            "Epoch 107/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3989 - mean_squared_error: 0.3200\n",
            "Epoch 107: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4085 - mean_squared_error: 0.3245 - val_loss: 0.3640 - val_mean_squared_error: 0.3631\n",
            "Epoch 108/500\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.4598 - mean_squared_error: 0.4482\n",
            "Epoch 108: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4103 - mean_squared_error: 0.3252 - val_loss: 0.3671 - val_mean_squared_error: 0.3645\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4091 - mean_squared_error: 0.3245\n",
            "Epoch 109: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4091 - mean_squared_error: 0.3245 - val_loss: 0.3563 - val_mean_squared_error: 0.3607\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4082 - mean_squared_error: 0.3250\n",
            "Epoch 110: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4082 - mean_squared_error: 0.3250 - val_loss: 0.3655 - val_mean_squared_error: 0.3637\n",
            "Epoch 111/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4009 - mean_squared_error: 0.2983\n",
            "Epoch 111: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4090 - mean_squared_error: 0.3236 - val_loss: 0.3695 - val_mean_squared_error: 0.3659\n",
            "Epoch 112/500\n",
            "1/8 [==>...........................] - ETA: 0s - loss: 0.6199 - mean_squared_error: 0.6348\n",
            "Epoch 112: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.4080 - mean_squared_error: 0.3240 - val_loss: 0.3611 - val_mean_squared_error: 0.3619\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4082 - mean_squared_error: 0.3245\n",
            "Epoch 113: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4082 - mean_squared_error: 0.3245 - val_loss: 0.3585 - val_mean_squared_error: 0.3613\n",
            "Epoch 114/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3912 - mean_squared_error: 0.2637\n",
            "Epoch 114: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4074 - mean_squared_error: 0.3240 - val_loss: 0.3621 - val_mean_squared_error: 0.3636\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4070 - mean_squared_error: 0.3224\n",
            "Epoch 115: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4070 - mean_squared_error: 0.3224 - val_loss: 0.3667 - val_mean_squared_error: 0.3654\n",
            "Epoch 116/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3929 - mean_squared_error: 0.2829\n",
            "Epoch 116: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4068 - mean_squared_error: 0.3219 - val_loss: 0.3676 - val_mean_squared_error: 0.3651\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4069 - mean_squared_error: 0.3219\n",
            "Epoch 117: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4069 - mean_squared_error: 0.3219 - val_loss: 0.3685 - val_mean_squared_error: 0.3661\n",
            "Epoch 118/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4119 - mean_squared_error: 0.3256\n",
            "Epoch 118: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4084 - mean_squared_error: 0.3236 - val_loss: 0.3593 - val_mean_squared_error: 0.3623\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4060 - mean_squared_error: 0.3221\n",
            "Epoch 119: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4060 - mean_squared_error: 0.3221 - val_loss: 0.3659 - val_mean_squared_error: 0.3652\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4074 - mean_squared_error: 0.3230\n",
            "Epoch 120: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4074 - mean_squared_error: 0.3230 - val_loss: 0.3645 - val_mean_squared_error: 0.3647\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4067 - mean_squared_error: 0.3216\n",
            "Epoch 121: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4067 - mean_squared_error: 0.3216 - val_loss: 0.3681 - val_mean_squared_error: 0.3660\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4068 - mean_squared_error: 0.3206\n",
            "Epoch 122: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4068 - mean_squared_error: 0.3206 - val_loss: 0.3694 - val_mean_squared_error: 0.3664\n",
            "Epoch 123/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4237 - mean_squared_error: 0.3418\n",
            "Epoch 123: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4064 - mean_squared_error: 0.3204 - val_loss: 0.3610 - val_mean_squared_error: 0.3636\n",
            "Epoch 124/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3936 - mean_squared_error: 0.2540\n",
            "Epoch 124: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4066 - mean_squared_error: 0.3219 - val_loss: 0.3603 - val_mean_squared_error: 0.3626\n",
            "Epoch 125/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4199 - mean_squared_error: 0.3419\n",
            "Epoch 125: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4057 - mean_squared_error: 0.3215 - val_loss: 0.3623 - val_mean_squared_error: 0.3636\n",
            "Epoch 126/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4093 - mean_squared_error: 0.3267\n",
            "Epoch 126: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4053 - mean_squared_error: 0.3199 - val_loss: 0.3673 - val_mean_squared_error: 0.3658\n",
            "Epoch 127/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4105 - mean_squared_error: 0.3340\n",
            "Epoch 127: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4057 - mean_squared_error: 0.3199 - val_loss: 0.3676 - val_mean_squared_error: 0.3657\n",
            "Epoch 128/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4133 - mean_squared_error: 0.3371\n",
            "Epoch 128: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4063 - mean_squared_error: 0.3197 - val_loss: 0.3685 - val_mean_squared_error: 0.3658\n",
            "Epoch 129/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3972 - mean_squared_error: 0.3113\n",
            "Epoch 129: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4051 - mean_squared_error: 0.3194 - val_loss: 0.3606 - val_mean_squared_error: 0.3635\n",
            "Epoch 130/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4125 - mean_squared_error: 0.3324\n",
            "Epoch 130: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4054 - mean_squared_error: 0.3206 - val_loss: 0.3604 - val_mean_squared_error: 0.3627\n",
            "Epoch 131/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3959 - mean_squared_error: 0.3121\n",
            "Epoch 131: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4057 - mean_squared_error: 0.3198 - val_loss: 0.3678 - val_mean_squared_error: 0.3657\n",
            "Epoch 132/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4005 - mean_squared_error: 0.2996\n",
            "Epoch 132: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4052 - mean_squared_error: 0.3198 - val_loss: 0.3695 - val_mean_squared_error: 0.3678\n",
            "Epoch 133/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4093 - mean_squared_error: 0.3344\n",
            "Epoch 133: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4058 - mean_squared_error: 0.3203 - val_loss: 0.3659 - val_mean_squared_error: 0.3670\n",
            "Epoch 134/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3815 - mean_squared_error: 0.2587\n",
            "Epoch 134: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4053 - mean_squared_error: 0.3204 - val_loss: 0.3575 - val_mean_squared_error: 0.3637\n",
            "Epoch 135/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3848 - mean_squared_error: 0.3261\n",
            "Epoch 135: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4047 - mean_squared_error: 0.3204 - val_loss: 0.3654 - val_mean_squared_error: 0.3651\n",
            "Epoch 136/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4067 - mean_squared_error: 0.3297\n",
            "Epoch 136: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4046 - mean_squared_error: 0.3182 - val_loss: 0.3661 - val_mean_squared_error: 0.3654\n",
            "Epoch 137/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3981 - mean_squared_error: 0.3263\n",
            "Epoch 137: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4041 - mean_squared_error: 0.3183 - val_loss: 0.3633 - val_mean_squared_error: 0.3644\n",
            "Epoch 138/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4176 - mean_squared_error: 0.3481\n",
            "Epoch 138: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4037 - mean_squared_error: 0.3189 - val_loss: 0.3583 - val_mean_squared_error: 0.3629\n",
            "Epoch 139/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4082 - mean_squared_error: 0.3333\n",
            "Epoch 139: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4038 - mean_squared_error: 0.3192 - val_loss: 0.3580 - val_mean_squared_error: 0.3627\n",
            "Epoch 140/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3837 - mean_squared_error: 0.2842\n",
            "Epoch 140: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.4034 - mean_squared_error: 0.3193 - val_loss: 0.3594 - val_mean_squared_error: 0.3637\n",
            "Epoch 141/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4085 - mean_squared_error: 0.3392\n",
            "Epoch 141: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4033 - mean_squared_error: 0.3188 - val_loss: 0.3670 - val_mean_squared_error: 0.3662\n",
            "Epoch 142/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4243 - mean_squared_error: 0.3501\n",
            "Epoch 142: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4058 - mean_squared_error: 0.3190 - val_loss: 0.3788 - val_mean_squared_error: 0.3711\n",
            "Epoch 143/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4085 - mean_squared_error: 0.3152\n",
            "Epoch 143: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4032 - mean_squared_error: 0.3171 - val_loss: 0.3671 - val_mean_squared_error: 0.3662\n",
            "Epoch 144/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4228 - mean_squared_error: 0.3546\n",
            "Epoch 144: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.4032 - mean_squared_error: 0.3181 - val_loss: 0.3584 - val_mean_squared_error: 0.3630\n",
            "Epoch 145/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4182 - mean_squared_error: 0.3417\n",
            "Epoch 145: val_loss did not improve from 0.35501\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4051 - mean_squared_error: 0.3180 - val_loss: 0.3705 - val_mean_squared_error: 0.3672\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "Fold: 4\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 4\n",
            "Epoch 1/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 2.2084 - mean_squared_error: 11.4326 \n",
            "Epoch 1: val_loss improved from inf to 1.27405, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 4s 93ms/step - loss: 2.1636 - mean_squared_error: 10.9500 - val_loss: 1.2740 - val_mean_squared_error: 2.4129\n",
            "Epoch 2/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 1.3468 - mean_squared_error: 3.0834\n",
            "Epoch 2: val_loss improved from 1.27405 to 1.06600, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1.3581 - mean_squared_error: 3.0463 - val_loss: 1.0660 - val_mean_squared_error: 1.7430\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.1706 - mean_squared_error: 2.0433\n",
            "Epoch 3: val_loss improved from 1.06600 to 1.04768, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 1.1706 - mean_squared_error: 2.0433 - val_loss: 1.0477 - val_mean_squared_error: 1.6506\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0939 - mean_squared_error: 1.8364\n",
            "Epoch 4: val_loss improved from 1.04768 to 0.91552, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 1.0939 - mean_squared_error: 1.8364 - val_loss: 0.9155 - val_mean_squared_error: 1.3214\n",
            "Epoch 5/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.9753 - mean_squared_error: 1.5660\n",
            "Epoch 5: val_loss improved from 0.91552 to 0.84901, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 1.0050 - mean_squared_error: 1.7024 - val_loss: 0.8490 - val_mean_squared_error: 1.1493\n",
            "Epoch 6/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.9422 - mean_squared_error: 1.3504\n",
            "Epoch 6: val_loss improved from 0.84901 to 0.82784, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.9241 - mean_squared_error: 1.3417 - val_loss: 0.8278 - val_mean_squared_error: 1.0842\n",
            "Epoch 7/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.8758 - mean_squared_error: 1.3252\n",
            "Epoch 7: val_loss improved from 0.82784 to 0.73465, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.8460 - mean_squared_error: 1.2088 - val_loss: 0.7346 - val_mean_squared_error: 0.8689\n",
            "Epoch 8/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7695 - mean_squared_error: 1.0628\n",
            "Epoch 8: val_loss improved from 0.73465 to 0.70589, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.7768 - mean_squared_error: 1.0726 - val_loss: 0.7059 - val_mean_squared_error: 0.8036\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.7113 - mean_squared_error: 0.8808\n",
            "Epoch 9: val_loss improved from 0.70589 to 0.64308, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.7113 - mean_squared_error: 0.8808 - val_loss: 0.6431 - val_mean_squared_error: 0.6822\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6666 - mean_squared_error: 0.7944\n",
            "Epoch 10: val_loss improved from 0.64308 to 0.60480, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.6666 - mean_squared_error: 0.7944 - val_loss: 0.6048 - val_mean_squared_error: 0.6158\n",
            "Epoch 11/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6267 - mean_squared_error: 0.6840\n",
            "Epoch 11: val_loss improved from 0.60480 to 0.56498, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.6341 - mean_squared_error: 0.7089 - val_loss: 0.5650 - val_mean_squared_error: 0.5464\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6064 - mean_squared_error: 0.6442\n",
            "Epoch 12: val_loss improved from 0.56498 to 0.53040, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.5971 - mean_squared_error: 0.6639 - val_loss: 0.5304 - val_mean_squared_error: 0.4861\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5666 - mean_squared_error: 0.6099\n",
            "Epoch 13: val_loss improved from 0.53040 to 0.52735, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.5666 - mean_squared_error: 0.6099 - val_loss: 0.5274 - val_mean_squared_error: 0.4656\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5541 - mean_squared_error: 0.5801\n",
            "Epoch 14: val_loss improved from 0.52735 to 0.51707, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.5541 - mean_squared_error: 0.5801 - val_loss: 0.5171 - val_mean_squared_error: 0.4425\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5320 - mean_squared_error: 0.5433\n",
            "Epoch 15: val_loss improved from 0.51707 to 0.49381, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.5320 - mean_squared_error: 0.5433 - val_loss: 0.4938 - val_mean_squared_error: 0.4129\n",
            "Epoch 16/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5206 - mean_squared_error: 0.5288\n",
            "Epoch 16: val_loss improved from 0.49381 to 0.45095, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.5277 - mean_squared_error: 0.5429 - val_loss: 0.4509 - val_mean_squared_error: 0.3736\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5083 - mean_squared_error: 0.4992\n",
            "Epoch 17: val_loss did not improve from 0.45095\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.5083 - mean_squared_error: 0.4992 - val_loss: 0.4757 - val_mean_squared_error: 0.3847\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4940 - mean_squared_error: 0.4840\n",
            "Epoch 18: val_loss did not improve from 0.45095\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4940 - mean_squared_error: 0.4840 - val_loss: 0.4540 - val_mean_squared_error: 0.3634\n",
            "Epoch 19/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4903 - mean_squared_error: 0.4713\n",
            "Epoch 19: val_loss improved from 0.45095 to 0.43680, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4819 - mean_squared_error: 0.4629 - val_loss: 0.4368 - val_mean_squared_error: 0.3463\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4770 - mean_squared_error: 0.4620\n",
            "Epoch 20: val_loss did not improve from 0.43680\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4770 - mean_squared_error: 0.4620 - val_loss: 0.4516 - val_mean_squared_error: 0.3495\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4536 - mean_squared_error: 0.4240\n",
            "Epoch 21: val_loss improved from 0.43680 to 0.40840, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.4536 - mean_squared_error: 0.4240 - val_loss: 0.4084 - val_mean_squared_error: 0.3150\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4614 - mean_squared_error: 0.4276\n",
            "Epoch 22: val_loss did not improve from 0.40840\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.4614 - mean_squared_error: 0.4276 - val_loss: 0.4643 - val_mean_squared_error: 0.3473\n",
            "Epoch 23/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4328 - mean_squared_error: 0.3950\n",
            "Epoch 23: val_loss improved from 0.40840 to 0.39133, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.4354 - mean_squared_error: 0.3963 - val_loss: 0.3913 - val_mean_squared_error: 0.2957\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4351 - mean_squared_error: 0.3878\n",
            "Epoch 24: val_loss improved from 0.39133 to 0.38838, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4351 - mean_squared_error: 0.3878 - val_loss: 0.3884 - val_mean_squared_error: 0.2889\n",
            "Epoch 25/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4256 - mean_squared_error: 0.3782\n",
            "Epoch 25: val_loss did not improve from 0.38838\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.4392 - mean_squared_error: 0.3951 - val_loss: 0.4099 - val_mean_squared_error: 0.2994\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4000 - mean_squared_error: 0.3492\n",
            "Epoch 26: val_loss improved from 0.38838 to 0.37573, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.4000 - mean_squared_error: 0.3492 - val_loss: 0.3757 - val_mean_squared_error: 0.2720\n",
            "Epoch 27/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3951 - mean_squared_error: 0.3512\n",
            "Epoch 27: val_loss improved from 0.37573 to 0.36587, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3894 - mean_squared_error: 0.3379 - val_loss: 0.3659 - val_mean_squared_error: 0.2616\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3793 - mean_squared_error: 0.3232\n",
            "Epoch 28: val_loss did not improve from 0.36587\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3793 - mean_squared_error: 0.3232 - val_loss: 0.3663 - val_mean_squared_error: 0.2572\n",
            "Epoch 29/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3819 - mean_squared_error: 0.3365\n",
            "Epoch 29: val_loss improved from 0.36587 to 0.35768, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3688 - mean_squared_error: 0.3104 - val_loss: 0.3577 - val_mean_squared_error: 0.2471\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3635 - mean_squared_error: 0.3004\n",
            "Epoch 30: val_loss improved from 0.35768 to 0.34884, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3635 - mean_squared_error: 0.3004 - val_loss: 0.3488 - val_mean_squared_error: 0.2380\n",
            "Epoch 31/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3591 - mean_squared_error: 0.2997\n",
            "Epoch 31: val_loss did not improve from 0.34884\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3598 - mean_squared_error: 0.2931 - val_loss: 0.3638 - val_mean_squared_error: 0.2455\n",
            "Epoch 32/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3173 - mean_squared_error: 0.2097\n",
            "Epoch 32: val_loss improved from 0.34884 to 0.34175, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3475 - mean_squared_error: 0.2802 - val_loss: 0.3417 - val_mean_squared_error: 0.2321\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3496 - mean_squared_error: 0.2756\n",
            "Epoch 33: val_loss did not improve from 0.34175\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3496 - mean_squared_error: 0.2756 - val_loss: 0.3492 - val_mean_squared_error: 0.2309\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3553 - mean_squared_error: 0.2800\n",
            "Epoch 34: val_loss did not improve from 0.34175\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3553 - mean_squared_error: 0.2800 - val_loss: 0.3649 - val_mean_squared_error: 0.2388\n",
            "Epoch 35/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3382 - mean_squared_error: 0.2703\n",
            "Epoch 35: val_loss improved from 0.34175 to 0.33813, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3411 - mean_squared_error: 0.2646 - val_loss: 0.3381 - val_mean_squared_error: 0.2202\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3313 - mean_squared_error: 0.2536\n",
            "Epoch 36: val_loss improved from 0.33813 to 0.33366, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.3313 - mean_squared_error: 0.2536 - val_loss: 0.3337 - val_mean_squared_error: 0.2166\n",
            "Epoch 37/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3475 - mean_squared_error: 0.2719\n",
            "Epoch 37: val_loss did not improve from 0.33366\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3260 - mean_squared_error: 0.2456 - val_loss: 0.3449 - val_mean_squared_error: 0.2175\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3282 - mean_squared_error: 0.2436\n",
            "Epoch 38: val_loss did not improve from 0.33366\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3282 - mean_squared_error: 0.2436 - val_loss: 0.3338 - val_mean_squared_error: 0.2136\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3176 - mean_squared_error: 0.2351\n",
            "Epoch 39: val_loss did not improve from 0.33366\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3176 - mean_squared_error: 0.2351 - val_loss: 0.3547 - val_mean_squared_error: 0.2211\n",
            "Epoch 40/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3240 - mean_squared_error: 0.2466\n",
            "Epoch 40: val_loss improved from 0.33366 to 0.33141, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3183 - mean_squared_error: 0.2343 - val_loss: 0.3314 - val_mean_squared_error: 0.2109\n",
            "Epoch 41/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3198 - mean_squared_error: 0.2348\n",
            "Epoch 41: val_loss did not improve from 0.33141\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3181 - mean_squared_error: 0.2289 - val_loss: 0.3318 - val_mean_squared_error: 0.2072\n",
            "Epoch 42/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3106 - mean_squared_error: 0.2221\n",
            "Epoch 42: val_loss improved from 0.33141 to 0.32920, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.3085 - mean_squared_error: 0.2201 - val_loss: 0.3292 - val_mean_squared_error: 0.2034\n",
            "Epoch 43/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2992 - mean_squared_error: 0.2076\n",
            "Epoch 43: val_loss improved from 0.32920 to 0.32518, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.3074 - mean_squared_error: 0.2170 - val_loss: 0.3252 - val_mean_squared_error: 0.2008\n",
            "Epoch 44/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2988 - mean_squared_error: 0.1922\n",
            "Epoch 44: val_loss did not improve from 0.32518\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3078 - mean_squared_error: 0.2139 - val_loss: 0.3358 - val_mean_squared_error: 0.2047\n",
            "Epoch 45/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2897 - mean_squared_error: 0.1725\n",
            "Epoch 45: val_loss improved from 0.32518 to 0.31910, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3077 - mean_squared_error: 0.2112 - val_loss: 0.3191 - val_mean_squared_error: 0.2141\n",
            "Epoch 46/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3106 - mean_squared_error: 0.1782\n",
            "Epoch 46: val_loss did not improve from 0.31910\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3274 - mean_squared_error: 0.2291 - val_loss: 0.3549 - val_mean_squared_error: 0.2166\n",
            "Epoch 47/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3006 - mean_squared_error: 0.1980\n",
            "Epoch 47: val_loss improved from 0.31910 to 0.30652, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.3145 - mean_squared_error: 0.2136 - val_loss: 0.3065 - val_mean_squared_error: 0.1973\n",
            "Epoch 48/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2901 - mean_squared_error: 0.1973\n",
            "Epoch 48: val_loss did not improve from 0.30652\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2909 - mean_squared_error: 0.1968 - val_loss: 0.3627 - val_mean_squared_error: 0.2219\n",
            "Epoch 49/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3138 - mean_squared_error: 0.2074\n",
            "Epoch 49: val_loss did not improve from 0.30652\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3203 - mean_squared_error: 0.2189 - val_loss: 0.3077 - val_mean_squared_error: 0.2072\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3239 - mean_squared_error: 0.2287\n",
            "Epoch 50: val_loss did not improve from 0.30652\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3194 - mean_squared_error: 0.2190 - val_loss: 0.3381 - val_mean_squared_error: 0.2051\n",
            "Epoch 51/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3047 - mean_squared_error: 0.1964\n",
            "Epoch 51: val_loss did not improve from 0.30652\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3006 - mean_squared_error: 0.2016 - val_loss: 0.3472 - val_mean_squared_error: 0.2120\n",
            "Epoch 52/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3066 - mean_squared_error: 0.2215\n",
            "Epoch 52: val_loss improved from 0.30652 to 0.30335, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.3033 - mean_squared_error: 0.2109 - val_loss: 0.3034 - val_mean_squared_error: 0.1961\n",
            "Epoch 53/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2831 - mean_squared_error: 0.1642\n",
            "Epoch 53: val_loss did not improve from 0.30335\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2911 - mean_squared_error: 0.1886 - val_loss: 0.3212 - val_mean_squared_error: 0.1968\n",
            "Epoch 54/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2753 - mean_squared_error: 0.1763\n",
            "Epoch 54: val_loss improved from 0.30335 to 0.30330, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2820 - mean_squared_error: 0.1788 - val_loss: 0.3033 - val_mean_squared_error: 0.1967\n",
            "Epoch 55/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3293 - mean_squared_error: 0.2237\n",
            "Epoch 55: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.3039 - mean_squared_error: 0.1929 - val_loss: 0.3461 - val_mean_squared_error: 0.2128\n",
            "Epoch 56/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2986 - mean_squared_error: 0.2027\n",
            "Epoch 56: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2872 - mean_squared_error: 0.1901 - val_loss: 0.3065 - val_mean_squared_error: 0.1970\n",
            "Epoch 57/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2824 - mean_squared_error: 0.1728\n",
            "Epoch 57: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2814 - mean_squared_error: 0.1776 - val_loss: 0.3256 - val_mean_squared_error: 0.2026\n",
            "Epoch 58/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2628 - mean_squared_error: 0.1689\n",
            "Epoch 58: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2738 - mean_squared_error: 0.1724 - val_loss: 0.3143 - val_mean_squared_error: 0.1981\n",
            "Epoch 59/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2664 - mean_squared_error: 0.1606\n",
            "Epoch 59: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2721 - mean_squared_error: 0.1719 - val_loss: 0.3139 - val_mean_squared_error: 0.1983\n",
            "Epoch 60/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2653 - mean_squared_error: 0.1375\n",
            "Epoch 60: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2691 - mean_squared_error: 0.1698 - val_loss: 0.3132 - val_mean_squared_error: 0.1984\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2715 - mean_squared_error: 0.1671\n",
            "Epoch 61: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2715 - mean_squared_error: 0.1671 - val_loss: 0.3243 - val_mean_squared_error: 0.2037\n",
            "Epoch 62/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2769 - mean_squared_error: 0.1618\n",
            "Epoch 62: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2719 - mean_squared_error: 0.1650 - val_loss: 0.3302 - val_mean_squared_error: 0.2049\n",
            "Epoch 63/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2836 - mean_squared_error: 0.1980\n",
            "Epoch 63: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2786 - mean_squared_error: 0.1773 - val_loss: 0.3066 - val_mean_squared_error: 0.1953\n",
            "Epoch 64/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2504 - mean_squared_error: 0.1359\n",
            "Epoch 64: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2668 - mean_squared_error: 0.1596 - val_loss: 0.3543 - val_mean_squared_error: 0.2212\n",
            "Epoch 65/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2774 - mean_squared_error: 0.1626\n",
            "Epoch 65: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2826 - mean_squared_error: 0.1714 - val_loss: 0.3060 - val_mean_squared_error: 0.1941\n",
            "Epoch 66/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2770 - mean_squared_error: 0.1697\n",
            "Epoch 66: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2682 - mean_squared_error: 0.1564 - val_loss: 0.3200 - val_mean_squared_error: 0.1963\n",
            "Epoch 67/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.1491\n",
            "Epoch 67: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2871 - mean_squared_error: 0.1666 - val_loss: 0.3228 - val_mean_squared_error: 0.1976\n",
            "Epoch 68/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3013 - mean_squared_error: 0.1919\n",
            "Epoch 68: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2848 - mean_squared_error: 0.1738 - val_loss: 0.3073 - val_mean_squared_error: 0.1928\n",
            "Epoch 69/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2849 - mean_squared_error: 0.1687\n",
            "Epoch 69: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2756 - mean_squared_error: 0.1589 - val_loss: 0.3190 - val_mean_squared_error: 0.1956\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2844 - mean_squared_error: 0.1700\n",
            "Epoch 70: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2744 - mean_squared_error: 0.1545 - val_loss: 0.3245 - val_mean_squared_error: 0.2003\n",
            "Epoch 71/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2466 - mean_squared_error: 0.1120\n",
            "Epoch 71: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2605 - mean_squared_error: 0.1470 - val_loss: 0.3177 - val_mean_squared_error: 0.1948\n",
            "Epoch 72/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2623 - mean_squared_error: 0.1561\n",
            "Epoch 72: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2609 - mean_squared_error: 0.1491 - val_loss: 0.3167 - val_mean_squared_error: 0.1934\n",
            "Epoch 73/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.1650\n",
            "Epoch 73: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2641 - mean_squared_error: 0.1467 - val_loss: 0.3288 - val_mean_squared_error: 0.1998\n",
            "Epoch 74/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2714 - mean_squared_error: 0.1580\n",
            "Epoch 74: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2551 - mean_squared_error: 0.1426 - val_loss: 0.3197 - val_mean_squared_error: 0.1945\n",
            "Epoch 75/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2505 - mean_squared_error: 0.1378\n",
            "Epoch 75: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2577 - mean_squared_error: 0.1433 - val_loss: 0.3260 - val_mean_squared_error: 0.2002\n",
            "Epoch 76/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2521 - mean_squared_error: 0.1332\n",
            "Epoch 76: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2544 - mean_squared_error: 0.1358 - val_loss: 0.3170 - val_mean_squared_error: 0.1929\n",
            "Epoch 77/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2518 - mean_squared_error: 0.1413\n",
            "Epoch 77: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2502 - mean_squared_error: 0.1332 - val_loss: 0.3186 - val_mean_squared_error: 0.1927\n",
            "Epoch 78/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2324 - mean_squared_error: 0.1010\n",
            "Epoch 78: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2499 - mean_squared_error: 0.1314 - val_loss: 0.3252 - val_mean_squared_error: 0.1956\n",
            "Epoch 79/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2619 - mean_squared_error: 0.1460\n",
            "Epoch 79: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2582 - mean_squared_error: 0.1373 - val_loss: 0.3273 - val_mean_squared_error: 0.1974\n",
            "Epoch 80/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2808 - mean_squared_error: 0.1570\n",
            "Epoch 80: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2609 - mean_squared_error: 0.1407 - val_loss: 0.3127 - val_mean_squared_error: 0.1887\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2515 - mean_squared_error: 0.1296\n",
            "Epoch 81: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2515 - mean_squared_error: 0.1296 - val_loss: 0.3316 - val_mean_squared_error: 0.1962\n",
            "Epoch 82/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2446 - mean_squared_error: 0.1206\n",
            "Epoch 82: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2472 - mean_squared_error: 0.1264 - val_loss: 0.3169 - val_mean_squared_error: 0.1889\n",
            "Epoch 83/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2410 - mean_squared_error: 0.1195\n",
            "Epoch 83: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2453 - mean_squared_error: 0.1250 - val_loss: 0.3189 - val_mean_squared_error: 0.1894\n",
            "Epoch 84/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2503 - mean_squared_error: 0.1276\n",
            "Epoch 84: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2487 - mean_squared_error: 0.1243 - val_loss: 0.3311 - val_mean_squared_error: 0.1959\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.1286\n",
            "Epoch 85: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2523 - mean_squared_error: 0.1286 - val_loss: 0.3120 - val_mean_squared_error: 0.1862\n",
            "Epoch 86/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2534 - mean_squared_error: 0.1238\n",
            "Epoch 86: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2436 - mean_squared_error: 0.1179 - val_loss: 0.3421 - val_mean_squared_error: 0.2036\n",
            "Epoch 87/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2400 - mean_squared_error: 0.1142\n",
            "Epoch 87: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2441 - mean_squared_error: 0.1212 - val_loss: 0.3077 - val_mean_squared_error: 0.1904\n",
            "Epoch 88/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2445 - mean_squared_error: 0.1174\n",
            "Epoch 88: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2536 - mean_squared_error: 0.1242 - val_loss: 0.3353 - val_mean_squared_error: 0.1997\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2449 - mean_squared_error: 0.1209\n",
            "Epoch 89: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2449 - mean_squared_error: 0.1209 - val_loss: 0.3157 - val_mean_squared_error: 0.1853\n",
            "Epoch 90/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2610 - mean_squared_error: 0.1340\n",
            "Epoch 90: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2434 - mean_squared_error: 0.1182 - val_loss: 0.3128 - val_mean_squared_error: 0.1844\n",
            "Epoch 91/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2518 - mean_squared_error: 0.1231\n",
            "Epoch 91: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2495 - mean_squared_error: 0.1193 - val_loss: 0.3368 - val_mean_squared_error: 0.1999\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.1201\n",
            "Epoch 92: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2523 - mean_squared_error: 0.1201 - val_loss: 0.3103 - val_mean_squared_error: 0.1863\n",
            "Epoch 93/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2442 - mean_squared_error: 0.1186\n",
            "Epoch 93: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2526 - mean_squared_error: 0.1229 - val_loss: 0.3315 - val_mean_squared_error: 0.1941\n",
            "Epoch 94/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2450 - mean_squared_error: 0.1164\n",
            "Epoch 94: val_loss did not improve from 0.30330\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2452 - mean_squared_error: 0.1147 - val_loss: 0.3227 - val_mean_squared_error: 0.1876\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "Fold: 5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 5\n",
            "Epoch 1/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 5.6626 - mean_squared_error: 51.7740 \n",
            "Epoch 1: val_loss improved from inf to 3.79858, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 4s 95ms/step - loss: 5.3895 - mean_squared_error: 48.8973 - val_loss: 3.7986 - val_mean_squared_error: 20.6828\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 3.8439 - mean_squared_error: 22.1462\n",
            "Epoch 2: val_loss improved from 3.79858 to 2.78171, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 3.8439 - mean_squared_error: 22.1462 - val_loss: 2.7817 - val_mean_squared_error: 9.5979\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 2.4832 - mean_squared_error: 8.5871 \n",
            "Epoch 3: val_loss improved from 2.78171 to 1.81488, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 2.4832 - mean_squared_error: 8.5871 - val_loss: 1.8149 - val_mean_squared_error: 3.6159\n",
            "Epoch 4/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 1.2803 - mean_squared_error: 1.9625\n",
            "Epoch 4: val_loss improved from 1.81488 to 1.12930, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 1.2263 - mean_squared_error: 1.8719 - val_loss: 1.1293 - val_mean_squared_error: 2.0753\n",
            "Epoch 5/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.8435 - mean_squared_error: 1.1871\n",
            "Epoch 5: val_loss did not improve from 1.12930\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 1.0051 - mean_squared_error: 1.8708 - val_loss: 1.1875 - val_mean_squared_error: 2.3803\n",
            "Epoch 6/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.9938 - mean_squared_error: 1.6888\n",
            "Epoch 6: val_loss improved from 1.12930 to 1.06462, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.9526 - mean_squared_error: 1.6075 - val_loss: 1.0646 - val_mean_squared_error: 1.9064\n",
            "Epoch 7/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.7955 - mean_squared_error: 1.0823\n",
            "Epoch 7: val_loss did not improve from 1.06462\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.7609 - mean_squared_error: 0.9792 - val_loss: 1.0894 - val_mean_squared_error: 1.7680\n",
            "Epoch 8/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.7690 - mean_squared_error: 0.9385\n",
            "Epoch 8: val_loss improved from 1.06462 to 1.02415, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.7477 - mean_squared_error: 0.8828 - val_loss: 1.0242 - val_mean_squared_error: 1.6273\n",
            "Epoch 9/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6796 - mean_squared_error: 0.7854\n",
            "Epoch 9: val_loss improved from 1.02415 to 0.94256, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.6786 - mean_squared_error: 0.7744 - val_loss: 0.9426 - val_mean_squared_error: 1.5308\n",
            "Epoch 10/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.6001 - mean_squared_error: 0.6510\n",
            "Epoch 10: val_loss improved from 0.94256 to 0.93337, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.6229 - mean_squared_error: 0.6796 - val_loss: 0.9334 - val_mean_squared_error: 1.4244\n",
            "Epoch 11/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5701 - mean_squared_error: 0.5830\n",
            "Epoch 11: val_loss improved from 0.93337 to 0.87638, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.5847 - mean_squared_error: 0.6152 - val_loss: 0.8764 - val_mean_squared_error: 1.3336\n",
            "Epoch 12/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.5605 - mean_squared_error: 0.5636\n",
            "Epoch 12: val_loss improved from 0.87638 to 0.83762, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.5577 - mean_squared_error: 0.5666 - val_loss: 0.8376 - val_mean_squared_error: 1.2677\n",
            "Epoch 13/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5235 - mean_squared_error: 0.5243\n",
            "Epoch 13: val_loss improved from 0.83762 to 0.82127, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.5363 - mean_squared_error: 0.5355 - val_loss: 0.8213 - val_mean_squared_error: 1.2156\n",
            "Epoch 14/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5391 - mean_squared_error: 0.5561\n",
            "Epoch 14: val_loss improved from 0.82127 to 0.80712, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.5198 - mean_squared_error: 0.5145 - val_loss: 0.8071 - val_mean_squared_error: 1.1688\n",
            "Epoch 15/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5159 - mean_squared_error: 0.5212\n",
            "Epoch 15: val_loss improved from 0.80712 to 0.78757, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.5079 - mean_squared_error: 0.4949 - val_loss: 0.7876 - val_mean_squared_error: 1.1434\n",
            "Epoch 16/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4784 - mean_squared_error: 0.4625\n",
            "Epoch 16: val_loss improved from 0.78757 to 0.77360, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.4928 - mean_squared_error: 0.4729 - val_loss: 0.7736 - val_mean_squared_error: 1.1064\n",
            "Epoch 17/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4731 - mean_squared_error: 0.4467\n",
            "Epoch 17: val_loss improved from 0.77360 to 0.75597, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.4794 - mean_squared_error: 0.4553 - val_loss: 0.7560 - val_mean_squared_error: 1.0637\n",
            "Epoch 18/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4654 - mean_squared_error: 0.4335\n",
            "Epoch 18: val_loss improved from 0.75597 to 0.73707, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.4695 - mean_squared_error: 0.4368 - val_loss: 0.7371 - val_mean_squared_error: 1.0192\n",
            "Epoch 19/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4060 - mean_squared_error: 0.3126\n",
            "Epoch 19: val_loss improved from 0.73707 to 0.71660, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.4585 - mean_squared_error: 0.4169 - val_loss: 0.7166 - val_mean_squared_error: 0.9686\n",
            "Epoch 20/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4959 - mean_squared_error: 0.4752\n",
            "Epoch 20: val_loss improved from 0.71660 to 0.69527, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.4473 - mean_squared_error: 0.3972 - val_loss: 0.6953 - val_mean_squared_error: 0.9141\n",
            "Epoch 21/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4159 - mean_squared_error: 0.3506\n",
            "Epoch 21: val_loss improved from 0.69527 to 0.67830, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.4340 - mean_squared_error: 0.3763 - val_loss: 0.6783 - val_mean_squared_error: 0.8849\n",
            "Epoch 22/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4286 - mean_squared_error: 0.3770\n",
            "Epoch 22: val_loss improved from 0.67830 to 0.65592, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.4218 - mean_squared_error: 0.3672 - val_loss: 0.6559 - val_mean_squared_error: 0.8344\n",
            "Epoch 23/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4259 - mean_squared_error: 0.3394\n",
            "Epoch 23: val_loss improved from 0.65592 to 0.65173, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.4130 - mean_squared_error: 0.3399 - val_loss: 0.6517 - val_mean_squared_error: 0.8296\n",
            "Epoch 24/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3874 - mean_squared_error: 0.3088\n",
            "Epoch 24: val_loss improved from 0.65173 to 0.61910, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.4109 - mean_squared_error: 0.3435 - val_loss: 0.6191 - val_mean_squared_error: 0.7584\n",
            "Epoch 25/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3858 - mean_squared_error: 0.3001\n",
            "Epoch 25: val_loss improved from 0.61910 to 0.61225, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3855 - mean_squared_error: 0.3128 - val_loss: 0.6122 - val_mean_squared_error: 0.7362\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4034 - mean_squared_error: 0.3454\n",
            "Epoch 26: val_loss improved from 0.61225 to 0.59874, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.3754 - mean_squared_error: 0.3019 - val_loss: 0.5987 - val_mean_squared_error: 0.7078\n",
            "Epoch 27/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3642 - mean_squared_error: 0.2933\n",
            "Epoch 27: val_loss improved from 0.59874 to 0.58022, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3667 - mean_squared_error: 0.2907 - val_loss: 0.5802 - val_mean_squared_error: 0.6742\n",
            "Epoch 28/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3712 - mean_squared_error: 0.3004\n",
            "Epoch 28: val_loss improved from 0.58022 to 0.56861, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3600 - mean_squared_error: 0.2794 - val_loss: 0.5686 - val_mean_squared_error: 0.6498\n",
            "Epoch 29/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3643 - mean_squared_error: 0.2933\n",
            "Epoch 29: val_loss improved from 0.56861 to 0.55644, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3570 - mean_squared_error: 0.2750 - val_loss: 0.5564 - val_mean_squared_error: 0.6269\n",
            "Epoch 30/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3463 - mean_squared_error: 0.2353\n",
            "Epoch 30: val_loss improved from 0.55644 to 0.54242, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3473 - mean_squared_error: 0.2590 - val_loss: 0.5424 - val_mean_squared_error: 0.5988\n",
            "Epoch 31/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3367 - mean_squared_error: 0.2608\n",
            "Epoch 31: val_loss improved from 0.54242 to 0.51592, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3451 - mean_squared_error: 0.2572 - val_loss: 0.5159 - val_mean_squared_error: 0.5665\n",
            "Epoch 32/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3137 - mean_squared_error: 0.2057\n",
            "Epoch 32: val_loss did not improve from 0.51592\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3380 - mean_squared_error: 0.2444 - val_loss: 0.5216 - val_mean_squared_error: 0.5539\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3311 - mean_squared_error: 0.2392\n",
            "Epoch 33: val_loss improved from 0.51592 to 0.50314, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3311 - mean_squared_error: 0.2392 - val_loss: 0.5031 - val_mean_squared_error: 0.5288\n",
            "Epoch 34/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3295 - mean_squared_error: 0.2445\n",
            "Epoch 34: val_loss improved from 0.50314 to 0.49135, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.3186 - mean_squared_error: 0.2283 - val_loss: 0.4913 - val_mean_squared_error: 0.5067\n",
            "Epoch 35/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3097 - mean_squared_error: 0.2168\n",
            "Epoch 35: val_loss improved from 0.49135 to 0.48437, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3147 - mean_squared_error: 0.2231 - val_loss: 0.4844 - val_mean_squared_error: 0.4885\n",
            "Epoch 36/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3119 - mean_squared_error: 0.2201\n",
            "Epoch 36: val_loss improved from 0.48437 to 0.46156, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3154 - mean_squared_error: 0.2178 - val_loss: 0.4616 - val_mean_squared_error: 0.4601\n",
            "Epoch 37/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3322 - mean_squared_error: 0.2344\n",
            "Epoch 37: val_loss improved from 0.46156 to 0.44190, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3168 - mean_squared_error: 0.2157 - val_loss: 0.4419 - val_mean_squared_error: 0.4434\n",
            "Epoch 38/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3006 - mean_squared_error: 0.1936\n",
            "Epoch 38: val_loss did not improve from 0.44190\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3065 - mean_squared_error: 0.2066 - val_loss: 0.4608 - val_mean_squared_error: 0.4450\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3070 - mean_squared_error: 0.2057\n",
            "Epoch 39: val_loss improved from 0.44190 to 0.43032, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3070 - mean_squared_error: 0.2057 - val_loss: 0.4303 - val_mean_squared_error: 0.4153\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3005 - mean_squared_error: 0.1992\n",
            "Epoch 40: val_loss did not improve from 0.43032\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.3005 - mean_squared_error: 0.1992 - val_loss: 0.4348 - val_mean_squared_error: 0.4035\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2892 - mean_squared_error: 0.1808\n",
            "Epoch 41: val_loss did not improve from 0.43032\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3015 - mean_squared_error: 0.1953 - val_loss: 0.4389 - val_mean_squared_error: 0.4031\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3014 - mean_squared_error: 0.1951\n",
            "Epoch 42: val_loss improved from 0.43032 to 0.41452, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3014 - mean_squared_error: 0.1951 - val_loss: 0.4145 - val_mean_squared_error: 0.3862\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3003 - mean_squared_error: 0.1922\n",
            "Epoch 43: val_loss did not improve from 0.41452\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.3003 - mean_squared_error: 0.1922 - val_loss: 0.4197 - val_mean_squared_error: 0.3783\n",
            "Epoch 44/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2969 - mean_squared_error: 0.1801\n",
            "Epoch 44: val_loss improved from 0.41452 to 0.39692, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2952 - mean_squared_error: 0.1854 - val_loss: 0.3969 - val_mean_squared_error: 0.3522\n",
            "Epoch 45/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2780 - mean_squared_error: 0.1629\n",
            "Epoch 45: val_loss improved from 0.39692 to 0.38875, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2870 - mean_squared_error: 0.1805 - val_loss: 0.3887 - val_mean_squared_error: 0.3386\n",
            "Epoch 46/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2741 - mean_squared_error: 0.1605\n",
            "Epoch 46: val_loss improved from 0.38875 to 0.37933, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2845 - mean_squared_error: 0.1770 - val_loss: 0.3793 - val_mean_squared_error: 0.3271\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2906 - mean_squared_error: 0.1775\n",
            "Epoch 47: val_loss did not improve from 0.37933\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2906 - mean_squared_error: 0.1775 - val_loss: 0.3856 - val_mean_squared_error: 0.3248\n",
            "Epoch 48/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2768 - mean_squared_error: 0.1671\n",
            "Epoch 48: val_loss improved from 0.37933 to 0.36264, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2758 - mean_squared_error: 0.1684 - val_loss: 0.3626 - val_mean_squared_error: 0.3077\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2749 - mean_squared_error: 0.1640\n",
            "Epoch 49: val_loss improved from 0.36264 to 0.35413, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2749 - mean_squared_error: 0.1640 - val_loss: 0.3541 - val_mean_squared_error: 0.2955\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2881 - mean_squared_error: 0.1782\n",
            "Epoch 50: val_loss improved from 0.35413 to 0.34619, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2800 - mean_squared_error: 0.1673 - val_loss: 0.3462 - val_mean_squared_error: 0.2856\n",
            "Epoch 51/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2886 - mean_squared_error: 0.1662\n",
            "Epoch 51: val_loss did not improve from 0.34619\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2845 - mean_squared_error: 0.1642 - val_loss: 0.3518 - val_mean_squared_error: 0.2851\n",
            "Epoch 52/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2695 - mean_squared_error: 0.1616\n",
            "Epoch 52: val_loss improved from 0.34619 to 0.34040, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2719 - mean_squared_error: 0.1581 - val_loss: 0.3404 - val_mean_squared_error: 0.2767\n",
            "Epoch 53/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2555 - mean_squared_error: 0.1431\n",
            "Epoch 53: val_loss did not improve from 0.34040\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2727 - mean_squared_error: 0.1575 - val_loss: 0.3443 - val_mean_squared_error: 0.2721\n",
            "Epoch 54/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2618 - mean_squared_error: 0.1502\n",
            "Epoch 54: val_loss improved from 0.34040 to 0.33065, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2657 - mean_squared_error: 0.1513 - val_loss: 0.3307 - val_mean_squared_error: 0.2579\n",
            "Epoch 55/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2754 - mean_squared_error: 0.1585\n",
            "Epoch 55: val_loss improved from 0.33065 to 0.32808, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2678 - mean_squared_error: 0.1540 - val_loss: 0.3281 - val_mean_squared_error: 0.2493\n",
            "Epoch 56/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2530 - mean_squared_error: 0.1377\n",
            "Epoch 56: val_loss did not improve from 0.32808\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2709 - mean_squared_error: 0.1507 - val_loss: 0.3571 - val_mean_squared_error: 0.2699\n",
            "Epoch 57/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2784 - mean_squared_error: 0.1618\n",
            "Epoch 57: val_loss did not improve from 0.32808\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2796 - mean_squared_error: 0.1595 - val_loss: 0.3331 - val_mean_squared_error: 0.2478\n",
            "Epoch 58/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2674 - mean_squared_error: 0.1550\n",
            "Epoch 58: val_loss improved from 0.32808 to 0.31530, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2614 - mean_squared_error: 0.1445 - val_loss: 0.3153 - val_mean_squared_error: 0.2361\n",
            "Epoch 59/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2412 - mean_squared_error: 0.1281\n",
            "Epoch 59: val_loss improved from 0.31530 to 0.30352, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2530 - mean_squared_error: 0.1400 - val_loss: 0.3035 - val_mean_squared_error: 0.2234\n",
            "Epoch 60/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2457 - mean_squared_error: 0.1325\n",
            "Epoch 60: val_loss did not improve from 0.30352\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2565 - mean_squared_error: 0.1415 - val_loss: 0.3088 - val_mean_squared_error: 0.2210\n",
            "Epoch 61/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2504 - mean_squared_error: 0.1416\n",
            "Epoch 61: val_loss did not improve from 0.30352\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2512 - mean_squared_error: 0.1379 - val_loss: 0.3086 - val_mean_squared_error: 0.2204\n",
            "Epoch 62/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2548 - mean_squared_error: 0.1338\n",
            "Epoch 62: val_loss improved from 0.30352 to 0.29735, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2566 - mean_squared_error: 0.1395 - val_loss: 0.2973 - val_mean_squared_error: 0.2116\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2549 - mean_squared_error: 0.1357\n",
            "Epoch 63: val_loss improved from 0.29735 to 0.29187, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2549 - mean_squared_error: 0.1357 - val_loss: 0.2919 - val_mean_squared_error: 0.2043\n",
            "Epoch 64/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2404 - mean_squared_error: 0.1138\n",
            "Epoch 64: val_loss improved from 0.29187 to 0.28453, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2534 - mean_squared_error: 0.1343 - val_loss: 0.2845 - val_mean_squared_error: 0.1980\n",
            "Epoch 65/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2520 - mean_squared_error: 0.1392\n",
            "Epoch 65: val_loss improved from 0.28453 to 0.28418, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2465 - mean_squared_error: 0.1330 - val_loss: 0.2842 - val_mean_squared_error: 0.1973\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2501 - mean_squared_error: 0.1327\n",
            "Epoch 66: val_loss did not improve from 0.28418\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2501 - mean_squared_error: 0.1327 - val_loss: 0.2987 - val_mean_squared_error: 0.2002\n",
            "Epoch 67/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2595 - mean_squared_error: 0.1377\n",
            "Epoch 67: val_loss improved from 0.28418 to 0.28415, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2557 - mean_squared_error: 0.1342 - val_loss: 0.2841 - val_mean_squared_error: 0.1925\n",
            "Epoch 68/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2605 - mean_squared_error: 0.1428\n",
            "Epoch 68: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2533 - mean_squared_error: 0.1349 - val_loss: 0.3016 - val_mean_squared_error: 0.1960\n",
            "Epoch 69/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2822 - mean_squared_error: 0.1570\n",
            "Epoch 69: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2692 - mean_squared_error: 0.1438 - val_loss: 0.3055 - val_mean_squared_error: 0.1985\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2370 - mean_squared_error: 0.1285\n",
            "Epoch 70: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2498 - mean_squared_error: 0.1304 - val_loss: 0.2960 - val_mean_squared_error: 0.1885\n",
            "Epoch 71/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.1380\n",
            "Epoch 71: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2676 - mean_squared_error: 0.1419 - val_loss: 0.3065 - val_mean_squared_error: 0.1967\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.1322\n",
            "Epoch 72: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2493 - mean_squared_error: 0.1322 - val_loss: 0.2853 - val_mean_squared_error: 0.1856\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2435 - mean_squared_error: 0.1268\n",
            "Epoch 73: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2435 - mean_squared_error: 0.1268 - val_loss: 0.2868 - val_mean_squared_error: 0.1798\n",
            "Epoch 74/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2421 - mean_squared_error: 0.1265\n",
            "Epoch 74: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2424 - mean_squared_error: 0.1255 - val_loss: 0.3056 - val_mean_squared_error: 0.1897\n",
            "Epoch 75/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2508 - mean_squared_error: 0.1360\n",
            "Epoch 75: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2540 - mean_squared_error: 0.1353 - val_loss: 0.2908 - val_mean_squared_error: 0.1809\n",
            "Epoch 76/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2314 - mean_squared_error: 0.1152\n",
            "Epoch 76: val_loss did not improve from 0.28415\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2411 - mean_squared_error: 0.1241 - val_loss: 0.2883 - val_mean_squared_error: 0.1810\n",
            "Epoch 77/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2184 - mean_squared_error: 0.1074\n",
            "Epoch 77: val_loss improved from 0.28415 to 0.28006, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2407 - mean_squared_error: 0.1238 - val_loss: 0.2801 - val_mean_squared_error: 0.1769\n",
            "Epoch 78/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2460 - mean_squared_error: 0.1278\n",
            "Epoch 78: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2447 - mean_squared_error: 0.1277 - val_loss: 0.2888 - val_mean_squared_error: 0.1787\n",
            "Epoch 79/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2424 - mean_squared_error: 0.1223\n",
            "Epoch 79: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2418 - mean_squared_error: 0.1228 - val_loss: 0.2951 - val_mean_squared_error: 0.1834\n",
            "Epoch 80/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2652 - mean_squared_error: 0.1434\n",
            "Epoch 80: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2539 - mean_squared_error: 0.1320 - val_loss: 0.3153 - val_mean_squared_error: 0.1939\n",
            "Epoch 81/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2416 - mean_squared_error: 0.1207\n",
            "Epoch 81: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2478 - mean_squared_error: 0.1281 - val_loss: 0.2820 - val_mean_squared_error: 0.1750\n",
            "Epoch 82/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2450 - mean_squared_error: 0.1287\n",
            "Epoch 82: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2365 - mean_squared_error: 0.1198 - val_loss: 0.2935 - val_mean_squared_error: 0.1793\n",
            "Epoch 83/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2467 - mean_squared_error: 0.1315\n",
            "Epoch 83: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2421 - mean_squared_error: 0.1240 - val_loss: 0.2936 - val_mean_squared_error: 0.1778\n",
            "Epoch 84/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2485 - mean_squared_error: 0.1303\n",
            "Epoch 84: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2413 - mean_squared_error: 0.1211 - val_loss: 0.2836 - val_mean_squared_error: 0.1758\n",
            "Epoch 85/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2404 - mean_squared_error: 0.1191\n",
            "Epoch 85: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2407 - mean_squared_error: 0.1222 - val_loss: 0.3158 - val_mean_squared_error: 0.1904\n",
            "Epoch 86/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2513 - mean_squared_error: 0.1304\n",
            "Epoch 86: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2531 - mean_squared_error: 0.1304 - val_loss: 0.3127 - val_mean_squared_error: 0.1851\n",
            "Epoch 87/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2576 - mean_squared_error: 0.1270\n",
            "Epoch 87: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2624 - mean_squared_error: 0.1331 - val_loss: 0.2994 - val_mean_squared_error: 0.1765\n",
            "Epoch 88/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2384 - mean_squared_error: 0.1145\n",
            "Epoch 88: val_loss did not improve from 0.28006\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2492 - mean_squared_error: 0.1258 - val_loss: 0.2847 - val_mean_squared_error: 0.1758\n",
            "Epoch 89/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2467 - mean_squared_error: 0.1251\n",
            "Epoch 89: val_loss improved from 0.28006 to 0.27957, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2352 - mean_squared_error: 0.1175 - val_loss: 0.2796 - val_mean_squared_error: 0.1665\n",
            "Epoch 90/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2400 - mean_squared_error: 0.1213\n",
            "Epoch 90: val_loss did not improve from 0.27957\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2377 - mean_squared_error: 0.1166 - val_loss: 0.2990 - val_mean_squared_error: 0.1763\n",
            "Epoch 91/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2384 - mean_squared_error: 0.1225\n",
            "Epoch 91: val_loss did not improve from 0.27957\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2455 - mean_squared_error: 0.1223 - val_loss: 0.3001 - val_mean_squared_error: 0.1773\n",
            "Epoch 92/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2565 - mean_squared_error: 0.1338\n",
            "Epoch 92: val_loss did not improve from 0.27957\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2413 - mean_squared_error: 0.1207 - val_loss: 0.3136 - val_mean_squared_error: 0.1846\n",
            "Epoch 93/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2588 - mean_squared_error: 0.1324\n",
            "Epoch 93: val_loss did not improve from 0.27957\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2496 - mean_squared_error: 0.1246 - val_loss: 0.3001 - val_mean_squared_error: 0.1708\n",
            "Epoch 94/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2600 - mean_squared_error: 0.1332\n",
            "Epoch 94: val_loss did not improve from 0.27957\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2383 - mean_squared_error: 0.1150 - val_loss: 0.2814 - val_mean_squared_error: 0.1668\n",
            "Epoch 95/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2492 - mean_squared_error: 0.1324\n",
            "Epoch 95: val_loss improved from 0.27957 to 0.27817, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.2350 - mean_squared_error: 0.1152 - val_loss: 0.2782 - val_mean_squared_error: 0.1635\n",
            "Epoch 96/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2319 - mean_squared_error: 0.1110\n",
            "Epoch 96: val_loss did not improve from 0.27817\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2339 - mean_squared_error: 0.1160 - val_loss: 0.2899 - val_mean_squared_error: 0.1694\n",
            "Epoch 97/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2245 - mean_squared_error: 0.1044\n",
            "Epoch 97: val_loss did not improve from 0.27817\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2428 - mean_squared_error: 0.1191 - val_loss: 0.2871 - val_mean_squared_error: 0.1656\n",
            "Epoch 98/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2263 - mean_squared_error: 0.1032\n",
            "Epoch 98: val_loss did not improve from 0.27817\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2431 - mean_squared_error: 0.1165 - val_loss: 0.2820 - val_mean_squared_error: 0.1608\n",
            "Epoch 99/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.0964\n",
            "Epoch 99: val_loss did not improve from 0.27817\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2381 - mean_squared_error: 0.1170 - val_loss: 0.2800 - val_mean_squared_error: 0.1645\n",
            "Epoch 100/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2396 - mean_squared_error: 0.1158\n",
            "Epoch 100: val_loss improved from 0.27817 to 0.27553, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.2370 - mean_squared_error: 0.1123 - val_loss: 0.2755 - val_mean_squared_error: 0.1600\n",
            "Epoch 101/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2420 - mean_squared_error: 0.1234\n",
            "Epoch 101: val_loss did not improve from 0.27553\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2361 - mean_squared_error: 0.1152 - val_loss: 0.2762 - val_mean_squared_error: 0.1568\n",
            "Epoch 102/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2345 - mean_squared_error: 0.1127\n",
            "Epoch 102: val_loss did not improve from 0.27553\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2350 - mean_squared_error: 0.1142 - val_loss: 0.2881 - val_mean_squared_error: 0.1651\n",
            "Epoch 103/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2370 - mean_squared_error: 0.1111\n",
            "Epoch 103: val_loss improved from 0.27553 to 0.27396, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.2432 - mean_squared_error: 0.1166 - val_loss: 0.2740 - val_mean_squared_error: 0.1569\n",
            "Epoch 104/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2480 - mean_squared_error: 0.1276\n",
            "Epoch 104: val_loss did not improve from 0.27396\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2429 - mean_squared_error: 0.1198 - val_loss: 0.2922 - val_mean_squared_error: 0.1636\n",
            "Epoch 105/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2503 - mean_squared_error: 0.1235\n",
            "Epoch 105: val_loss did not improve from 0.27396\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2447 - mean_squared_error: 0.1142 - val_loss: 0.2894 - val_mean_squared_error: 0.1632\n",
            "Epoch 106/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2362 - mean_squared_error: 0.1160\n",
            "Epoch 106: val_loss did not improve from 0.27396\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2360 - mean_squared_error: 0.1142 - val_loss: 0.2760 - val_mean_squared_error: 0.1555\n",
            "Epoch 107/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2075 - mean_squared_error: 0.0947\n",
            "Epoch 107: val_loss did not improve from 0.27396\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2288 - mean_squared_error: 0.1103 - val_loss: 0.2916 - val_mean_squared_error: 0.1639\n",
            "Epoch 108/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2329 - mean_squared_error: 0.1148\n",
            "Epoch 108: val_loss did not improve from 0.27396\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2410 - mean_squared_error: 0.1153 - val_loss: 0.3002 - val_mean_squared_error: 0.1680\n",
            "Epoch 109/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2618 - mean_squared_error: 0.1349\n",
            "Epoch 109: val_loss improved from 0.27396 to 0.27210, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2395 - mean_squared_error: 0.1170 - val_loss: 0.2721 - val_mean_squared_error: 0.1542\n",
            "Epoch 110/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2169 - mean_squared_error: 0.1041\n",
            "Epoch 110: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2252 - mean_squared_error: 0.1063 - val_loss: 0.2796 - val_mean_squared_error: 0.1527\n",
            "Epoch 111/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2282 - mean_squared_error: 0.1080\n",
            "Epoch 111: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2282 - mean_squared_error: 0.1082 - val_loss: 0.2826 - val_mean_squared_error: 0.1575\n",
            "Epoch 112/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2314 - mean_squared_error: 0.1150\n",
            "Epoch 112: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2261 - mean_squared_error: 0.1078 - val_loss: 0.2735 - val_mean_squared_error: 0.1509\n",
            "Epoch 113/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2321 - mean_squared_error: 0.1101\n",
            "Epoch 113: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2315 - mean_squared_error: 0.1112 - val_loss: 0.2836 - val_mean_squared_error: 0.1526\n",
            "Epoch 114/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2425 - mean_squared_error: 0.1123\n",
            "Epoch 114: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2323 - mean_squared_error: 0.1088 - val_loss: 0.2784 - val_mean_squared_error: 0.1536\n",
            "Epoch 115/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2327 - mean_squared_error: 0.1028\n",
            "Epoch 115: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2296 - mean_squared_error: 0.1085 - val_loss: 0.2882 - val_mean_squared_error: 0.1559\n",
            "Epoch 116/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2376 - mean_squared_error: 0.1125\n",
            "Epoch 116: val_loss did not improve from 0.27210\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2300 - mean_squared_error: 0.1085 - val_loss: 0.2772 - val_mean_squared_error: 0.1521\n",
            "Epoch 117/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2366 - mean_squared_error: 0.1177\n",
            "Epoch 117: val_loss improved from 0.27210 to 0.26966, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.2258 - mean_squared_error: 0.1064 - val_loss: 0.2697 - val_mean_squared_error: 0.1496\n",
            "Epoch 118/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2267 - mean_squared_error: 0.1119\n",
            "Epoch 118: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2268 - mean_squared_error: 0.1083 - val_loss: 0.2780 - val_mean_squared_error: 0.1514\n",
            "Epoch 119/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2236 - mean_squared_error: 0.1060\n",
            "Epoch 119: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2274 - mean_squared_error: 0.1069 - val_loss: 0.2750 - val_mean_squared_error: 0.1502\n",
            "Epoch 120/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2189 - mean_squared_error: 0.1072\n",
            "Epoch 120: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2251 - mean_squared_error: 0.1074 - val_loss: 0.2705 - val_mean_squared_error: 0.1453\n",
            "Epoch 121/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2227 - mean_squared_error: 0.1119\n",
            "Epoch 121: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2231 - mean_squared_error: 0.1055 - val_loss: 0.2784 - val_mean_squared_error: 0.1500\n",
            "Epoch 122/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2296 - mean_squared_error: 0.1054\n",
            "Epoch 122: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2274 - mean_squared_error: 0.1070 - val_loss: 0.2698 - val_mean_squared_error: 0.1475\n",
            "Epoch 123/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2264 - mean_squared_error: 0.1194\n",
            "Epoch 123: val_loss did not improve from 0.26966\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2248 - mean_squared_error: 0.1063 - val_loss: 0.2744 - val_mean_squared_error: 0.1491\n",
            "Epoch 124/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2174 - mean_squared_error: 0.0961\n",
            "Epoch 124: val_loss improved from 0.26966 to 0.26948, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2233 - mean_squared_error: 0.1053 - val_loss: 0.2695 - val_mean_squared_error: 0.1458\n",
            "Epoch 125/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2218 - mean_squared_error: 0.1032\n",
            "Epoch 125: val_loss improved from 0.26948 to 0.26864, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2242 - mean_squared_error: 0.1043 - val_loss: 0.2686 - val_mean_squared_error: 0.1423\n",
            "Epoch 126/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2247 - mean_squared_error: 0.1044\n",
            "Epoch 126: val_loss did not improve from 0.26864\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2265 - mean_squared_error: 0.1066 - val_loss: 0.2785 - val_mean_squared_error: 0.1472\n",
            "Epoch 127/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2319 - mean_squared_error: 0.1093\n",
            "Epoch 127: val_loss did not improve from 0.26864\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2316 - mean_squared_error: 0.1074 - val_loss: 0.2723 - val_mean_squared_error: 0.1454\n",
            "Epoch 128/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2198 - mean_squared_error: 0.1037\n",
            "Epoch 128: val_loss did not improve from 0.26864\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2313 - mean_squared_error: 0.1075 - val_loss: 0.2777 - val_mean_squared_error: 0.1473\n",
            "Epoch 129/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2223 - mean_squared_error: 0.0955\n",
            "Epoch 129: val_loss did not improve from 0.26864\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2335 - mean_squared_error: 0.1100 - val_loss: 0.2852 - val_mean_squared_error: 0.1493\n",
            "Epoch 130/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2226 - mean_squared_error: 0.1002\n",
            "Epoch 130: val_loss did not improve from 0.26864\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2277 - mean_squared_error: 0.1065 - val_loss: 0.2725 - val_mean_squared_error: 0.1430\n",
            "Epoch 131/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2256 - mean_squared_error: 0.1016\n",
            "Epoch 131: val_loss improved from 0.26864 to 0.26568, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2273 - mean_squared_error: 0.1062 - val_loss: 0.2657 - val_mean_squared_error: 0.1384\n",
            "Epoch 132/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2201 - mean_squared_error: 0.1015\n",
            "Epoch 132: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2281 - mean_squared_error: 0.1071 - val_loss: 0.2705 - val_mean_squared_error: 0.1468\n",
            "Epoch 133/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2312 - mean_squared_error: 0.1033\n",
            "Epoch 133: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2322 - mean_squared_error: 0.1060 - val_loss: 0.2818 - val_mean_squared_error: 0.1501\n",
            "Epoch 134/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2242 - mean_squared_error: 0.1026\n",
            "Epoch 134: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2312 - mean_squared_error: 0.1058 - val_loss: 0.2730 - val_mean_squared_error: 0.1413\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2330 - mean_squared_error: 0.1070\n",
            "Epoch 135: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2330 - mean_squared_error: 0.1070 - val_loss: 0.2707 - val_mean_squared_error: 0.1426\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2290 - mean_squared_error: 0.1075\n",
            "Epoch 136: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2290 - mean_squared_error: 0.1075 - val_loss: 0.2835 - val_mean_squared_error: 0.1483\n",
            "Epoch 137/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2351 - mean_squared_error: 0.1103\n",
            "Epoch 137: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2332 - mean_squared_error: 0.1089 - val_loss: 0.2755 - val_mean_squared_error: 0.1464\n",
            "Epoch 138/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2392 - mean_squared_error: 0.1099\n",
            "Epoch 138: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2351 - mean_squared_error: 0.1109 - val_loss: 0.2857 - val_mean_squared_error: 0.1476\n",
            "Epoch 139/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2457 - mean_squared_error: 0.1223\n",
            "Epoch 139: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2389 - mean_squared_error: 0.1152 - val_loss: 0.2769 - val_mean_squared_error: 0.1457\n",
            "Epoch 140/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2366 - mean_squared_error: 0.1105\n",
            "Epoch 140: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2325 - mean_squared_error: 0.1059 - val_loss: 0.2668 - val_mean_squared_error: 0.1384\n",
            "Epoch 141/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2330 - mean_squared_error: 0.1070\n",
            "Epoch 141: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2319 - mean_squared_error: 0.1077 - val_loss: 0.2808 - val_mean_squared_error: 0.1456\n",
            "Epoch 142/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2373 - mean_squared_error: 0.1123\n",
            "Epoch 142: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2265 - mean_squared_error: 0.1045 - val_loss: 0.2660 - val_mean_squared_error: 0.1431\n",
            "Epoch 143/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2230 - mean_squared_error: 0.1027\n",
            "Epoch 143: val_loss did not improve from 0.26568\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2212 - mean_squared_error: 0.1015 - val_loss: 0.2686 - val_mean_squared_error: 0.1394\n",
            "Epoch 144/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2286 - mean_squared_error: 0.1071\n",
            "Epoch 144: val_loss improved from 0.26568 to 0.25730, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2299 - mean_squared_error: 0.1065 - val_loss: 0.2573 - val_mean_squared_error: 0.1348\n",
            "Epoch 145/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2274 - mean_squared_error: 0.1018\n",
            "Epoch 145: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2264 - mean_squared_error: 0.1033 - val_loss: 0.2623 - val_mean_squared_error: 0.1398\n",
            "Epoch 146/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2316 - mean_squared_error: 0.1124\n",
            "Epoch 146: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2256 - mean_squared_error: 0.1055 - val_loss: 0.2845 - val_mean_squared_error: 0.1475\n",
            "Epoch 147/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2270 - mean_squared_error: 0.1065\n",
            "Epoch 147: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2343 - mean_squared_error: 0.1086 - val_loss: 0.2809 - val_mean_squared_error: 0.1456\n",
            "Epoch 148/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2364 - mean_squared_error: 0.1080\n",
            "Epoch 148: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.2316 - mean_squared_error: 0.1054 - val_loss: 0.2674 - val_mean_squared_error: 0.1390\n",
            "Epoch 149/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2355 - mean_squared_error: 0.1126\n",
            "Epoch 149: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2345 - mean_squared_error: 0.1101 - val_loss: 0.2831 - val_mean_squared_error: 0.1468\n",
            "Epoch 150/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2381 - mean_squared_error: 0.1115\n",
            "Epoch 150: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2327 - mean_squared_error: 0.1070 - val_loss: 0.2749 - val_mean_squared_error: 0.1426\n",
            "Epoch 151/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2033 - mean_squared_error: 0.0824\n",
            "Epoch 151: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2220 - mean_squared_error: 0.1027 - val_loss: 0.2704 - val_mean_squared_error: 0.1423\n",
            "Epoch 152/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2266 - mean_squared_error: 0.1101\n",
            "Epoch 152: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2285 - mean_squared_error: 0.1062 - val_loss: 0.2747 - val_mean_squared_error: 0.1406\n",
            "Epoch 153/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2292 - mean_squared_error: 0.1011\n",
            "Epoch 153: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2312 - mean_squared_error: 0.1061 - val_loss: 0.2629 - val_mean_squared_error: 0.1377\n",
            "Epoch 154/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2136 - mean_squared_error: 0.0933\n",
            "Epoch 154: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2202 - mean_squared_error: 0.1009 - val_loss: 0.2684 - val_mean_squared_error: 0.1417\n",
            "Epoch 155/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2234 - mean_squared_error: 0.1057\n",
            "Epoch 155: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2205 - mean_squared_error: 0.1013 - val_loss: 0.2642 - val_mean_squared_error: 0.1394\n",
            "Epoch 156/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2122 - mean_squared_error: 0.0962\n",
            "Epoch 156: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2168 - mean_squared_error: 0.1003 - val_loss: 0.2645 - val_mean_squared_error: 0.1372\n",
            "Epoch 157/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2218 - mean_squared_error: 0.1013\n",
            "Epoch 157: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2194 - mean_squared_error: 0.1012 - val_loss: 0.2665 - val_mean_squared_error: 0.1393\n",
            "Epoch 158/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2330 - mean_squared_error: 0.1092\n",
            "Epoch 158: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2369 - mean_squared_error: 0.1103 - val_loss: 0.2952 - val_mean_squared_error: 0.1560\n",
            "Epoch 159/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2325 - mean_squared_error: 0.1079\n",
            "Epoch 159: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2415 - mean_squared_error: 0.1134 - val_loss: 0.2834 - val_mean_squared_error: 0.1454\n",
            "Epoch 160/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2234 - mean_squared_error: 0.1044\n",
            "Epoch 160: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2267 - mean_squared_error: 0.1063 - val_loss: 0.2608 - val_mean_squared_error: 0.1374\n",
            "Epoch 161/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2184 - mean_squared_error: 0.0961\n",
            "Epoch 161: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2235 - mean_squared_error: 0.1014 - val_loss: 0.2725 - val_mean_squared_error: 0.1428\n",
            "Epoch 162/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2079 - mean_squared_error: 0.0915\n",
            "Epoch 162: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2207 - mean_squared_error: 0.1027 - val_loss: 0.2757 - val_mean_squared_error: 0.1413\n",
            "Epoch 163/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2145 - mean_squared_error: 0.0913\n",
            "Epoch 163: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2265 - mean_squared_error: 0.1036 - val_loss: 0.2581 - val_mean_squared_error: 0.1355\n",
            "Epoch 164/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2220 - mean_squared_error: 0.0990\n",
            "Epoch 164: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2188 - mean_squared_error: 0.0989 - val_loss: 0.2603 - val_mean_squared_error: 0.1371\n",
            "Epoch 165/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2110 - mean_squared_error: 0.0956\n",
            "Epoch 165: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2167 - mean_squared_error: 0.0988 - val_loss: 0.2676 - val_mean_squared_error: 0.1413\n",
            "Epoch 166/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2181 - mean_squared_error: 0.1051\n",
            "Epoch 166: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2138 - mean_squared_error: 0.0987 - val_loss: 0.2601 - val_mean_squared_error: 0.1367\n",
            "Epoch 167/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2253 - mean_squared_error: 0.1055\n",
            "Epoch 167: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2213 - mean_squared_error: 0.1014 - val_loss: 0.2593 - val_mean_squared_error: 0.1341\n",
            "Epoch 168/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2354 - mean_squared_error: 0.1081\n",
            "Epoch 168: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2253 - mean_squared_error: 0.1010 - val_loss: 0.2627 - val_mean_squared_error: 0.1367\n",
            "Epoch 169/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2273 - mean_squared_error: 0.1084\n",
            "Epoch 169: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2197 - mean_squared_error: 0.1019 - val_loss: 0.2697 - val_mean_squared_error: 0.1393\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2129 - mean_squared_error: 0.0974\n",
            "Epoch 170: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2129 - mean_squared_error: 0.0974 - val_loss: 0.2612 - val_mean_squared_error: 0.1360\n",
            "Epoch 171/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2086 - mean_squared_error: 0.0872\n",
            "Epoch 171: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2204 - mean_squared_error: 0.0983 - val_loss: 0.2595 - val_mean_squared_error: 0.1339\n",
            "Epoch 172/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2149 - mean_squared_error: 0.1005\n",
            "Epoch 172: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2151 - mean_squared_error: 0.0982 - val_loss: 0.2613 - val_mean_squared_error: 0.1344\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2139 - mean_squared_error: 0.0971\n",
            "Epoch 173: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2139 - mean_squared_error: 0.0971 - val_loss: 0.2659 - val_mean_squared_error: 0.1373\n",
            "Epoch 174/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2232 - mean_squared_error: 0.1002\n",
            "Epoch 174: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2219 - mean_squared_error: 0.1011 - val_loss: 0.2703 - val_mean_squared_error: 0.1364\n",
            "Epoch 175/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2087 - mean_squared_error: 0.0986\n",
            "Epoch 175: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2124 - mean_squared_error: 0.0970 - val_loss: 0.2615 - val_mean_squared_error: 0.1360\n",
            "Epoch 176/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2165 - mean_squared_error: 0.0984\n",
            "Epoch 176: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2171 - mean_squared_error: 0.0976 - val_loss: 0.2607 - val_mean_squared_error: 0.1324\n",
            "Epoch 177/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2174 - mean_squared_error: 0.0951\n",
            "Epoch 177: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2125 - mean_squared_error: 0.0959 - val_loss: 0.2636 - val_mean_squared_error: 0.1359\n",
            "Epoch 178/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2095 - mean_squared_error: 0.0888\n",
            "Epoch 178: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2135 - mean_squared_error: 0.0970 - val_loss: 0.2719 - val_mean_squared_error: 0.1394\n",
            "Epoch 179/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2043 - mean_squared_error: 0.0942\n",
            "Epoch 179: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2130 - mean_squared_error: 0.0965 - val_loss: 0.2666 - val_mean_squared_error: 0.1364\n",
            "Epoch 180/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2198 - mean_squared_error: 0.0992\n",
            "Epoch 180: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2194 - mean_squared_error: 0.1001 - val_loss: 0.2673 - val_mean_squared_error: 0.1372\n",
            "Epoch 181/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2071 - mean_squared_error: 0.0945\n",
            "Epoch 181: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2109 - mean_squared_error: 0.0949 - val_loss: 0.2711 - val_mean_squared_error: 0.1383\n",
            "Epoch 182/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.1992 - mean_squared_error: 0.0804\n",
            "Epoch 182: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2112 - mean_squared_error: 0.0954 - val_loss: 0.2627 - val_mean_squared_error: 0.1351\n",
            "Epoch 183/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2140 - mean_squared_error: 0.1005\n",
            "Epoch 183: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2185 - mean_squared_error: 0.0985 - val_loss: 0.2697 - val_mean_squared_error: 0.1377\n",
            "Epoch 184/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2310 - mean_squared_error: 0.1054\n",
            "Epoch 184: val_loss did not improve from 0.25730\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2269 - mean_squared_error: 0.1011 - val_loss: 0.2754 - val_mean_squared_error: 0.1405\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "Fold: 6\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 6\n",
            "Epoch 1/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 9.1222 - mean_squared_error: 181.0698   \n",
            "Epoch 1: val_loss improved from inf to 4.97976, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 38s 98ms/step - loss: 8.2005 - mean_squared_error: 159.3440 - val_loss: 4.9798 - val_mean_squared_error: 39.9257\n",
            "Epoch 2/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 2.8818 - mean_squared_error: 28.4170\n",
            "Epoch 2: val_loss improved from 4.97976 to 2.47475, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 2.8798 - mean_squared_error: 26.5147 - val_loss: 2.4747 - val_mean_squared_error: 8.2371\n",
            "Epoch 3/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 1.7511 - mean_squared_error: 7.4197 \n",
            "Epoch 3: val_loss improved from 2.47475 to 1.05450, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 1.7039 - mean_squared_error: 7.3396 - val_loss: 1.0545 - val_mean_squared_error: 2.4396\n",
            "Epoch 4/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.8032 - mean_squared_error: 1.3558\n",
            "Epoch 4: val_loss improved from 1.05450 to 1.02146, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.8353 - mean_squared_error: 1.5547 - val_loss: 1.0215 - val_mean_squared_error: 1.7467\n",
            "Epoch 5/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6919 - mean_squared_error: 0.8928\n",
            "Epoch 5: val_loss improved from 1.02146 to 0.78098, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.7179 - mean_squared_error: 0.9508 - val_loss: 0.7810 - val_mean_squared_error: 1.1482\n",
            "Epoch 6/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7139 - mean_squared_error: 0.9043\n",
            "Epoch 6: val_loss improved from 0.78098 to 0.73320, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.6926 - mean_squared_error: 0.8435 - val_loss: 0.7332 - val_mean_squared_error: 1.0760\n",
            "Epoch 7/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6238 - mean_squared_error: 0.7601\n",
            "Epoch 7: val_loss did not improve from 0.73320\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6196 - mean_squared_error: 0.7322 - val_loss: 0.7480 - val_mean_squared_error: 1.0606\n",
            "Epoch 8/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5342 - mean_squared_error: 0.6156\n",
            "Epoch 8: val_loss improved from 0.73320 to 0.63057, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.5292 - mean_squared_error: 0.5960 - val_loss: 0.6306 - val_mean_squared_error: 0.8224\n",
            "Epoch 9/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5408 - mean_squared_error: 0.6569\n",
            "Epoch 9: val_loss improved from 0.63057 to 0.62139, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.5132 - mean_squared_error: 0.6024 - val_loss: 0.6214 - val_mean_squared_error: 0.7603\n",
            "Epoch 10/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4755 - mean_squared_error: 0.5222\n",
            "Epoch 10: val_loss improved from 0.62139 to 0.58229, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.4746 - mean_squared_error: 0.5280 - val_loss: 0.5823 - val_mean_squared_error: 0.6847\n",
            "Epoch 11/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3804 - mean_squared_error: 0.3540\n",
            "Epoch 11: val_loss did not improve from 0.58229\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4197 - mean_squared_error: 0.4215 - val_loss: 0.5907 - val_mean_squared_error: 0.6602\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3989 - mean_squared_error: 0.3644\n",
            "Epoch 12: val_loss improved from 0.58229 to 0.49654, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3977 - mean_squared_error: 0.3603 - val_loss: 0.4965 - val_mean_squared_error: 0.5324\n",
            "Epoch 13/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3388 - mean_squared_error: 0.2975\n",
            "Epoch 13: val_loss did not improve from 0.49654\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3438 - mean_squared_error: 0.2961 - val_loss: 0.5179 - val_mean_squared_error: 0.5014\n",
            "Epoch 14/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3810 - mean_squared_error: 0.3289\n",
            "Epoch 14: val_loss did not improve from 0.49654\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3702 - mean_squared_error: 0.3050 - val_loss: 0.6311 - val_mean_squared_error: 0.5880\n",
            "Epoch 15/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3704 - mean_squared_error: 0.2911\n",
            "Epoch 15: val_loss improved from 0.49654 to 0.43084, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3760 - mean_squared_error: 0.3104 - val_loss: 0.4308 - val_mean_squared_error: 0.3871\n",
            "Epoch 16/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3266 - mean_squared_error: 0.2418\n",
            "Epoch 16: val_loss improved from 0.43084 to 0.40571, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3206 - mean_squared_error: 0.2435 - val_loss: 0.4057 - val_mean_squared_error: 0.3525\n",
            "Epoch 17/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3047 - mean_squared_error: 0.2393\n",
            "Epoch 17: val_loss did not improve from 0.40571\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3026 - mean_squared_error: 0.2257 - val_loss: 0.4164 - val_mean_squared_error: 0.3420\n",
            "Epoch 18/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3143 - mean_squared_error: 0.2307\n",
            "Epoch 18: val_loss improved from 0.40571 to 0.38302, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.3081 - mean_squared_error: 0.2286 - val_loss: 0.3830 - val_mean_squared_error: 0.2993\n",
            "Epoch 19/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3108 - mean_squared_error: 0.2317\n",
            "Epoch 19: val_loss improved from 0.38302 to 0.36140, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.3278 - mean_squared_error: 0.2306 - val_loss: 0.3614 - val_mean_squared_error: 0.2913\n",
            "Epoch 20/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3129 - mean_squared_error: 0.2346\n",
            "Epoch 20: val_loss improved from 0.36140 to 0.35539, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.2829 - mean_squared_error: 0.1925 - val_loss: 0.3554 - val_mean_squared_error: 0.2545\n",
            "Epoch 21/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2821 - mean_squared_error: 0.1784\n",
            "Epoch 21: val_loss did not improve from 0.35539\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2889 - mean_squared_error: 0.1920 - val_loss: 0.3812 - val_mean_squared_error: 0.2691\n",
            "Epoch 22/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2507 - mean_squared_error: 0.1433\n",
            "Epoch 22: val_loss did not improve from 0.35539\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2749 - mean_squared_error: 0.1796 - val_loss: 0.4257 - val_mean_squared_error: 0.2857\n",
            "Epoch 23/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2992 - mean_squared_error: 0.2000\n",
            "Epoch 23: val_loss did not improve from 0.35539\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2911 - mean_squared_error: 0.1837 - val_loss: 0.4569 - val_mean_squared_error: 0.3077\n",
            "Epoch 24/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3012 - mean_squared_error: 0.1651\n",
            "Epoch 24: val_loss did not improve from 0.35539\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3853 - mean_squared_error: 0.3132 - val_loss: 0.3721 - val_mean_squared_error: 0.2465\n",
            "Epoch 25/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3277 - mean_squared_error: 0.2014\n",
            "Epoch 25: val_loss did not improve from 0.35539\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.3481 - mean_squared_error: 0.2274 - val_loss: 0.4005 - val_mean_squared_error: 0.2834\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3517 - mean_squared_error: 0.2577\n",
            "Epoch 26: val_loss improved from 0.35539 to 0.32665, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.3240 - mean_squared_error: 0.2152 - val_loss: 0.3266 - val_mean_squared_error: 0.2226\n",
            "Epoch 27/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3239 - mean_squared_error: 0.2010\n",
            "Epoch 27: val_loss did not improve from 0.32665\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.3184 - mean_squared_error: 0.1906 - val_loss: 0.3421 - val_mean_squared_error: 0.2126\n",
            "Epoch 28/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3485 - mean_squared_error: 0.2742\n",
            "Epoch 28: val_loss did not improve from 0.32665\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3349 - mean_squared_error: 0.2452 - val_loss: 0.3723 - val_mean_squared_error: 0.2242\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2890 - mean_squared_error: 0.2110\n",
            "Epoch 29: val_loss did not improve from 0.32665\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2744 - mean_squared_error: 0.1799 - val_loss: 0.3548 - val_mean_squared_error: 0.2184\n",
            "Epoch 30/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2656 - mean_squared_error: 0.1586\n",
            "Epoch 30: val_loss did not improve from 0.32665\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2622 - mean_squared_error: 0.1575 - val_loss: 0.3725 - val_mean_squared_error: 0.2308\n",
            "Epoch 31/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2606 - mean_squared_error: 0.1637\n",
            "Epoch 31: val_loss improved from 0.32665 to 0.30464, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 0.2582 - mean_squared_error: 0.1533 - val_loss: 0.3046 - val_mean_squared_error: 0.1991\n",
            "Epoch 32/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.1868 - mean_squared_error: 0.0577\n",
            "Epoch 32: val_loss improved from 0.30464 to 0.29433, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.2531 - mean_squared_error: 0.1410 - val_loss: 0.2943 - val_mean_squared_error: 0.1788\n",
            "Epoch 33/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.1628\n",
            "Epoch 33: val_loss did not improve from 0.29433\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2436 - mean_squared_error: 0.1401 - val_loss: 0.3264 - val_mean_squared_error: 0.1895\n",
            "Epoch 34/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2154 - mean_squared_error: 0.0897\n",
            "Epoch 34: val_loss improved from 0.29433 to 0.28194, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.2353 - mean_squared_error: 0.1305 - val_loss: 0.2819 - val_mean_squared_error: 0.1685\n",
            "Epoch 35/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1628\n",
            "Epoch 35: val_loss did not improve from 0.28194\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2556 - mean_squared_error: 0.1444 - val_loss: 0.2826 - val_mean_squared_error: 0.1761\n",
            "Epoch 36/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2585 - mean_squared_error: 0.1316\n",
            "Epoch 36: val_loss did not improve from 0.28194\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2454 - mean_squared_error: 0.1319 - val_loss: 0.2855 - val_mean_squared_error: 0.1805\n",
            "Epoch 37/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2161 - mean_squared_error: 0.1045\n",
            "Epoch 37: val_loss did not improve from 0.28194\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2395 - mean_squared_error: 0.1327 - val_loss: 0.2919 - val_mean_squared_error: 0.1706\n",
            "Epoch 38/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2657 - mean_squared_error: 0.1584\n",
            "Epoch 38: val_loss did not improve from 0.28194\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2582 - mean_squared_error: 0.1414 - val_loss: 0.3006 - val_mean_squared_error: 0.1742\n",
            "Epoch 39/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2379 - mean_squared_error: 0.1315\n",
            "Epoch 39: val_loss improved from 0.28194 to 0.27282, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.2570 - mean_squared_error: 0.1403 - val_loss: 0.2728 - val_mean_squared_error: 0.1444\n",
            "Epoch 40/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2570 - mean_squared_error: 0.1379\n",
            "Epoch 40: val_loss did not improve from 0.27282\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2567 - mean_squared_error: 0.1336 - val_loss: 0.3001 - val_mean_squared_error: 0.1786\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2452 - mean_squared_error: 0.1280\n",
            "Epoch 41: val_loss improved from 0.27282 to 0.26209, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2497 - mean_squared_error: 0.1336 - val_loss: 0.2621 - val_mean_squared_error: 0.1440\n",
            "Epoch 42/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2435 - mean_squared_error: 0.1272\n",
            "Epoch 42: val_loss improved from 0.26209 to 0.26142, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.2302 - mean_squared_error: 0.1177 - val_loss: 0.2614 - val_mean_squared_error: 0.1391\n",
            "Epoch 43/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2195 - mean_squared_error: 0.1152\n",
            "Epoch 43: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2136 - mean_squared_error: 0.1097 - val_loss: 0.2711 - val_mean_squared_error: 0.1445\n",
            "Epoch 44/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2104 - mean_squared_error: 0.0941\n",
            "Epoch 44: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2328 - mean_squared_error: 0.1181 - val_loss: 0.2684 - val_mean_squared_error: 0.1505\n",
            "Epoch 45/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2417 - mean_squared_error: 0.0917\n",
            "Epoch 45: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2574 - mean_squared_error: 0.1246 - val_loss: 0.2868 - val_mean_squared_error: 0.1521\n",
            "Epoch 46/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2152 - mean_squared_error: 0.0985\n",
            "Epoch 46: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2285 - mean_squared_error: 0.1158 - val_loss: 0.3524 - val_mean_squared_error: 0.2020\n",
            "Epoch 47/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2403 - mean_squared_error: 0.1214\n",
            "Epoch 47: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2440 - mean_squared_error: 0.1246 - val_loss: 0.3123 - val_mean_squared_error: 0.1599\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2193 - mean_squared_error: 0.1074\n",
            "Epoch 48: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2193 - mean_squared_error: 0.1074 - val_loss: 0.2631 - val_mean_squared_error: 0.1244\n",
            "Epoch 49/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2283 - mean_squared_error: 0.1159\n",
            "Epoch 49: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2222 - mean_squared_error: 0.1078 - val_loss: 0.3344 - val_mean_squared_error: 0.1789\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2362 - mean_squared_error: 0.1200\n",
            "Epoch 50: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2453 - mean_squared_error: 0.1254 - val_loss: 0.2783 - val_mean_squared_error: 0.1370\n",
            "Epoch 51/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2178 - mean_squared_error: 0.1028\n",
            "Epoch 51: val_loss did not improve from 0.26142\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2193 - mean_squared_error: 0.1069 - val_loss: 0.2934 - val_mean_squared_error: 0.1519\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.1091\n",
            "Epoch 52: val_loss improved from 0.26142 to 0.25231, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2248 - mean_squared_error: 0.1091 - val_loss: 0.2523 - val_mean_squared_error: 0.1131\n",
            "Epoch 53/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2110 - mean_squared_error: 0.1013\n",
            "Epoch 53: val_loss improved from 0.25231 to 0.24863, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.2118 - mean_squared_error: 0.1003 - val_loss: 0.2486 - val_mean_squared_error: 0.1277\n",
            "Epoch 54/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2159 - mean_squared_error: 0.1071\n",
            "Epoch 54: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2064 - mean_squared_error: 0.0986 - val_loss: 0.2512 - val_mean_squared_error: 0.1273\n",
            "Epoch 55/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2207 - mean_squared_error: 0.0946\n",
            "Epoch 55: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2218 - mean_squared_error: 0.1018 - val_loss: 0.2689 - val_mean_squared_error: 0.1239\n",
            "Epoch 56/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.1019\n",
            "Epoch 56: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2181 - mean_squared_error: 0.0991 - val_loss: 0.2701 - val_mean_squared_error: 0.1225\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2071 - mean_squared_error: 0.0933\n",
            "Epoch 57: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2071 - mean_squared_error: 0.0933 - val_loss: 0.2539 - val_mean_squared_error: 0.1087\n",
            "Epoch 58/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1999 - mean_squared_error: 0.0942\n",
            "Epoch 58: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1972 - mean_squared_error: 0.0889 - val_loss: 0.2535 - val_mean_squared_error: 0.1159\n",
            "Epoch 59/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2026 - mean_squared_error: 0.0903\n",
            "Epoch 59: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2067 - mean_squared_error: 0.0913 - val_loss: 0.2604 - val_mean_squared_error: 0.1224\n",
            "Epoch 60/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.1226\n",
            "Epoch 60: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2463 - mean_squared_error: 0.1159 - val_loss: 0.3418 - val_mean_squared_error: 0.1797\n",
            "Epoch 61/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2732 - mean_squared_error: 0.1423\n",
            "Epoch 61: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2637 - mean_squared_error: 0.1342 - val_loss: 0.2996 - val_mean_squared_error: 0.1421\n",
            "Epoch 62/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2203 - mean_squared_error: 0.1036\n",
            "Epoch 62: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2308 - mean_squared_error: 0.1091 - val_loss: 0.2905 - val_mean_squared_error: 0.1346\n",
            "Epoch 63/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2374 - mean_squared_error: 0.1117\n",
            "Epoch 63: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2330 - mean_squared_error: 0.1054 - val_loss: 0.2778 - val_mean_squared_error: 0.1237\n",
            "Epoch 64/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2061 - mean_squared_error: 0.0835\n",
            "Epoch 64: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2106 - mean_squared_error: 0.0897 - val_loss: 0.2669 - val_mean_squared_error: 0.1216\n",
            "Epoch 65/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1974 - mean_squared_error: 0.0845\n",
            "Epoch 65: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1994 - mean_squared_error: 0.0846 - val_loss: 0.2503 - val_mean_squared_error: 0.1084\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1974 - mean_squared_error: 0.0882\n",
            "Epoch 66: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1974 - mean_squared_error: 0.0882 - val_loss: 0.2646 - val_mean_squared_error: 0.1068\n",
            "Epoch 67/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2213 - mean_squared_error: 0.0954\n",
            "Epoch 67: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2226 - mean_squared_error: 0.0973 - val_loss: 0.2799 - val_mean_squared_error: 0.1394\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2243 - mean_squared_error: 0.1025\n",
            "Epoch 68: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2243 - mean_squared_error: 0.1025 - val_loss: 0.2708 - val_mean_squared_error: 0.1230\n",
            "Epoch 69/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2141 - mean_squared_error: 0.0956\n",
            "Epoch 69: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2142 - mean_squared_error: 0.0927 - val_loss: 0.2519 - val_mean_squared_error: 0.1209\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2031 - mean_squared_error: 0.0965\n",
            "Epoch 70: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2015 - mean_squared_error: 0.0914 - val_loss: 0.2554 - val_mean_squared_error: 0.1164\n",
            "Epoch 71/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2084 - mean_squared_error: 0.0929\n",
            "Epoch 71: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1946 - mean_squared_error: 0.0833 - val_loss: 0.2581 - val_mean_squared_error: 0.1117\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1947 - mean_squared_error: 0.0861\n",
            "Epoch 72: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1947 - mean_squared_error: 0.0861 - val_loss: 0.2568 - val_mean_squared_error: 0.1057\n",
            "Epoch 73/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2012 - mean_squared_error: 0.0917\n",
            "Epoch 73: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1877 - mean_squared_error: 0.0774 - val_loss: 0.2540 - val_mean_squared_error: 0.1153\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1980 - mean_squared_error: 0.0836\n",
            "Epoch 74: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1980 - mean_squared_error: 0.0836 - val_loss: 0.2697 - val_mean_squared_error: 0.1216\n",
            "Epoch 75/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2045 - mean_squared_error: 0.0917\n",
            "Epoch 75: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2047 - mean_squared_error: 0.0900 - val_loss: 0.2523 - val_mean_squared_error: 0.1003\n",
            "Epoch 76/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1886 - mean_squared_error: 0.0786\n",
            "Epoch 76: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1971 - mean_squared_error: 0.0856 - val_loss: 0.2598 - val_mean_squared_error: 0.1175\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1924 - mean_squared_error: 0.0816\n",
            "Epoch 77: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1924 - mean_squared_error: 0.0816 - val_loss: 0.2564 - val_mean_squared_error: 0.1099\n",
            "Epoch 78/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.1786 - mean_squared_error: 0.0621\n",
            "Epoch 78: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2002 - mean_squared_error: 0.0819 - val_loss: 0.2519 - val_mean_squared_error: 0.1066\n",
            "Epoch 79/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.1900 - mean_squared_error: 0.0729\n",
            "Epoch 79: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.1921 - mean_squared_error: 0.0813 - val_loss: 0.2541 - val_mean_squared_error: 0.1077\n",
            "Epoch 80/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1820 - mean_squared_error: 0.0781\n",
            "Epoch 80: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1759 - mean_squared_error: 0.0735 - val_loss: 0.2617 - val_mean_squared_error: 0.1147\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1803 - mean_squared_error: 0.0736\n",
            "Epoch 81: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.1803 - mean_squared_error: 0.0736 - val_loss: 0.2695 - val_mean_squared_error: 0.1199\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2040 - mean_squared_error: 0.0882\n",
            "Epoch 82: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2040 - mean_squared_error: 0.0882 - val_loss: 0.2636 - val_mean_squared_error: 0.1154\n",
            "Epoch 83/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.0971\n",
            "Epoch 83: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2271 - mean_squared_error: 0.1032 - val_loss: 0.2882 - val_mean_squared_error: 0.1350\n",
            "Epoch 84/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2423 - mean_squared_error: 0.1035\n",
            "Epoch 84: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2426 - mean_squared_error: 0.1065 - val_loss: 0.2587 - val_mean_squared_error: 0.1095\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2071 - mean_squared_error: 0.0858\n",
            "Epoch 85: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2071 - mean_squared_error: 0.0858 - val_loss: 0.2763 - val_mean_squared_error: 0.1265\n",
            "Epoch 86/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2111 - mean_squared_error: 0.0891\n",
            "Epoch 86: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2027 - mean_squared_error: 0.0831 - val_loss: 0.2637 - val_mean_squared_error: 0.1171\n",
            "Epoch 87/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1813 - mean_squared_error: 0.0720\n",
            "Epoch 87: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1845 - mean_squared_error: 0.0737 - val_loss: 0.2599 - val_mean_squared_error: 0.1076\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.1940 - mean_squared_error: 0.0787\n",
            "Epoch 88: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1940 - mean_squared_error: 0.0787 - val_loss: 0.3040 - val_mean_squared_error: 0.1450\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2237 - mean_squared_error: 0.0952\n",
            "Epoch 89: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2237 - mean_squared_error: 0.0952 - val_loss: 0.2763 - val_mean_squared_error: 0.1246\n",
            "Epoch 90/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2087 - mean_squared_error: 0.0870\n",
            "Epoch 90: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2065 - mean_squared_error: 0.0835 - val_loss: 0.2803 - val_mean_squared_error: 0.1263\n",
            "Epoch 91/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2267 - mean_squared_error: 0.0990\n",
            "Epoch 91: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2217 - mean_squared_error: 0.0941 - val_loss: 0.3151 - val_mean_squared_error: 0.1484\n",
            "Epoch 92/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2265 - mean_squared_error: 0.1000\n",
            "Epoch 92: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2269 - mean_squared_error: 0.0974 - val_loss: 0.2997 - val_mean_squared_error: 0.1366\n",
            "Epoch 93/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.1829 - mean_squared_error: 0.0732\n",
            "Epoch 93: val_loss did not improve from 0.24863\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.1868 - mean_squared_error: 0.0769 - val_loss: 0.2497 - val_mean_squared_error: 0.0943\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "Fold: 7\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 7\n",
            "Epoch 1/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 1.2791 - mean_squared_error: 2.0511 \n",
            "Epoch 1: val_loss improved from inf to 1.13389, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 4s 119ms/step - loss: 1.2628 - mean_squared_error: 1.9977 - val_loss: 1.1339 - val_mean_squared_error: 1.7202\n",
            "Epoch 2/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 1.0931 - mean_squared_error: 1.8446\n",
            "Epoch 2: val_loss improved from 1.13389 to 0.90561, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 1.0912 - mean_squared_error: 1.9172 - val_loss: 0.9056 - val_mean_squared_error: 1.2990\n",
            "Epoch 3/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 1.0305 - mean_squared_error: 1.8084\n",
            "Epoch 3: val_loss improved from 0.90561 to 0.78896, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.9988 - mean_squared_error: 1.8279 - val_loss: 0.7890 - val_mean_squared_error: 1.0752\n",
            "Epoch 4/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.8639 - mean_squared_error: 1.2152\n",
            "Epoch 4: val_loss improved from 0.78896 to 0.70767, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.8905 - mean_squared_error: 1.4030 - val_loss: 0.7077 - val_mean_squared_error: 0.9335\n",
            "Epoch 5/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.7696 - mean_squared_error: 0.9532\n",
            "Epoch 5: val_loss improved from 0.70767 to 0.63930, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.7918 - mean_squared_error: 1.0577 - val_loss: 0.6393 - val_mean_squared_error: 0.7982\n",
            "Epoch 6/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.7488 - mean_squared_error: 0.9334\n",
            "Epoch 6: val_loss improved from 0.63930 to 0.58327, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.7122 - mean_squared_error: 0.8613 - val_loss: 0.5833 - val_mean_squared_error: 0.7033\n",
            "Epoch 7/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.6635 - mean_squared_error: 0.8154\n",
            "Epoch 7: val_loss improved from 0.58327 to 0.46694, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.6403 - mean_squared_error: 0.7441 - val_loss: 0.4669 - val_mean_squared_error: 0.6028\n",
            "Epoch 8/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.6079 - mean_squared_error: 0.7638\n",
            "Epoch 8: val_loss improved from 0.46694 to 0.45941, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.5912 - mean_squared_error: 0.7057 - val_loss: 0.4594 - val_mean_squared_error: 0.5879\n",
            "Epoch 9/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5648 - mean_squared_error: 0.6634\n",
            "Epoch 9: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.5566 - mean_squared_error: 0.6280 - val_loss: 0.4768 - val_mean_squared_error: 0.5939\n",
            "Epoch 10/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5014 - mean_squared_error: 0.5373\n",
            "Epoch 10: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.5348 - mean_squared_error: 0.5902 - val_loss: 0.4943 - val_mean_squared_error: 0.5986\n",
            "Epoch 11/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4496 - mean_squared_error: 0.4275\n",
            "Epoch 11: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.5181 - mean_squared_error: 0.5517 - val_loss: 0.5020 - val_mean_squared_error: 0.5905\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5189 - mean_squared_error: 0.5255\n",
            "Epoch 12: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.5195 - mean_squared_error: 0.5476 - val_loss: 0.4884 - val_mean_squared_error: 0.5870\n",
            "Epoch 13/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4763 - mean_squared_error: 0.4848\n",
            "Epoch 13: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4930 - mean_squared_error: 0.5189 - val_loss: 0.4928 - val_mean_squared_error: 0.5635\n",
            "Epoch 14/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5032 - mean_squared_error: 0.5230\n",
            "Epoch 14: val_loss did not improve from 0.45941\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.5212 - mean_squared_error: 0.5295 - val_loss: 0.4780 - val_mean_squared_error: 0.5498\n",
            "Epoch 15/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4878 - mean_squared_error: 0.5119\n",
            "Epoch 15: val_loss improved from 0.45941 to 0.45874, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.4859 - mean_squared_error: 0.4973 - val_loss: 0.4587 - val_mean_squared_error: 0.5495\n",
            "Epoch 16/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4646 - mean_squared_error: 0.4397\n",
            "Epoch 16: val_loss improved from 0.45874 to 0.45455, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.4631 - mean_squared_error: 0.4620 - val_loss: 0.4545 - val_mean_squared_error: 0.5210\n",
            "Epoch 17/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4275 - mean_squared_error: 0.4230\n",
            "Epoch 17: val_loss improved from 0.45455 to 0.44624, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.4531 - mean_squared_error: 0.4423 - val_loss: 0.4462 - val_mean_squared_error: 0.5098\n",
            "Epoch 18/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4521 - mean_squared_error: 0.4370\n",
            "Epoch 18: val_loss improved from 0.44624 to 0.43810, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.4447 - mean_squared_error: 0.4280 - val_loss: 0.4381 - val_mean_squared_error: 0.4936\n",
            "Epoch 19/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4400 - mean_squared_error: 0.4239\n",
            "Epoch 19: val_loss improved from 0.43810 to 0.42060, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.4336 - mean_squared_error: 0.4148 - val_loss: 0.4206 - val_mean_squared_error: 0.4888\n",
            "Epoch 20/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.4611 - mean_squared_error: 0.4374\n",
            "Epoch 20: val_loss did not improve from 0.42060\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4381 - mean_squared_error: 0.4205 - val_loss: 0.4251 - val_mean_squared_error: 0.4657\n",
            "Epoch 21/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4263 - mean_squared_error: 0.3880\n",
            "Epoch 21: val_loss improved from 0.42060 to 0.41490, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.4247 - mean_squared_error: 0.3869 - val_loss: 0.4149 - val_mean_squared_error: 0.4587\n",
            "Epoch 22/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3795 - mean_squared_error: 0.2901\n",
            "Epoch 22: val_loss improved from 0.41490 to 0.40704, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.4136 - mean_squared_error: 0.3779 - val_loss: 0.4070 - val_mean_squared_error: 0.4476\n",
            "Epoch 23/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3868 - mean_squared_error: 0.3285\n",
            "Epoch 23: val_loss improved from 0.40704 to 0.40104, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.4023 - mean_squared_error: 0.3676 - val_loss: 0.4010 - val_mean_squared_error: 0.4480\n",
            "Epoch 24/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3734 - mean_squared_error: 0.3164\n",
            "Epoch 24: val_loss improved from 0.40104 to 0.39755, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.3948 - mean_squared_error: 0.3551 - val_loss: 0.3975 - val_mean_squared_error: 0.4305\n",
            "Epoch 25/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3650 - mean_squared_error: 0.3009\n",
            "Epoch 25: val_loss improved from 0.39755 to 0.38203, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3917 - mean_squared_error: 0.3454 - val_loss: 0.3820 - val_mean_squared_error: 0.4170\n",
            "Epoch 26/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3963 - mean_squared_error: 0.3561\n",
            "Epoch 26: val_loss improved from 0.38203 to 0.37953, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.3841 - mean_squared_error: 0.3338 - val_loss: 0.3795 - val_mean_squared_error: 0.4056\n",
            "Epoch 27/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3918 - mean_squared_error: 0.3529\n",
            "Epoch 27: val_loss improved from 0.37953 to 0.37037, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3807 - mean_squared_error: 0.3281 - val_loss: 0.3704 - val_mean_squared_error: 0.3957\n",
            "Epoch 28/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3824 - mean_squared_error: 0.3344\n",
            "Epoch 28: val_loss did not improve from 0.37037\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3723 - mean_squared_error: 0.3165 - val_loss: 0.3764 - val_mean_squared_error: 0.3892\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4003 - mean_squared_error: 0.3620\n",
            "Epoch 29: val_loss improved from 0.37037 to 0.36908, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3730 - mean_squared_error: 0.3142 - val_loss: 0.3691 - val_mean_squared_error: 0.3845\n",
            "Epoch 30/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3762 - mean_squared_error: 0.3253\n",
            "Epoch 30: val_loss improved from 0.36908 to 0.35901, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3616 - mean_squared_error: 0.3018 - val_loss: 0.3590 - val_mean_squared_error: 0.3611\n",
            "Epoch 31/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3733 - mean_squared_error: 0.3128\n",
            "Epoch 31: val_loss did not improve from 0.35901\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3733 - mean_squared_error: 0.3045 - val_loss: 0.3606 - val_mean_squared_error: 0.3640\n",
            "Epoch 32/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3306 - mean_squared_error: 0.2390\n",
            "Epoch 32: val_loss improved from 0.35901 to 0.35549, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3610 - mean_squared_error: 0.2928 - val_loss: 0.3555 - val_mean_squared_error: 0.3521\n",
            "Epoch 33/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3628 - mean_squared_error: 0.3208\n",
            "Epoch 33: val_loss did not improve from 0.35549\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3580 - mean_squared_error: 0.2896 - val_loss: 0.3561 - val_mean_squared_error: 0.3504\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3505 - mean_squared_error: 0.2775\n",
            "Epoch 34: val_loss did not improve from 0.35549\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3505 - mean_squared_error: 0.2775 - val_loss: 0.3562 - val_mean_squared_error: 0.3456\n",
            "Epoch 35/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3423 - mean_squared_error: 0.2721\n",
            "Epoch 35: val_loss improved from 0.35549 to 0.34464, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3487 - mean_squared_error: 0.2780 - val_loss: 0.3446 - val_mean_squared_error: 0.3327\n",
            "Epoch 36/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3387 - mean_squared_error: 0.2563\n",
            "Epoch 36: val_loss did not improve from 0.34464\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3454 - mean_squared_error: 0.2694 - val_loss: 0.3462 - val_mean_squared_error: 0.3315\n",
            "Epoch 37/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3571 - mean_squared_error: 0.3009\n",
            "Epoch 37: val_loss improved from 0.34464 to 0.33260, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.3440 - mean_squared_error: 0.2668 - val_loss: 0.3326 - val_mean_squared_error: 0.3156\n",
            "Epoch 38/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3331 - mean_squared_error: 0.2481\n",
            "Epoch 38: val_loss did not improve from 0.33260\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3361 - mean_squared_error: 0.2590 - val_loss: 0.3457 - val_mean_squared_error: 0.3209\n",
            "Epoch 39/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3451 - mean_squared_error: 0.2722\n",
            "Epoch 39: val_loss did not improve from 0.33260\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3358 - mean_squared_error: 0.2548 - val_loss: 0.3412 - val_mean_squared_error: 0.3132\n",
            "Epoch 40/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3440 - mean_squared_error: 0.2757\n",
            "Epoch 40: val_loss improved from 0.33260 to 0.32945, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.3324 - mean_squared_error: 0.2490 - val_loss: 0.3295 - val_mean_squared_error: 0.3007\n",
            "Epoch 41/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3322 - mean_squared_error: 0.2510\n",
            "Epoch 41: val_loss improved from 0.32945 to 0.32859, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3247 - mean_squared_error: 0.2411 - val_loss: 0.3286 - val_mean_squared_error: 0.2972\n",
            "Epoch 42/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3237 - mean_squared_error: 0.2474\n",
            "Epoch 42: val_loss did not improve from 0.32859\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3285 - mean_squared_error: 0.2431 - val_loss: 0.3342 - val_mean_squared_error: 0.2992\n",
            "Epoch 43/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3279 - mean_squared_error: 0.2349\n",
            "Epoch 43: val_loss improved from 0.32859 to 0.31939, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3262 - mean_squared_error: 0.2383 - val_loss: 0.3194 - val_mean_squared_error: 0.2855\n",
            "Epoch 44/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3173 - mean_squared_error: 0.2046\n",
            "Epoch 44: val_loss did not improve from 0.31939\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3261 - mean_squared_error: 0.2375 - val_loss: 0.3276 - val_mean_squared_error: 0.2889\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3275 - mean_squared_error: 0.2372\n",
            "Epoch 45: val_loss improved from 0.31939 to 0.31659, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3275 - mean_squared_error: 0.2372 - val_loss: 0.3166 - val_mean_squared_error: 0.2775\n",
            "Epoch 46/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3008 - mean_squared_error: 0.1863\n",
            "Epoch 46: val_loss improved from 0.31659 to 0.30645, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.3182 - mean_squared_error: 0.2302 - val_loss: 0.3064 - val_mean_squared_error: 0.2719\n",
            "Epoch 47/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2988 - mean_squared_error: 0.2119\n",
            "Epoch 47: val_loss did not improve from 0.30645\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3130 - mean_squared_error: 0.2273 - val_loss: 0.3166 - val_mean_squared_error: 0.2750\n",
            "Epoch 48/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2916 - mean_squared_error: 0.1988\n",
            "Epoch 48: val_loss did not improve from 0.30645\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3038 - mean_squared_error: 0.2169 - val_loss: 0.3098 - val_mean_squared_error: 0.2699\n",
            "Epoch 49/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3033 - mean_squared_error: 0.2199\n",
            "Epoch 49: val_loss improved from 0.30645 to 0.28653, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3007 - mean_squared_error: 0.2148 - val_loss: 0.2865 - val_mean_squared_error: 0.2550\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3050 - mean_squared_error: 0.2272\n",
            "Epoch 50: val_loss did not improve from 0.28653\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2961 - mean_squared_error: 0.2106 - val_loss: 0.3025 - val_mean_squared_error: 0.2584\n",
            "Epoch 51/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.1847\n",
            "Epoch 51: val_loss did not improve from 0.28653\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2935 - mean_squared_error: 0.2059 - val_loss: 0.2872 - val_mean_squared_error: 0.2480\n",
            "Epoch 52/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2792 - mean_squared_error: 0.1893\n",
            "Epoch 52: val_loss did not improve from 0.28653\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2978 - mean_squared_error: 0.2102 - val_loss: 0.2962 - val_mean_squared_error: 0.2492\n",
            "Epoch 53/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2849 - mean_squared_error: 0.1951\n",
            "Epoch 53: val_loss improved from 0.28653 to 0.28458, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2917 - mean_squared_error: 0.2038 - val_loss: 0.2846 - val_mean_squared_error: 0.2429\n",
            "Epoch 54/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2969 - mean_squared_error: 0.2128\n",
            "Epoch 54: val_loss did not improve from 0.28458\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2922 - mean_squared_error: 0.2042 - val_loss: 0.2923 - val_mean_squared_error: 0.2439\n",
            "Epoch 55/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3448 - mean_squared_error: 0.2612\n",
            "Epoch 55: val_loss did not improve from 0.28458\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3187 - mean_squared_error: 0.2285 - val_loss: 0.2981 - val_mean_squared_error: 0.2459\n",
            "Epoch 56/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2974 - mean_squared_error: 0.2105\n",
            "Epoch 56: val_loss improved from 0.28458 to 0.27754, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2920 - mean_squared_error: 0.2024 - val_loss: 0.2775 - val_mean_squared_error: 0.2357\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2862 - mean_squared_error: 0.1968\n",
            "Epoch 57: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2862 - mean_squared_error: 0.1968 - val_loss: 0.3013 - val_mean_squared_error: 0.2458\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2875 - mean_squared_error: 0.1962\n",
            "Epoch 58: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2875 - mean_squared_error: 0.1962 - val_loss: 0.2876 - val_mean_squared_error: 0.2378\n",
            "Epoch 59/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2992 - mean_squared_error: 0.2022\n",
            "Epoch 59: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2911 - mean_squared_error: 0.2000 - val_loss: 0.2925 - val_mean_squared_error: 0.2403\n",
            "Epoch 60/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2957 - mean_squared_error: 0.2135\n",
            "Epoch 60: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2948 - mean_squared_error: 0.2022 - val_loss: 0.2789 - val_mean_squared_error: 0.2310\n",
            "Epoch 61/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2914 - mean_squared_error: 0.2043\n",
            "Epoch 61: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2949 - mean_squared_error: 0.2016 - val_loss: 0.2876 - val_mean_squared_error: 0.2353\n",
            "Epoch 62/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2973 - mean_squared_error: 0.1976\n",
            "Epoch 62: val_loss did not improve from 0.27754\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2902 - mean_squared_error: 0.1947 - val_loss: 0.2940 - val_mean_squared_error: 0.2384\n",
            "Epoch 63/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2701 - mean_squared_error: 0.1820\n",
            "Epoch 63: val_loss improved from 0.27754 to 0.27578, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2844 - mean_squared_error: 0.1935 - val_loss: 0.2758 - val_mean_squared_error: 0.2283\n",
            "Epoch 64/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2779 - mean_squared_error: 0.1649\n",
            "Epoch 64: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2955 - mean_squared_error: 0.2001 - val_loss: 0.2953 - val_mean_squared_error: 0.2382\n",
            "Epoch 65/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2747 - mean_squared_error: 0.1887\n",
            "Epoch 65: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2857 - mean_squared_error: 0.1994 - val_loss: 0.2759 - val_mean_squared_error: 0.2257\n",
            "Epoch 66/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2991 - mean_squared_error: 0.2122\n",
            "Epoch 66: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2992 - mean_squared_error: 0.2097 - val_loss: 0.3048 - val_mean_squared_error: 0.2424\n",
            "Epoch 67/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3004 - mean_squared_error: 0.2075\n",
            "Epoch 67: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2821 - mean_squared_error: 0.1902 - val_loss: 0.2807 - val_mean_squared_error: 0.2242\n",
            "Epoch 68/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2966 - mean_squared_error: 0.2073\n",
            "Epoch 68: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2793 - mean_squared_error: 0.1885 - val_loss: 0.2886 - val_mean_squared_error: 0.2282\n",
            "Epoch 69/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.1907\n",
            "Epoch 69: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2798 - mean_squared_error: 0.1869 - val_loss: 0.2893 - val_mean_squared_error: 0.2290\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2586 - mean_squared_error: 0.1717\n",
            "Epoch 70: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2779 - mean_squared_error: 0.1894 - val_loss: 0.2785 - val_mean_squared_error: 0.2235\n",
            "Epoch 71/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2730 - mean_squared_error: 0.1802\n",
            "Epoch 71: val_loss did not improve from 0.27578\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2835 - mean_squared_error: 0.1922 - val_loss: 0.2967 - val_mean_squared_error: 0.2346\n",
            "Epoch 72/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2857 - mean_squared_error: 0.2005\n",
            "Epoch 72: val_loss improved from 0.27578 to 0.27550, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2855 - mean_squared_error: 0.1904 - val_loss: 0.2755 - val_mean_squared_error: 0.2199\n",
            "Epoch 73/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2592 - mean_squared_error: 0.1571\n",
            "Epoch 73: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2794 - mean_squared_error: 0.1845 - val_loss: 0.2973 - val_mean_squared_error: 0.2336\n",
            "Epoch 74/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2981 - mean_squared_error: 0.2073\n",
            "Epoch 74: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2781 - mean_squared_error: 0.1825 - val_loss: 0.2811 - val_mean_squared_error: 0.2215\n",
            "Epoch 75/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2787 - mean_squared_error: 0.1905\n",
            "Epoch 75: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2749 - mean_squared_error: 0.1809 - val_loss: 0.2851 - val_mean_squared_error: 0.2226\n",
            "Epoch 76/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2563 - mean_squared_error: 0.1523\n",
            "Epoch 76: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2737 - mean_squared_error: 0.1798 - val_loss: 0.2884 - val_mean_squared_error: 0.2237\n",
            "Epoch 77/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.1692\n",
            "Epoch 77: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2798 - mean_squared_error: 0.1851 - val_loss: 0.2804 - val_mean_squared_error: 0.2173\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2730 - mean_squared_error: 0.1792\n",
            "Epoch 78: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2730 - mean_squared_error: 0.1792 - val_loss: 0.2850 - val_mean_squared_error: 0.2205\n",
            "Epoch 79/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2719 - mean_squared_error: 0.1748\n",
            "Epoch 79: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2698 - mean_squared_error: 0.1765 - val_loss: 0.2831 - val_mean_squared_error: 0.2169\n",
            "Epoch 80/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2660 - mean_squared_error: 0.1668\n",
            "Epoch 80: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2693 - mean_squared_error: 0.1751 - val_loss: 0.2867 - val_mean_squared_error: 0.2189\n",
            "Epoch 81/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2838 - mean_squared_error: 0.2003\n",
            "Epoch 81: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2680 - mean_squared_error: 0.1730 - val_loss: 0.2761 - val_mean_squared_error: 0.2145\n",
            "Epoch 82/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2843 - mean_squared_error: 0.1858\n",
            "Epoch 82: val_loss did not improve from 0.27550\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2733 - mean_squared_error: 0.1780 - val_loss: 0.2922 - val_mean_squared_error: 0.2220\n",
            "Epoch 83/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2457 - mean_squared_error: 0.1561\n",
            "Epoch 83: val_loss improved from 0.27550 to 0.27334, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.2655 - mean_squared_error: 0.1733 - val_loss: 0.2733 - val_mean_squared_error: 0.2099\n",
            "Epoch 84/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2908 - mean_squared_error: 0.2002\n",
            "Epoch 84: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2857 - mean_squared_error: 0.1910 - val_loss: 0.2923 - val_mean_squared_error: 0.2204\n",
            "Epoch 85/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2669 - mean_squared_error: 0.1782\n",
            "Epoch 85: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2672 - mean_squared_error: 0.1708 - val_loss: 0.2746 - val_mean_squared_error: 0.2075\n",
            "Epoch 86/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2818 - mean_squared_error: 0.1999\n",
            "Epoch 86: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2647 - mean_squared_error: 0.1705 - val_loss: 0.2828 - val_mean_squared_error: 0.2138\n",
            "Epoch 87/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2567 - mean_squared_error: 0.1549\n",
            "Epoch 87: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2685 - mean_squared_error: 0.1712 - val_loss: 0.2839 - val_mean_squared_error: 0.2131\n",
            "Epoch 88/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2528 - mean_squared_error: 0.1488\n",
            "Epoch 88: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2652 - mean_squared_error: 0.1698 - val_loss: 0.2813 - val_mean_squared_error: 0.2103\n",
            "Epoch 89/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2826 - mean_squared_error: 0.1861\n",
            "Epoch 89: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2642 - mean_squared_error: 0.1691 - val_loss: 0.2791 - val_mean_squared_error: 0.2100\n",
            "Epoch 90/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.1823\n",
            "Epoch 90: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2698 - mean_squared_error: 0.1708 - val_loss: 0.2914 - val_mean_squared_error: 0.2193\n",
            "Epoch 91/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2879 - mean_squared_error: 0.1895\n",
            "Epoch 91: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2775 - mean_squared_error: 0.1744 - val_loss: 0.2755 - val_mean_squared_error: 0.2051\n",
            "Epoch 92/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2723 - mean_squared_error: 0.1807\n",
            "Epoch 92: val_loss did not improve from 0.27334\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2632 - mean_squared_error: 0.1672 - val_loss: 0.2954 - val_mean_squared_error: 0.2212\n",
            "Epoch 93/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2810 - mean_squared_error: 0.1883\n",
            "Epoch 93: val_loss improved from 0.27334 to 0.26938, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.2734 - mean_squared_error: 0.1731 - val_loss: 0.2694 - val_mean_squared_error: 0.2016\n",
            "Epoch 94/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2820 - mean_squared_error: 0.1802\n",
            "Epoch 94: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2653 - mean_squared_error: 0.1638 - val_loss: 0.2860 - val_mean_squared_error: 0.2125\n",
            "Epoch 95/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2729 - mean_squared_error: 0.1762\n",
            "Epoch 95: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2619 - mean_squared_error: 0.1632 - val_loss: 0.2742 - val_mean_squared_error: 0.2035\n",
            "Epoch 96/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2526 - mean_squared_error: 0.1559\n",
            "Epoch 96: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2626 - mean_squared_error: 0.1654 - val_loss: 0.2784 - val_mean_squared_error: 0.2043\n",
            "Epoch 97/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2489 - mean_squared_error: 0.1407\n",
            "Epoch 97: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2603 - mean_squared_error: 0.1624 - val_loss: 0.2701 - val_mean_squared_error: 0.1962\n",
            "Epoch 98/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2651 - mean_squared_error: 0.1603\n",
            "Epoch 98: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2572 - mean_squared_error: 0.1605 - val_loss: 0.2782 - val_mean_squared_error: 0.2027\n",
            "Epoch 99/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2393 - mean_squared_error: 0.1345\n",
            "Epoch 99: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2609 - mean_squared_error: 0.1623 - val_loss: 0.2739 - val_mean_squared_error: 0.2000\n",
            "Epoch 100/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2717 - mean_squared_error: 0.1721\n",
            "Epoch 100: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2583 - mean_squared_error: 0.1605 - val_loss: 0.2715 - val_mean_squared_error: 0.1952\n",
            "Epoch 101/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2489 - mean_squared_error: 0.1511\n",
            "Epoch 101: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2566 - mean_squared_error: 0.1586 - val_loss: 0.2800 - val_mean_squared_error: 0.2024\n",
            "Epoch 102/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2488 - mean_squared_error: 0.1580\n",
            "Epoch 102: val_loss did not improve from 0.26938\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2568 - mean_squared_error: 0.1582 - val_loss: 0.2715 - val_mean_squared_error: 0.1966\n",
            "Epoch 103/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2556 - mean_squared_error: 0.1547\n",
            "Epoch 103: val_loss improved from 0.26938 to 0.26761, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.2589 - mean_squared_error: 0.1582 - val_loss: 0.2676 - val_mean_squared_error: 0.1958\n",
            "Epoch 104/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2591 - mean_squared_error: 0.1674\n",
            "Epoch 104: val_loss did not improve from 0.26761\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2548 - mean_squared_error: 0.1572 - val_loss: 0.2792 - val_mean_squared_error: 0.2022\n",
            "Epoch 105/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2709 - mean_squared_error: 0.1823\n",
            "Epoch 105: val_loss did not improve from 0.26761\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2583 - mean_squared_error: 0.1575 - val_loss: 0.2779 - val_mean_squared_error: 0.1991\n",
            "Epoch 106/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2720 - mean_squared_error: 0.1739\n",
            "Epoch 106: val_loss improved from 0.26761 to 0.26744, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.2610 - mean_squared_error: 0.1610 - val_loss: 0.2674 - val_mean_squared_error: 0.1936\n",
            "Epoch 107/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2435 - mean_squared_error: 0.1484\n",
            "Epoch 107: val_loss did not improve from 0.26744\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2522 - mean_squared_error: 0.1537 - val_loss: 0.2742 - val_mean_squared_error: 0.1979\n",
            "Epoch 108/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2629 - mean_squared_error: 0.1687\n",
            "Epoch 108: val_loss did not improve from 0.26744\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2526 - mean_squared_error: 0.1526 - val_loss: 0.2722 - val_mean_squared_error: 0.1943\n",
            "Epoch 109/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2566 - mean_squared_error: 0.1623\n",
            "Epoch 109: val_loss did not improve from 0.26744\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2562 - mean_squared_error: 0.1582 - val_loss: 0.2699 - val_mean_squared_error: 0.1914\n",
            "Epoch 110/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2622 - mean_squared_error: 0.1628\n",
            "Epoch 110: val_loss did not improve from 0.26744\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2524 - mean_squared_error: 0.1523 - val_loss: 0.2794 - val_mean_squared_error: 0.2005\n",
            "Epoch 111/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2460 - mean_squared_error: 0.1486\n",
            "Epoch 111: val_loss did not improve from 0.26744\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2535 - mean_squared_error: 0.1506 - val_loss: 0.2703 - val_mean_squared_error: 0.1949\n",
            "Epoch 112/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2511 - mean_squared_error: 0.1524\n",
            "Epoch 112: val_loss improved from 0.26744 to 0.26445, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.2500 - mean_squared_error: 0.1512 - val_loss: 0.2644 - val_mean_squared_error: 0.1910\n",
            "Epoch 113/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2473 - mean_squared_error: 0.1538\n",
            "Epoch 113: val_loss did not improve from 0.26445\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2503 - mean_squared_error: 0.1526 - val_loss: 0.2651 - val_mean_squared_error: 0.1893\n",
            "Epoch 114/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2391 - mean_squared_error: 0.1321\n",
            "Epoch 114: val_loss did not improve from 0.26445\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2489 - mean_squared_error: 0.1509 - val_loss: 0.2663 - val_mean_squared_error: 0.1889\n",
            "Epoch 115/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2315 - mean_squared_error: 0.1203\n",
            "Epoch 115: val_loss improved from 0.26445 to 0.25244, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2558 - mean_squared_error: 0.1527 - val_loss: 0.2524 - val_mean_squared_error: 0.1816\n",
            "Epoch 116/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.1547\n",
            "Epoch 116: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2722 - mean_squared_error: 0.1649 - val_loss: 0.2867 - val_mean_squared_error: 0.2079\n",
            "Epoch 117/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2800 - mean_squared_error: 0.1814\n",
            "Epoch 117: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2553 - mean_squared_error: 0.1526 - val_loss: 0.2594 - val_mean_squared_error: 0.1842\n",
            "Epoch 118/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2502 - mean_squared_error: 0.1416\n",
            "Epoch 118: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2507 - mean_squared_error: 0.1492 - val_loss: 0.2595 - val_mean_squared_error: 0.1842\n",
            "Epoch 119/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2436 - mean_squared_error: 0.1364\n",
            "Epoch 119: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2481 - mean_squared_error: 0.1473 - val_loss: 0.2746 - val_mean_squared_error: 0.1946\n",
            "Epoch 120/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2463 - mean_squared_error: 0.1410\n",
            "Epoch 120: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2494 - mean_squared_error: 0.1479 - val_loss: 0.2628 - val_mean_squared_error: 0.1854\n",
            "Epoch 121/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2399 - mean_squared_error: 0.1317\n",
            "Epoch 121: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2457 - mean_squared_error: 0.1469 - val_loss: 0.2581 - val_mean_squared_error: 0.1835\n",
            "Epoch 122/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2443 - mean_squared_error: 0.1487\n",
            "Epoch 122: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2456 - mean_squared_error: 0.1450 - val_loss: 0.2751 - val_mean_squared_error: 0.1959\n",
            "Epoch 123/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2495 - mean_squared_error: 0.1528\n",
            "Epoch 123: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2450 - mean_squared_error: 0.1455 - val_loss: 0.2536 - val_mean_squared_error: 0.1820\n",
            "Epoch 124/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2296 - mean_squared_error: 0.1227\n",
            "Epoch 124: val_loss did not improve from 0.25244\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2449 - mean_squared_error: 0.1455 - val_loss: 0.2620 - val_mean_squared_error: 0.1843\n",
            "Epoch 125/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2454 - mean_squared_error: 0.1496\n",
            "Epoch 125: val_loss improved from 0.25244 to 0.24970, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2459 - mean_squared_error: 0.1459 - val_loss: 0.2497 - val_mean_squared_error: 0.1773\n",
            "Epoch 126/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2465 - mean_squared_error: 0.1460\n",
            "Epoch 126: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2456 - mean_squared_error: 0.1421 - val_loss: 0.2652 - val_mean_squared_error: 0.1842\n",
            "Epoch 127/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2466 - mean_squared_error: 0.1515\n",
            "Epoch 127: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2435 - mean_squared_error: 0.1441 - val_loss: 0.2548 - val_mean_squared_error: 0.1786\n",
            "Epoch 128/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2539 - mean_squared_error: 0.1560\n",
            "Epoch 128: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2446 - mean_squared_error: 0.1447 - val_loss: 0.2604 - val_mean_squared_error: 0.1844\n",
            "Epoch 129/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2495 - mean_squared_error: 0.1497\n",
            "Epoch 129: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2510 - mean_squared_error: 0.1486 - val_loss: 0.2600 - val_mean_squared_error: 0.1856\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2488 - mean_squared_error: 0.1463\n",
            "Epoch 130: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2488 - mean_squared_error: 0.1463 - val_loss: 0.2534 - val_mean_squared_error: 0.1756\n",
            "Epoch 131/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2363 - mean_squared_error: 0.1369\n",
            "Epoch 131: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2465 - mean_squared_error: 0.1462 - val_loss: 0.2611 - val_mean_squared_error: 0.1783\n",
            "Epoch 132/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2553 - mean_squared_error: 0.1451\n",
            "Epoch 132: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2513 - mean_squared_error: 0.1454 - val_loss: 0.2586 - val_mean_squared_error: 0.1793\n",
            "Epoch 133/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2509 - mean_squared_error: 0.1545\n",
            "Epoch 133: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2418 - mean_squared_error: 0.1419 - val_loss: 0.2522 - val_mean_squared_error: 0.1746\n",
            "Epoch 134/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2484 - mean_squared_error: 0.1443\n",
            "Epoch 134: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2444 - mean_squared_error: 0.1435 - val_loss: 0.2583 - val_mean_squared_error: 0.1777\n",
            "Epoch 135/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2494 - mean_squared_error: 0.1463\n",
            "Epoch 135: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2445 - mean_squared_error: 0.1424 - val_loss: 0.2509 - val_mean_squared_error: 0.1753\n",
            "Epoch 136/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2439 - mean_squared_error: 0.1444\n",
            "Epoch 136: val_loss did not improve from 0.24970\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2433 - mean_squared_error: 0.1403 - val_loss: 0.2623 - val_mean_squared_error: 0.1816\n",
            "Epoch 137/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2461 - mean_squared_error: 0.1455\n",
            "Epoch 137: val_loss improved from 0.24970 to 0.24462, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2476 - mean_squared_error: 0.1442 - val_loss: 0.2446 - val_mean_squared_error: 0.1718\n",
            "Epoch 138/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2412 - mean_squared_error: 0.1439\n",
            "Epoch 138: val_loss did not improve from 0.24462\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2382 - mean_squared_error: 0.1400 - val_loss: 0.2636 - val_mean_squared_error: 0.1844\n",
            "Epoch 139/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2389 - mean_squared_error: 0.1361\n",
            "Epoch 139: val_loss did not improve from 0.24462\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2427 - mean_squared_error: 0.1410 - val_loss: 0.2567 - val_mean_squared_error: 0.1792\n",
            "Epoch 140/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2440 - mean_squared_error: 0.1454\n",
            "Epoch 140: val_loss did not improve from 0.24462\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2437 - mean_squared_error: 0.1397 - val_loss: 0.2485 - val_mean_squared_error: 0.1693\n",
            "Epoch 141/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2414 - mean_squared_error: 0.1416\n",
            "Epoch 141: val_loss did not improve from 0.24462\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2400 - mean_squared_error: 0.1380 - val_loss: 0.2497 - val_mean_squared_error: 0.1703\n",
            "Epoch 142/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2487 - mean_squared_error: 0.1488\n",
            "Epoch 142: val_loss did not improve from 0.24462\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2402 - mean_squared_error: 0.1368 - val_loss: 0.2586 - val_mean_squared_error: 0.1761\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2408 - mean_squared_error: 0.1384\n",
            "Epoch 143: val_loss improved from 0.24462 to 0.24462, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2408 - mean_squared_error: 0.1384 - val_loss: 0.2446 - val_mean_squared_error: 0.1710\n",
            "Epoch 144/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2379 - mean_squared_error: 0.1418\n",
            "Epoch 144: val_loss improved from 0.24462 to 0.24180, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2426 - mean_squared_error: 0.1402 - val_loss: 0.2418 - val_mean_squared_error: 0.1690\n",
            "Epoch 145/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2182 - mean_squared_error: 0.1057\n",
            "Epoch 145: val_loss did not improve from 0.24180\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2393 - mean_squared_error: 0.1372 - val_loss: 0.2574 - val_mean_squared_error: 0.1801\n",
            "Epoch 146/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2472 - mean_squared_error: 0.1484\n",
            "Epoch 146: val_loss did not improve from 0.24180\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2365 - mean_squared_error: 0.1368 - val_loss: 0.2440 - val_mean_squared_error: 0.1706\n",
            "Epoch 147/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2336 - mean_squared_error: 0.1360\n",
            "Epoch 147: val_loss did not improve from 0.24180\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2364 - mean_squared_error: 0.1368 - val_loss: 0.2521 - val_mean_squared_error: 0.1733\n",
            "Epoch 148/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2494 - mean_squared_error: 0.1468\n",
            "Epoch 148: val_loss did not improve from 0.24180\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2404 - mean_squared_error: 0.1369 - val_loss: 0.2499 - val_mean_squared_error: 0.1700\n",
            "Epoch 149/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2316 - mean_squared_error: 0.1314\n",
            "Epoch 149: val_loss improved from 0.24180 to 0.24081, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2376 - mean_squared_error: 0.1359 - val_loss: 0.2408 - val_mean_squared_error: 0.1620\n",
            "Epoch 150/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2422 - mean_squared_error: 0.1461\n",
            "Epoch 150: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2436 - mean_squared_error: 0.1414 - val_loss: 0.2675 - val_mean_squared_error: 0.1868\n",
            "Epoch 151/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2452 - mean_squared_error: 0.1233\n",
            "Epoch 151: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2497 - mean_squared_error: 0.1396 - val_loss: 0.2539 - val_mean_squared_error: 0.1621\n",
            "Epoch 152/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2567 - mean_squared_error: 0.1504\n",
            "Epoch 152: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2531 - mean_squared_error: 0.1479 - val_loss: 0.2695 - val_mean_squared_error: 0.1926\n",
            "Epoch 153/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2481 - mean_squared_error: 0.1386\n",
            "Epoch 153: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2531 - mean_squared_error: 0.1457 - val_loss: 0.2632 - val_mean_squared_error: 0.1675\n",
            "Epoch 154/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2505 - mean_squared_error: 0.1424\n",
            "Epoch 154: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2488 - mean_squared_error: 0.1396 - val_loss: 0.2757 - val_mean_squared_error: 0.1910\n",
            "Epoch 155/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2415 - mean_squared_error: 0.1402\n",
            "Epoch 155: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2493 - mean_squared_error: 0.1448 - val_loss: 0.2418 - val_mean_squared_error: 0.1630\n",
            "Epoch 156/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2397 - mean_squared_error: 0.1425\n",
            "Epoch 156: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2349 - mean_squared_error: 0.1345 - val_loss: 0.2505 - val_mean_squared_error: 0.1703\n",
            "Epoch 157/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2403 - mean_squared_error: 0.1351\n",
            "Epoch 157: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2389 - mean_squared_error: 0.1344 - val_loss: 0.2427 - val_mean_squared_error: 0.1688\n",
            "Epoch 158/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2420 - mean_squared_error: 0.1447\n",
            "Epoch 158: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2396 - mean_squared_error: 0.1373 - val_loss: 0.2455 - val_mean_squared_error: 0.1673\n",
            "Epoch 159/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2417 - mean_squared_error: 0.1347\n",
            "Epoch 159: val_loss did not improve from 0.24081\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2400 - mean_squared_error: 0.1333 - val_loss: 0.2595 - val_mean_squared_error: 0.1724\n",
            "Epoch 160/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2351 - mean_squared_error: 0.1343\n",
            "Epoch 160: val_loss improved from 0.24081 to 0.23679, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2367 - mean_squared_error: 0.1328 - val_loss: 0.2368 - val_mean_squared_error: 0.1601\n",
            "Epoch 161/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2383 - mean_squared_error: 0.1310\n",
            "Epoch 161: val_loss did not improve from 0.23679\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2375 - mean_squared_error: 0.1338 - val_loss: 0.2456 - val_mean_squared_error: 0.1671\n",
            "Epoch 162/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2189 - mean_squared_error: 0.1150\n",
            "Epoch 162: val_loss did not improve from 0.23679\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2311 - mean_squared_error: 0.1300 - val_loss: 0.2528 - val_mean_squared_error: 0.1672\n",
            "Epoch 163/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2358 - mean_squared_error: 0.1267\n",
            "Epoch 163: val_loss did not improve from 0.23679\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2407 - mean_squared_error: 0.1340 - val_loss: 0.2522 - val_mean_squared_error: 0.1733\n",
            "Epoch 164/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2397 - mean_squared_error: 0.1408\n",
            "Epoch 164: val_loss did not improve from 0.23679\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2339 - mean_squared_error: 0.1329 - val_loss: 0.2428 - val_mean_squared_error: 0.1660\n",
            "Epoch 165/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2243 - mean_squared_error: 0.1228\n",
            "Epoch 165: val_loss did not improve from 0.23679\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2394 - mean_squared_error: 0.1314 - val_loss: 0.2609 - val_mean_squared_error: 0.1824\n",
            "Epoch 166/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2479 - mean_squared_error: 0.1439\n",
            "Epoch 166: val_loss improved from 0.23679 to 0.23616, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.2382 - mean_squared_error: 0.1320 - val_loss: 0.2362 - val_mean_squared_error: 0.1625\n",
            "Epoch 167/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2478 - mean_squared_error: 0.1400\n",
            "Epoch 167: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2401 - mean_squared_error: 0.1311 - val_loss: 0.2488 - val_mean_squared_error: 0.1648\n",
            "Epoch 168/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2340 - mean_squared_error: 0.1390\n",
            "Epoch 168: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2332 - mean_squared_error: 0.1283 - val_loss: 0.2499 - val_mean_squared_error: 0.1716\n",
            "Epoch 169/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2390 - mean_squared_error: 0.1384\n",
            "Epoch 169: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2314 - mean_squared_error: 0.1293 - val_loss: 0.2390 - val_mean_squared_error: 0.1606\n",
            "Epoch 170/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2253 - mean_squared_error: 0.1161\n",
            "Epoch 170: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2349 - mean_squared_error: 0.1285 - val_loss: 0.2607 - val_mean_squared_error: 0.1709\n",
            "Epoch 171/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2222 - mean_squared_error: 0.1145\n",
            "Epoch 171: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2312 - mean_squared_error: 0.1265 - val_loss: 0.2392 - val_mean_squared_error: 0.1627\n",
            "Epoch 172/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2380 - mean_squared_error: 0.1306\n",
            "Epoch 172: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2355 - mean_squared_error: 0.1294 - val_loss: 0.2400 - val_mean_squared_error: 0.1603\n",
            "Epoch 173/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2234 - mean_squared_error: 0.1143\n",
            "Epoch 173: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2367 - mean_squared_error: 0.1282 - val_loss: 0.2668 - val_mean_squared_error: 0.1817\n",
            "Epoch 174/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2485 - mean_squared_error: 0.1405\n",
            "Epoch 174: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2478 - mean_squared_error: 0.1353 - val_loss: 0.2455 - val_mean_squared_error: 0.1513\n",
            "Epoch 175/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2536 - mean_squared_error: 0.1379\n",
            "Epoch 175: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2419 - mean_squared_error: 0.1341 - val_loss: 0.2473 - val_mean_squared_error: 0.1704\n",
            "Epoch 176/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2313 - mean_squared_error: 0.1287\n",
            "Epoch 176: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2402 - mean_squared_error: 0.1294 - val_loss: 0.2664 - val_mean_squared_error: 0.1673\n",
            "Epoch 177/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2421 - mean_squared_error: 0.1355\n",
            "Epoch 177: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2318 - mean_squared_error: 0.1248 - val_loss: 0.2514 - val_mean_squared_error: 0.1708\n",
            "Epoch 178/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2411 - mean_squared_error: 0.1317\n",
            "Epoch 178: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2512 - mean_squared_error: 0.1410 - val_loss: 0.2518 - val_mean_squared_error: 0.1538\n",
            "Epoch 179/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2520 - mean_squared_error: 0.1328\n",
            "Epoch 179: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2499 - mean_squared_error: 0.1304 - val_loss: 0.2879 - val_mean_squared_error: 0.1834\n",
            "Epoch 180/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2651 - mean_squared_error: 0.1443\n",
            "Epoch 180: val_loss did not improve from 0.23616\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2515 - mean_squared_error: 0.1315 - val_loss: 0.2810 - val_mean_squared_error: 0.1877\n",
            "Epoch 181/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2040 - mean_squared_error: 0.0940\n",
            "Epoch 181: val_loss improved from 0.23616 to 0.23267, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2342 - mean_squared_error: 0.1232 - val_loss: 0.2327 - val_mean_squared_error: 0.1556\n",
            "Epoch 182/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2256 - mean_squared_error: 0.1078\n",
            "Epoch 182: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2339 - mean_squared_error: 0.1244 - val_loss: 0.2407 - val_mean_squared_error: 0.1555\n",
            "Epoch 183/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2379 - mean_squared_error: 0.1302\n",
            "Epoch 183: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2293 - mean_squared_error: 0.1199 - val_loss: 0.2573 - val_mean_squared_error: 0.1687\n",
            "Epoch 184/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2299 - mean_squared_error: 0.1130\n",
            "Epoch 184: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2337 - mean_squared_error: 0.1212 - val_loss: 0.2471 - val_mean_squared_error: 0.1577\n",
            "Epoch 185/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2326 - mean_squared_error: 0.1273\n",
            "Epoch 185: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2304 - mean_squared_error: 0.1220 - val_loss: 0.2406 - val_mean_squared_error: 0.1603\n",
            "Epoch 186/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2345 - mean_squared_error: 0.1307\n",
            "Epoch 186: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2305 - mean_squared_error: 0.1215 - val_loss: 0.2474 - val_mean_squared_error: 0.1633\n",
            "Epoch 187/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2214 - mean_squared_error: 0.1125\n",
            "Epoch 187: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2285 - mean_squared_error: 0.1196 - val_loss: 0.2604 - val_mean_squared_error: 0.1627\n",
            "Epoch 188/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2328 - mean_squared_error: 0.1196\n",
            "Epoch 188: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2364 - mean_squared_error: 0.1235 - val_loss: 0.2504 - val_mean_squared_error: 0.1673\n",
            "Epoch 189/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2171 - mean_squared_error: 0.1034\n",
            "Epoch 189: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2273 - mean_squared_error: 0.1204 - val_loss: 0.2422 - val_mean_squared_error: 0.1560\n",
            "Epoch 190/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2342 - mean_squared_error: 0.1265\n",
            "Epoch 190: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2320 - mean_squared_error: 0.1217 - val_loss: 0.2589 - val_mean_squared_error: 0.1809\n",
            "Epoch 191/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2299 - mean_squared_error: 0.1198\n",
            "Epoch 191: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2354 - mean_squared_error: 0.1223 - val_loss: 0.2422 - val_mean_squared_error: 0.1612\n",
            "Epoch 192/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2231 - mean_squared_error: 0.1168\n",
            "Epoch 192: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2299 - mean_squared_error: 0.1216 - val_loss: 0.2536 - val_mean_squared_error: 0.1637\n",
            "Epoch 193/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2355 - mean_squared_error: 0.1211\n",
            "Epoch 193: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2360 - mean_squared_error: 0.1203 - val_loss: 0.2743 - val_mean_squared_error: 0.1717\n",
            "Epoch 194/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2353 - mean_squared_error: 0.1241\n",
            "Epoch 194: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2292 - mean_squared_error: 0.1196 - val_loss: 0.2409 - val_mean_squared_error: 0.1706\n",
            "Epoch 195/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2129 - mean_squared_error: 0.0993\n",
            "Epoch 195: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2284 - mean_squared_error: 0.1200 - val_loss: 0.2665 - val_mean_squared_error: 0.1624\n",
            "Epoch 196/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2463 - mean_squared_error: 0.1312\n",
            "Epoch 196: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2397 - mean_squared_error: 0.1260 - val_loss: 0.2660 - val_mean_squared_error: 0.1835\n",
            "Epoch 197/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2437 - mean_squared_error: 0.1315\n",
            "Epoch 197: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2388 - mean_squared_error: 0.1222 - val_loss: 0.2461 - val_mean_squared_error: 0.1520\n",
            "Epoch 198/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2736 - mean_squared_error: 0.1528\n",
            "Epoch 198: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2610 - mean_squared_error: 0.1404 - val_loss: 0.2603 - val_mean_squared_error: 0.1614\n",
            "Epoch 199/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2419 - mean_squared_error: 0.1233\n",
            "Epoch 199: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2338 - mean_squared_error: 0.1206 - val_loss: 0.2569 - val_mean_squared_error: 0.1628\n",
            "Epoch 200/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2296 - mean_squared_error: 0.1169\n",
            "Epoch 200: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2354 - mean_squared_error: 0.1226 - val_loss: 0.2405 - val_mean_squared_error: 0.1588\n",
            "Epoch 201/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2363 - mean_squared_error: 0.1190\n",
            "Epoch 201: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2345 - mean_squared_error: 0.1236 - val_loss: 0.2526 - val_mean_squared_error: 0.1590\n",
            "Epoch 202/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2447 - mean_squared_error: 0.1303\n",
            "Epoch 202: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2307 - mean_squared_error: 0.1174 - val_loss: 0.2669 - val_mean_squared_error: 0.1620\n",
            "Epoch 203/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2132 - mean_squared_error: 0.1008\n",
            "Epoch 203: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2252 - mean_squared_error: 0.1122 - val_loss: 0.2371 - val_mean_squared_error: 0.1545\n",
            "Epoch 204/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2285 - mean_squared_error: 0.1163\n",
            "Epoch 204: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2255 - mean_squared_error: 0.1136 - val_loss: 0.2475 - val_mean_squared_error: 0.1615\n",
            "Epoch 205/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2266 - mean_squared_error: 0.1073\n",
            "Epoch 205: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2242 - mean_squared_error: 0.1122 - val_loss: 0.2491 - val_mean_squared_error: 0.1569\n",
            "Epoch 206/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2242 - mean_squared_error: 0.1130\n",
            "Epoch 206: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2256 - mean_squared_error: 0.1139 - val_loss: 0.2399 - val_mean_squared_error: 0.1502\n",
            "Epoch 207/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2311 - mean_squared_error: 0.1161\n",
            "Epoch 207: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2254 - mean_squared_error: 0.1141 - val_loss: 0.2527 - val_mean_squared_error: 0.1650\n",
            "Epoch 208/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2131 - mean_squared_error: 0.0955\n",
            "Epoch 208: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2212 - mean_squared_error: 0.1104 - val_loss: 0.2491 - val_mean_squared_error: 0.1541\n",
            "Epoch 209/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2226 - mean_squared_error: 0.1057\n",
            "Epoch 209: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2213 - mean_squared_error: 0.1113 - val_loss: 0.2455 - val_mean_squared_error: 0.1595\n",
            "Epoch 210/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2236 - mean_squared_error: 0.1146\n",
            "Epoch 210: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2305 - mean_squared_error: 0.1147 - val_loss: 0.2475 - val_mean_squared_error: 0.1533\n",
            "Epoch 211/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2283 - mean_squared_error: 0.1144\n",
            "Epoch 211: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2233 - mean_squared_error: 0.1108 - val_loss: 0.2525 - val_mean_squared_error: 0.1663\n",
            "Epoch 212/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2372 - mean_squared_error: 0.1201\n",
            "Epoch 212: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2269 - mean_squared_error: 0.1124 - val_loss: 0.2564 - val_mean_squared_error: 0.1608\n",
            "Epoch 213/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2335 - mean_squared_error: 0.1195\n",
            "Epoch 213: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2282 - mean_squared_error: 0.1127 - val_loss: 0.2441 - val_mean_squared_error: 0.1551\n",
            "Epoch 214/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2032 - mean_squared_error: 0.0938\n",
            "Epoch 214: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2270 - mean_squared_error: 0.1134 - val_loss: 0.2407 - val_mean_squared_error: 0.1594\n",
            "Epoch 215/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2323 - mean_squared_error: 0.1239\n",
            "Epoch 215: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2232 - mean_squared_error: 0.1121 - val_loss: 0.2449 - val_mean_squared_error: 0.1443\n",
            "Epoch 216/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2357 - mean_squared_error: 0.1263\n",
            "Epoch 216: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2271 - mean_squared_error: 0.1131 - val_loss: 0.2645 - val_mean_squared_error: 0.1563\n",
            "Epoch 217/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2163 - mean_squared_error: 0.0965\n",
            "Epoch 217: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2242 - mean_squared_error: 0.1090 - val_loss: 0.2419 - val_mean_squared_error: 0.1567\n",
            "Epoch 218/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2337 - mean_squared_error: 0.1166\n",
            "Epoch 218: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2261 - mean_squared_error: 0.1119 - val_loss: 0.2543 - val_mean_squared_error: 0.1507\n",
            "Epoch 219/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2168 - mean_squared_error: 0.1053\n",
            "Epoch 219: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2275 - mean_squared_error: 0.1128 - val_loss: 0.2460 - val_mean_squared_error: 0.1588\n",
            "Epoch 220/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2405 - mean_squared_error: 0.1209\n",
            "Epoch 220: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2324 - mean_squared_error: 0.1130 - val_loss: 0.2448 - val_mean_squared_error: 0.1426\n",
            "Epoch 221/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2238 - mean_squared_error: 0.1127\n",
            "Epoch 221: val_loss did not improve from 0.23267\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2291 - mean_squared_error: 0.1124 - val_loss: 0.2519 - val_mean_squared_error: 0.1639\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "Fold: 8\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 8\n",
            "Epoch 1/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 3.7349 - mean_squared_error: 22.4537 \n",
            "Epoch 1: val_loss improved from inf to 2.24554, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 4s 97ms/step - loss: 3.4400 - mean_squared_error: 19.0332 - val_loss: 2.2455 - val_mean_squared_error: 5.5081\n",
            "Epoch 2/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 1.5364 - mean_squared_error: 2.9862\n",
            "Epoch 2: val_loss improved from 2.24554 to 1.09969, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 1.3847 - mean_squared_error: 2.6294 - val_loss: 1.0997 - val_mean_squared_error: 1.6891\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 1.0496 - mean_squared_error: 2.9191\n",
            "Epoch 3: val_loss did not improve from 1.09969\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0496 - mean_squared_error: 2.9191 - val_loss: 1.2395 - val_mean_squared_error: 2.4388\n",
            "Epoch 4/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.9558 - mean_squared_error: 2.0540\n",
            "Epoch 4: val_loss improved from 1.09969 to 0.92139, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.9169 - mean_squared_error: 1.9410 - val_loss: 0.9214 - val_mean_squared_error: 1.1728\n",
            "Epoch 5/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7433 - mean_squared_error: 0.9497\n",
            "Epoch 5: val_loss improved from 0.92139 to 0.85148, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.7595 - mean_squared_error: 0.9949 - val_loss: 0.8515 - val_mean_squared_error: 1.0206\n",
            "Epoch 6/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7474 - mean_squared_error: 0.9476\n",
            "Epoch 6: val_loss improved from 0.85148 to 0.75189, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.7358 - mean_squared_error: 0.9157 - val_loss: 0.7519 - val_mean_squared_error: 0.8543\n",
            "Epoch 7/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7093 - mean_squared_error: 1.0714\n",
            "Epoch 7: val_loss improved from 0.75189 to 0.71371, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.6808 - mean_squared_error: 1.0009 - val_loss: 0.7137 - val_mean_squared_error: 0.7894\n",
            "Epoch 8/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6352 - mean_squared_error: 0.8379\n",
            "Epoch 8: val_loss improved from 0.71371 to 0.67152, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.6493 - mean_squared_error: 0.8684 - val_loss: 0.6715 - val_mean_squared_error: 0.7070\n",
            "Epoch 9/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6338 - mean_squared_error: 0.7790\n",
            "Epoch 9: val_loss improved from 0.67152 to 0.63532, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.6297 - mean_squared_error: 0.7540 - val_loss: 0.6353 - val_mean_squared_error: 0.6571\n",
            "Epoch 10/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6142 - mean_squared_error: 0.7299\n",
            "Epoch 10: val_loss improved from 0.63532 to 0.62111, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.6204 - mean_squared_error: 0.7492 - val_loss: 0.6211 - val_mean_squared_error: 0.6252\n",
            "Epoch 11/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5933 - mean_squared_error: 0.6553\n",
            "Epoch 11: val_loss improved from 0.62111 to 0.61389, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.6066 - mean_squared_error: 0.6871 - val_loss: 0.6139 - val_mean_squared_error: 0.6040\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6045 - mean_squared_error: 0.6557\n",
            "Epoch 12: val_loss improved from 0.61389 to 0.59104, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.6014 - mean_squared_error: 0.6769 - val_loss: 0.5910 - val_mean_squared_error: 0.5720\n",
            "Epoch 13/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5704 - mean_squared_error: 0.6263\n",
            "Epoch 13: val_loss did not improve from 0.59104\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5884 - mean_squared_error: 0.6400 - val_loss: 0.6182 - val_mean_squared_error: 0.6100\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.6038 - mean_squared_error: 0.6675\n",
            "Epoch 14: val_loss improved from 0.59104 to 0.57142, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.6038 - mean_squared_error: 0.6675 - val_loss: 0.5714 - val_mean_squared_error: 0.5316\n",
            "Epoch 15/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.5659 - mean_squared_error: 0.6170\n",
            "Epoch 15: val_loss improved from 0.57142 to 0.57123, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.5747 - mean_squared_error: 0.6002 - val_loss: 0.5712 - val_mean_squared_error: 0.5400\n",
            "Epoch 16/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5586 - mean_squared_error: 0.5907\n",
            "Epoch 16: val_loss improved from 0.57123 to 0.56132, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.5492 - mean_squared_error: 0.5994 - val_loss: 0.5613 - val_mean_squared_error: 0.5109\n",
            "Epoch 17/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.6113 - mean_squared_error: 0.7246\n",
            "Epoch 17: val_loss improved from 0.56132 to 0.55094, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.5431 - mean_squared_error: 0.5889 - val_loss: 0.5509 - val_mean_squared_error: 0.4999\n",
            "Epoch 18/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.5344 - mean_squared_error: 0.5309\n",
            "Epoch 18: val_loss improved from 0.55094 to 0.53124, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.5367 - mean_squared_error: 0.5387 - val_loss: 0.5312 - val_mean_squared_error: 0.4668\n",
            "Epoch 19/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5187 - mean_squared_error: 0.5359\n",
            "Epoch 19: val_loss improved from 0.53124 to 0.51993, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.5103 - mean_squared_error: 0.5157 - val_loss: 0.5199 - val_mean_squared_error: 0.4414\n",
            "Epoch 20/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5161 - mean_squared_error: 0.5469\n",
            "Epoch 20: val_loss improved from 0.51993 to 0.51578, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.5112 - mean_squared_error: 0.5268 - val_loss: 0.5158 - val_mean_squared_error: 0.4439\n",
            "Epoch 21/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4720 - mean_squared_error: 0.4405\n",
            "Epoch 21: val_loss improved from 0.51578 to 0.50086, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.4922 - mean_squared_error: 0.4874 - val_loss: 0.5009 - val_mean_squared_error: 0.4071\n",
            "Epoch 22/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4791 - mean_squared_error: 0.4661\n",
            "Epoch 22: val_loss did not improve from 0.50086\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4879 - mean_squared_error: 0.4819 - val_loss: 0.5155 - val_mean_squared_error: 0.4328\n",
            "Epoch 23/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4915 - mean_squared_error: 0.4641\n",
            "Epoch 23: val_loss did not improve from 0.50086\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.5125 - mean_squared_error: 0.5067 - val_loss: 0.5096 - val_mean_squared_error: 0.4233\n",
            "Epoch 24/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4648 - mean_squared_error: 0.4258\n",
            "Epoch 24: val_loss did not improve from 0.50086\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4984 - mean_squared_error: 0.4799 - val_loss: 0.5348 - val_mean_squared_error: 0.4443\n",
            "Epoch 25/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4856 - mean_squared_error: 0.4767\n",
            "Epoch 25: val_loss did not improve from 0.50086\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.4744 - mean_squared_error: 0.4571 - val_loss: 0.5100 - val_mean_squared_error: 0.4177\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4830 - mean_squared_error: 0.5049\n",
            "Epoch 26: val_loss did not improve from 0.50086\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4835 - mean_squared_error: 0.4667 - val_loss: 0.5180 - val_mean_squared_error: 0.4200\n",
            "Epoch 27/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4443 - mean_squared_error: 0.4283\n",
            "Epoch 27: val_loss improved from 0.50086 to 0.48229, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.4563 - mean_squared_error: 0.4543 - val_loss: 0.4823 - val_mean_squared_error: 0.3782\n",
            "Epoch 28/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4452 - mean_squared_error: 0.4374\n",
            "Epoch 28: val_loss improved from 0.48229 to 0.46533, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.4533 - mean_squared_error: 0.4187 - val_loss: 0.4653 - val_mean_squared_error: 0.3534\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4427 - mean_squared_error: 0.4383\n",
            "Epoch 29: val_loss improved from 0.46533 to 0.44045, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.4351 - mean_squared_error: 0.4059 - val_loss: 0.4404 - val_mean_squared_error: 0.3197\n",
            "Epoch 30/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3994 - mean_squared_error: 0.3471\n",
            "Epoch 30: val_loss improved from 0.44045 to 0.43567, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.4082 - mean_squared_error: 0.3694 - val_loss: 0.4357 - val_mean_squared_error: 0.3112\n",
            "Epoch 31/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4066 - mean_squared_error: 0.3851\n",
            "Epoch 31: val_loss did not improve from 0.43567\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.4189 - mean_squared_error: 0.3738 - val_loss: 0.4549 - val_mean_squared_error: 0.3321\n",
            "Epoch 32/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3273 - mean_squared_error: 0.2079\n",
            "Epoch 32: val_loss improved from 0.43567 to 0.40111, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.3987 - mean_squared_error: 0.3472 - val_loss: 0.4011 - val_mean_squared_error: 0.2807\n",
            "Epoch 33/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4165 - mean_squared_error: 0.3991\n",
            "Epoch 33: val_loss improved from 0.40111 to 0.39133, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.3885 - mean_squared_error: 0.3373 - val_loss: 0.3913 - val_mean_squared_error: 0.2724\n",
            "Epoch 34/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3387 - mean_squared_error: 0.2235\n",
            "Epoch 34: val_loss did not improve from 0.39133\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3815 - mean_squared_error: 0.3270 - val_loss: 0.4165 - val_mean_squared_error: 0.2864\n",
            "Epoch 35/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3834 - mean_squared_error: 0.3503\n",
            "Epoch 35: val_loss improved from 0.39133 to 0.38892, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 43ms/step - loss: 0.3842 - mean_squared_error: 0.3276 - val_loss: 0.3889 - val_mean_squared_error: 0.2620\n",
            "Epoch 36/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3579 - mean_squared_error: 0.3167\n",
            "Epoch 36: val_loss did not improve from 0.38892\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3662 - mean_squared_error: 0.3090 - val_loss: 0.3949 - val_mean_squared_error: 0.2638\n",
            "Epoch 37/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3796 - mean_squared_error: 0.3401\n",
            "Epoch 37: val_loss did not improve from 0.38892\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3681 - mean_squared_error: 0.3103 - val_loss: 0.4133 - val_mean_squared_error: 0.2830\n",
            "Epoch 38/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3605 - mean_squared_error: 0.3047\n",
            "Epoch 38: val_loss did not improve from 0.38892\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3923 - mean_squared_error: 0.3398 - val_loss: 0.4643 - val_mean_squared_error: 0.3336\n",
            "Epoch 39/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3920 - mean_squared_error: 0.3488\n",
            "Epoch 39: val_loss improved from 0.38892 to 0.37027, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.3776 - mean_squared_error: 0.3164 - val_loss: 0.3703 - val_mean_squared_error: 0.2353\n",
            "Epoch 40/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3632 - mean_squared_error: 0.3059\n",
            "Epoch 40: val_loss improved from 0.37027 to 0.35541, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.3598 - mean_squared_error: 0.2920 - val_loss: 0.3554 - val_mean_squared_error: 0.2266\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3329 - mean_squared_error: 0.2616\n",
            "Epoch 41: val_loss did not improve from 0.35541\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3481 - mean_squared_error: 0.2828 - val_loss: 0.3783 - val_mean_squared_error: 0.2392\n",
            "Epoch 42/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3704 - mean_squared_error: 0.2967\n",
            "Epoch 42: val_loss improved from 0.35541 to 0.35047, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.3654 - mean_squared_error: 0.2999 - val_loss: 0.3505 - val_mean_squared_error: 0.2188\n",
            "Epoch 43/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3916 - mean_squared_error: 0.3239\n",
            "Epoch 43: val_loss did not improve from 0.35047\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3843 - mean_squared_error: 0.3101 - val_loss: 0.3533 - val_mean_squared_error: 0.2158\n",
            "Epoch 44/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3562 - mean_squared_error: 0.2650\n",
            "Epoch 44: val_loss did not improve from 0.35047\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3708 - mean_squared_error: 0.2886 - val_loss: 0.3713 - val_mean_squared_error: 0.2289\n",
            "Epoch 45/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3186 - mean_squared_error: 0.2079\n",
            "Epoch 45: val_loss did not improve from 0.35047\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3482 - mean_squared_error: 0.2700 - val_loss: 0.3792 - val_mean_squared_error: 0.2366\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3644 - mean_squared_error: 0.2814\n",
            "Epoch 46: val_loss did not improve from 0.35047\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.3644 - mean_squared_error: 0.2814 - val_loss: 0.4018 - val_mean_squared_error: 0.2540\n",
            "Epoch 47/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3673 - mean_squared_error: 0.2829\n",
            "Epoch 47: val_loss improved from 0.35047 to 0.33580, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.3634 - mean_squared_error: 0.2748 - val_loss: 0.3358 - val_mean_squared_error: 0.1997\n",
            "Epoch 48/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3300 - mean_squared_error: 0.2467\n",
            "Epoch 48: val_loss improved from 0.33580 to 0.32521, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.3289 - mean_squared_error: 0.2535 - val_loss: 0.3252 - val_mean_squared_error: 0.1939\n",
            "Epoch 49/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3318 - mean_squared_error: 0.2608\n",
            "Epoch 49: val_loss did not improve from 0.32521\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3284 - mean_squared_error: 0.2534 - val_loss: 0.3254 - val_mean_squared_error: 0.1921\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3242 - mean_squared_error: 0.2580\n",
            "Epoch 50: val_loss improved from 0.32521 to 0.32273, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.3269 - mean_squared_error: 0.2492 - val_loss: 0.3227 - val_mean_squared_error: 0.1864\n",
            "Epoch 51/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3001 - mean_squared_error: 0.1872\n",
            "Epoch 51: val_loss improved from 0.32273 to 0.32064, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.3201 - mean_squared_error: 0.2439 - val_loss: 0.3206 - val_mean_squared_error: 0.1908\n",
            "Epoch 52/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3037 - mean_squared_error: 0.2314\n",
            "Epoch 52: val_loss did not improve from 0.32064\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3207 - mean_squared_error: 0.2455 - val_loss: 0.3241 - val_mean_squared_error: 0.1951\n",
            "Epoch 53/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3366 - mean_squared_error: 0.2360\n",
            "Epoch 53: val_loss did not improve from 0.32064\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.3438 - mean_squared_error: 0.2669 - val_loss: 0.3524 - val_mean_squared_error: 0.2011\n",
            "Epoch 54/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3291 - mean_squared_error: 0.2558\n",
            "Epoch 54: val_loss did not improve from 0.32064\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3381 - mean_squared_error: 0.2604 - val_loss: 0.3409 - val_mean_squared_error: 0.1959\n",
            "Epoch 55/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3306 - mean_squared_error: 0.2593\n",
            "Epoch 55: val_loss did not improve from 0.32064\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3169 - mean_squared_error: 0.2388 - val_loss: 0.3346 - val_mean_squared_error: 0.1913\n",
            "Epoch 56/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3353 - mean_squared_error: 0.2483\n",
            "Epoch 56: val_loss improved from 0.32064 to 0.31851, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3396 - mean_squared_error: 0.2567 - val_loss: 0.3185 - val_mean_squared_error: 0.1743\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3228 - mean_squared_error: 0.2393\n",
            "Epoch 57: val_loss improved from 0.31851 to 0.29903, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3228 - mean_squared_error: 0.2393 - val_loss: 0.2990 - val_mean_squared_error: 0.1696\n",
            "Epoch 58/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3187 - mean_squared_error: 0.2465\n",
            "Epoch 58: val_loss did not improve from 0.29903\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3170 - mean_squared_error: 0.2332 - val_loss: 0.2998 - val_mean_squared_error: 0.1677\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3070 - mean_squared_error: 0.2247\n",
            "Epoch 59: val_loss did not improve from 0.29903\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3070 - mean_squared_error: 0.2247 - val_loss: 0.3148 - val_mean_squared_error: 0.1730\n",
            "Epoch 60/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3105 - mean_squared_error: 0.2130\n",
            "Epoch 60: val_loss did not improve from 0.29903\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3361 - mean_squared_error: 0.2544 - val_loss: 0.3453 - val_mean_squared_error: 0.1908\n",
            "Epoch 61/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3069 - mean_squared_error: 0.2252\n",
            "Epoch 61: val_loss improved from 0.29903 to 0.29259, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.3053 - mean_squared_error: 0.2180 - val_loss: 0.2926 - val_mean_squared_error: 0.1521\n",
            "Epoch 62/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3023 - mean_squared_error: 0.2001\n",
            "Epoch 62: val_loss improved from 0.29259 to 0.28576, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3037 - mean_squared_error: 0.2165 - val_loss: 0.2858 - val_mean_squared_error: 0.1533\n",
            "Epoch 63/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3091 - mean_squared_error: 0.2309\n",
            "Epoch 63: val_loss did not improve from 0.28576\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3041 - mean_squared_error: 0.2161 - val_loss: 0.3133 - val_mean_squared_error: 0.1721\n",
            "Epoch 64/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2993 - mean_squared_error: 0.1989\n",
            "Epoch 64: val_loss did not improve from 0.28576\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3087 - mean_squared_error: 0.2216 - val_loss: 0.3007 - val_mean_squared_error: 0.1620\n",
            "Epoch 65/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2956 - mean_squared_error: 0.2031\n",
            "Epoch 65: val_loss did not improve from 0.28576\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3060 - mean_squared_error: 0.2142 - val_loss: 0.2870 - val_mean_squared_error: 0.1517\n",
            "Epoch 66/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3094 - mean_squared_error: 0.2264\n",
            "Epoch 66: val_loss did not improve from 0.28576\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3112 - mean_squared_error: 0.2225 - val_loss: 0.2912 - val_mean_squared_error: 0.1440\n",
            "Epoch 67/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2986 - mean_squared_error: 0.1946\n",
            "Epoch 67: val_loss improved from 0.28576 to 0.28362, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2981 - mean_squared_error: 0.2065 - val_loss: 0.2836 - val_mean_squared_error: 0.1459\n",
            "Epoch 68/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3084 - mean_squared_error: 0.2233\n",
            "Epoch 68: val_loss improved from 0.28362 to 0.27627, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2935 - mean_squared_error: 0.2042 - val_loss: 0.2763 - val_mean_squared_error: 0.1384\n",
            "Epoch 69/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2944 - mean_squared_error: 0.2049\n",
            "Epoch 69: val_loss improved from 0.27627 to 0.27425, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.2951 - mean_squared_error: 0.2037 - val_loss: 0.2742 - val_mean_squared_error: 0.1339\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2968 - mean_squared_error: 0.2149\n",
            "Epoch 70: val_loss did not improve from 0.27425\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3011 - mean_squared_error: 0.2066 - val_loss: 0.3331 - val_mean_squared_error: 0.1743\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3052 - mean_squared_error: 0.2086\n",
            "Epoch 71: val_loss did not improve from 0.27425\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3052 - mean_squared_error: 0.2086 - val_loss: 0.2906 - val_mean_squared_error: 0.1464\n",
            "Epoch 72/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3027 - mean_squared_error: 0.2151\n",
            "Epoch 72: val_loss did not improve from 0.27425\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2929 - mean_squared_error: 0.1990 - val_loss: 0.2787 - val_mean_squared_error: 0.1367\n",
            "Epoch 73/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2810 - mean_squared_error: 0.1883\n",
            "Epoch 73: val_loss did not improve from 0.27425\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2861 - mean_squared_error: 0.1935 - val_loss: 0.2845 - val_mean_squared_error: 0.1382\n",
            "Epoch 74/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2928 - mean_squared_error: 0.1995\n",
            "Epoch 74: val_loss improved from 0.27425 to 0.27222, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2867 - mean_squared_error: 0.1919 - val_loss: 0.2722 - val_mean_squared_error: 0.1315\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2854 - mean_squared_error: 0.1902\n",
            "Epoch 75: val_loss improved from 0.27222 to 0.26469, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2854 - mean_squared_error: 0.1902 - val_loss: 0.2647 - val_mean_squared_error: 0.1253\n",
            "Epoch 76/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2846 - mean_squared_error: 0.1938\n",
            "Epoch 76: val_loss did not improve from 0.26469\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2856 - mean_squared_error: 0.1921 - val_loss: 0.2828 - val_mean_squared_error: 0.1365\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2881 - mean_squared_error: 0.1917\n",
            "Epoch 77: val_loss improved from 0.26469 to 0.25927, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.2881 - mean_squared_error: 0.1917 - val_loss: 0.2593 - val_mean_squared_error: 0.1195\n",
            "Epoch 78/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2817 - mean_squared_error: 0.1712\n",
            "Epoch 78: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2893 - mean_squared_error: 0.1881 - val_loss: 0.2720 - val_mean_squared_error: 0.1266\n",
            "Epoch 79/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2859 - mean_squared_error: 0.1889\n",
            "Epoch 79: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2861 - mean_squared_error: 0.1878 - val_loss: 0.3033 - val_mean_squared_error: 0.1474\n",
            "Epoch 80/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3001 - mean_squared_error: 0.2020\n",
            "Epoch 80: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3124 - mean_squared_error: 0.2111 - val_loss: 0.2836 - val_mean_squared_error: 0.1363\n",
            "Epoch 81/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3567 - mean_squared_error: 0.2694\n",
            "Epoch 81: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3499 - mean_squared_error: 0.2621 - val_loss: 0.2846 - val_mean_squared_error: 0.1307\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3053 - mean_squared_error: 0.1927\n",
            "Epoch 82: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3053 - mean_squared_error: 0.1927 - val_loss: 0.3214 - val_mean_squared_error: 0.1668\n",
            "Epoch 83/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3143 - mean_squared_error: 0.2188\n",
            "Epoch 83: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3252 - mean_squared_error: 0.2356 - val_loss: 0.2724 - val_mean_squared_error: 0.1302\n",
            "Epoch 84/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3124 - mean_squared_error: 0.2130\n",
            "Epoch 84: val_loss did not improve from 0.25927\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3015 - mean_squared_error: 0.1972 - val_loss: 0.2714 - val_mean_squared_error: 0.1260\n",
            "Epoch 85/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2830 - mean_squared_error: 0.1878\n",
            "Epoch 85: val_loss improved from 0.25927 to 0.24691, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2752 - mean_squared_error: 0.1782 - val_loss: 0.2469 - val_mean_squared_error: 0.1097\n",
            "Epoch 86/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2900 - mean_squared_error: 0.1903\n",
            "Epoch 86: val_loss did not improve from 0.24691\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2756 - mean_squared_error: 0.1721 - val_loss: 0.2915 - val_mean_squared_error: 0.1339\n",
            "Epoch 87/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2593 - mean_squared_error: 0.1502\n",
            "Epoch 87: val_loss did not improve from 0.24691\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2749 - mean_squared_error: 0.1730 - val_loss: 0.2623 - val_mean_squared_error: 0.1144\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2758 - mean_squared_error: 0.1743\n",
            "Epoch 88: val_loss did not improve from 0.24691\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2758 - mean_squared_error: 0.1743 - val_loss: 0.2677 - val_mean_squared_error: 0.1175\n",
            "Epoch 89/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2786 - mean_squared_error: 0.1803\n",
            "Epoch 89: val_loss did not improve from 0.24691\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2757 - mean_squared_error: 0.1712 - val_loss: 0.2682 - val_mean_squared_error: 0.1150\n",
            "Epoch 90/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2848 - mean_squared_error: 0.1898\n",
            "Epoch 90: val_loss improved from 0.24691 to 0.24255, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2720 - mean_squared_error: 0.1685 - val_loss: 0.2426 - val_mean_squared_error: 0.1056\n",
            "Epoch 91/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2853 - mean_squared_error: 0.1858\n",
            "Epoch 91: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2767 - mean_squared_error: 0.1741 - val_loss: 0.2879 - val_mean_squared_error: 0.1341\n",
            "Epoch 92/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2918 - mean_squared_error: 0.1905\n",
            "Epoch 92: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2745 - mean_squared_error: 0.1716 - val_loss: 0.2461 - val_mean_squared_error: 0.1022\n",
            "Epoch 93/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2706 - mean_squared_error: 0.1617\n",
            "Epoch 93: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2698 - mean_squared_error: 0.1666 - val_loss: 0.2556 - val_mean_squared_error: 0.1050\n",
            "Epoch 94/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2826 - mean_squared_error: 0.1750\n",
            "Epoch 94: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2717 - mean_squared_error: 0.1681 - val_loss: 0.2513 - val_mean_squared_error: 0.1057\n",
            "Epoch 95/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.1706\n",
            "Epoch 95: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2708 - mean_squared_error: 0.1647 - val_loss: 0.2552 - val_mean_squared_error: 0.1062\n",
            "Epoch 96/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2875 - mean_squared_error: 0.1691\n",
            "Epoch 96: val_loss did not improve from 0.24255\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2873 - mean_squared_error: 0.1769 - val_loss: 0.2916 - val_mean_squared_error: 0.1374\n",
            "Epoch 97/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3041 - mean_squared_error: 0.1991\n",
            "Epoch 97: val_loss improved from 0.24255 to 0.23975, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2981 - mean_squared_error: 0.1894 - val_loss: 0.2398 - val_mean_squared_error: 0.1025\n",
            "Epoch 98/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2775 - mean_squared_error: 0.1618\n",
            "Epoch 98: val_loss did not improve from 0.23975\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2843 - mean_squared_error: 0.1718 - val_loss: 0.2447 - val_mean_squared_error: 0.0966\n",
            "Epoch 99/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2698 - mean_squared_error: 0.1611\n",
            "Epoch 99: val_loss improved from 0.23975 to 0.23969, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2718 - mean_squared_error: 0.1647 - val_loss: 0.2397 - val_mean_squared_error: 0.0965\n",
            "Epoch 100/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2824 - mean_squared_error: 0.1781\n",
            "Epoch 100: val_loss did not improve from 0.23969\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2730 - mean_squared_error: 0.1655 - val_loss: 0.2554 - val_mean_squared_error: 0.1058\n",
            "Epoch 101/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2767 - mean_squared_error: 0.1774\n",
            "Epoch 101: val_loss improved from 0.23969 to 0.23892, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2723 - mean_squared_error: 0.1667 - val_loss: 0.2389 - val_mean_squared_error: 0.1029\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2722 - mean_squared_error: 0.1622\n",
            "Epoch 102: val_loss did not improve from 0.23892\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2722 - mean_squared_error: 0.1622 - val_loss: 0.2511 - val_mean_squared_error: 0.1014\n",
            "Epoch 103/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2704 - mean_squared_error: 0.1667\n",
            "Epoch 103: val_loss did not improve from 0.23892\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2628 - mean_squared_error: 0.1555 - val_loss: 0.2423 - val_mean_squared_error: 0.0949\n",
            "Epoch 104/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2628 - mean_squared_error: 0.1585\n",
            "Epoch 104: val_loss improved from 0.23892 to 0.23666, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2628 - mean_squared_error: 0.1542 - val_loss: 0.2367 - val_mean_squared_error: 0.0901\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2644 - mean_squared_error: 0.1561\n",
            "Epoch 105: val_loss did not improve from 0.23666\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2644 - mean_squared_error: 0.1561 - val_loss: 0.2384 - val_mean_squared_error: 0.0921\n",
            "Epoch 106/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2616 - mean_squared_error: 0.1612\n",
            "Epoch 106: val_loss did not improve from 0.23666\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2591 - mean_squared_error: 0.1547 - val_loss: 0.2613 - val_mean_squared_error: 0.1073\n",
            "Epoch 107/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2789 - mean_squared_error: 0.1601\n",
            "Epoch 107: val_loss did not improve from 0.23666\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2733 - mean_squared_error: 0.1605 - val_loss: 0.2383 - val_mean_squared_error: 0.0994\n",
            "Epoch 108/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2672 - mean_squared_error: 0.1558\n",
            "Epoch 108: val_loss did not improve from 0.23666\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2816 - mean_squared_error: 0.1718 - val_loss: 0.2547 - val_mean_squared_error: 0.1021\n",
            "Epoch 109/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2948 - mean_squared_error: 0.1786\n",
            "Epoch 109: val_loss did not improve from 0.23666\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2924 - mean_squared_error: 0.1794 - val_loss: 0.2861 - val_mean_squared_error: 0.1270\n",
            "Epoch 110/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3273 - mean_squared_error: 0.2279\n",
            "Epoch 110: val_loss improved from 0.23666 to 0.22705, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 27ms/step - loss: 0.3165 - mean_squared_error: 0.2101 - val_loss: 0.2270 - val_mean_squared_error: 0.0830\n",
            "Epoch 111/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2659 - mean_squared_error: 0.1374\n",
            "Epoch 111: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2751 - mean_squared_error: 0.1567 - val_loss: 0.2439 - val_mean_squared_error: 0.0928\n",
            "Epoch 112/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2682 - mean_squared_error: 0.1616\n",
            "Epoch 112: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2576 - mean_squared_error: 0.1495 - val_loss: 0.2692 - val_mean_squared_error: 0.1111\n",
            "Epoch 113/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2686 - mean_squared_error: 0.1634\n",
            "Epoch 113: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2737 - mean_squared_error: 0.1646 - val_loss: 0.2393 - val_mean_squared_error: 0.0952\n",
            "Epoch 114/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.1365\n",
            "Epoch 114: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2605 - mean_squared_error: 0.1503 - val_loss: 0.2728 - val_mean_squared_error: 0.1122\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2667 - mean_squared_error: 0.1499\n",
            "Epoch 115: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2667 - mean_squared_error: 0.1499 - val_loss: 0.2704 - val_mean_squared_error: 0.1193\n",
            "Epoch 116/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2911 - mean_squared_error: 0.1529\n",
            "Epoch 116: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2881 - mean_squared_error: 0.1595 - val_loss: 0.3236 - val_mean_squared_error: 0.1660\n",
            "Epoch 117/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2965 - mean_squared_error: 0.1806\n",
            "Epoch 117: val_loss did not improve from 0.22705\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2898 - mean_squared_error: 0.1703 - val_loss: 0.2321 - val_mean_squared_error: 0.0900\n",
            "Epoch 118/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2806 - mean_squared_error: 0.1699\n",
            "Epoch 118: val_loss improved from 0.22705 to 0.21859, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2665 - mean_squared_error: 0.1544 - val_loss: 0.2186 - val_mean_squared_error: 0.0783\n",
            "Epoch 119/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2459 - mean_squared_error: 0.1310\n",
            "Epoch 119: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2599 - mean_squared_error: 0.1510 - val_loss: 0.2781 - val_mean_squared_error: 0.1193\n",
            "Epoch 120/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2791 - mean_squared_error: 0.1505\n",
            "Epoch 120: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2830 - mean_squared_error: 0.1702 - val_loss: 0.2313 - val_mean_squared_error: 0.0826\n",
            "Epoch 121/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2603 - mean_squared_error: 0.1433\n",
            "Epoch 121: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2587 - mean_squared_error: 0.1448 - val_loss: 0.2279 - val_mean_squared_error: 0.0810\n",
            "Epoch 122/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2575 - mean_squared_error: 0.1424\n",
            "Epoch 122: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2606 - mean_squared_error: 0.1444 - val_loss: 0.2583 - val_mean_squared_error: 0.0978\n",
            "Epoch 123/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2553 - mean_squared_error: 0.1373\n",
            "Epoch 123: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2609 - mean_squared_error: 0.1456 - val_loss: 0.2389 - val_mean_squared_error: 0.0867\n",
            "Epoch 124/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2407 - mean_squared_error: 0.1128\n",
            "Epoch 124: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2681 - mean_squared_error: 0.1513 - val_loss: 0.2309 - val_mean_squared_error: 0.0809\n",
            "Epoch 125/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2535 - mean_squared_error: 0.1363\n",
            "Epoch 125: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2546 - mean_squared_error: 0.1417 - val_loss: 0.2689 - val_mean_squared_error: 0.1097\n",
            "Epoch 126/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2514 - mean_squared_error: 0.1279\n",
            "Epoch 126: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2621 - mean_squared_error: 0.1471 - val_loss: 0.2191 - val_mean_squared_error: 0.0728\n",
            "Epoch 127/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2588 - mean_squared_error: 0.1352\n",
            "Epoch 127: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2545 - mean_squared_error: 0.1405 - val_loss: 0.2190 - val_mean_squared_error: 0.0744\n",
            "Epoch 128/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2697 - mean_squared_error: 0.1554\n",
            "Epoch 128: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2719 - mean_squared_error: 0.1540 - val_loss: 0.2403 - val_mean_squared_error: 0.0844\n",
            "Epoch 129/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.1473\n",
            "Epoch 129: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2567 - mean_squared_error: 0.1439 - val_loss: 0.2433 - val_mean_squared_error: 0.0873\n",
            "Epoch 130/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.1526\n",
            "Epoch 130: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2527 - mean_squared_error: 0.1389 - val_loss: 0.2476 - val_mean_squared_error: 0.0880\n",
            "Epoch 131/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2494 - mean_squared_error: 0.1513\n",
            "Epoch 131: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2504 - mean_squared_error: 0.1394 - val_loss: 0.2530 - val_mean_squared_error: 0.0958\n",
            "Epoch 132/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1377\n",
            "Epoch 132: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2497 - mean_squared_error: 0.1385 - val_loss: 0.2237 - val_mean_squared_error: 0.0736\n",
            "Epoch 133/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2674 - mean_squared_error: 0.1574\n",
            "Epoch 133: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2493 - mean_squared_error: 0.1368 - val_loss: 0.2197 - val_mean_squared_error: 0.0714\n",
            "Epoch 134/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2487 - mean_squared_error: 0.1305\n",
            "Epoch 134: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2500 - mean_squared_error: 0.1385 - val_loss: 0.2507 - val_mean_squared_error: 0.0948\n",
            "Epoch 135/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2529 - mean_squared_error: 0.1419\n",
            "Epoch 135: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2663 - mean_squared_error: 0.1474 - val_loss: 0.2489 - val_mean_squared_error: 0.0957\n",
            "Epoch 136/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3012 - mean_squared_error: 0.1820\n",
            "Epoch 136: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2752 - mean_squared_error: 0.1551 - val_loss: 0.2284 - val_mean_squared_error: 0.0797\n",
            "Epoch 137/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3017 - mean_squared_error: 0.1835\n",
            "Epoch 137: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2732 - mean_squared_error: 0.1566 - val_loss: 0.2570 - val_mean_squared_error: 0.0955\n",
            "Epoch 138/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2471 - mean_squared_error: 0.1393\n",
            "Epoch 138: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2570 - mean_squared_error: 0.1429 - val_loss: 0.2387 - val_mean_squared_error: 0.0866\n",
            "Epoch 139/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2606 - mean_squared_error: 0.1468\n",
            "Epoch 139: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2587 - mean_squared_error: 0.1376 - val_loss: 0.2701 - val_mean_squared_error: 0.1049\n",
            "Epoch 140/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2169 - mean_squared_error: 0.0919\n",
            "Epoch 140: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2457 - mean_squared_error: 0.1336 - val_loss: 0.2395 - val_mean_squared_error: 0.0819\n",
            "Epoch 141/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2597 - mean_squared_error: 0.1525\n",
            "Epoch 141: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2500 - mean_squared_error: 0.1344 - val_loss: 0.2411 - val_mean_squared_error: 0.0852\n",
            "Epoch 142/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2553 - mean_squared_error: 0.1423\n",
            "Epoch 142: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2527 - mean_squared_error: 0.1362 - val_loss: 0.2599 - val_mean_squared_error: 0.1014\n",
            "Epoch 143/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2473 - mean_squared_error: 0.1406\n",
            "Epoch 143: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2432 - mean_squared_error: 0.1323 - val_loss: 0.2339 - val_mean_squared_error: 0.0756\n",
            "Epoch 144/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2750 - mean_squared_error: 0.1580\n",
            "Epoch 144: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2547 - mean_squared_error: 0.1351 - val_loss: 0.2325 - val_mean_squared_error: 0.0764\n",
            "Epoch 145/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2512 - mean_squared_error: 0.1359\n",
            "Epoch 145: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2432 - mean_squared_error: 0.1310 - val_loss: 0.2320 - val_mean_squared_error: 0.0767\n",
            "Epoch 146/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.1355\n",
            "Epoch 146: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2488 - mean_squared_error: 0.1331 - val_loss: 0.2290 - val_mean_squared_error: 0.0818\n",
            "Epoch 147/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.1380\n",
            "Epoch 147: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2603 - mean_squared_error: 0.1401 - val_loss: 0.2481 - val_mean_squared_error: 0.0896\n",
            "Epoch 148/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2623 - mean_squared_error: 0.1484\n",
            "Epoch 148: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2525 - mean_squared_error: 0.1352 - val_loss: 0.2486 - val_mean_squared_error: 0.0861\n",
            "Epoch 149/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2424 - mean_squared_error: 0.1249\n",
            "Epoch 149: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2484 - mean_squared_error: 0.1313 - val_loss: 0.2282 - val_mean_squared_error: 0.0734\n",
            "Epoch 150/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2670 - mean_squared_error: 0.1559\n",
            "Epoch 150: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2471 - mean_squared_error: 0.1316 - val_loss: 0.2521 - val_mean_squared_error: 0.0961\n",
            "Epoch 151/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2519 - mean_squared_error: 0.1380\n",
            "Epoch 151: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2588 - mean_squared_error: 0.1399 - val_loss: 0.2339 - val_mean_squared_error: 0.0758\n",
            "Epoch 152/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2718 - mean_squared_error: 0.1545\n",
            "Epoch 152: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2455 - mean_squared_error: 0.1297 - val_loss: 0.2365 - val_mean_squared_error: 0.0766\n",
            "Epoch 153/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2458 - mean_squared_error: 0.1199\n",
            "Epoch 153: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2484 - mean_squared_error: 0.1311 - val_loss: 0.2456 - val_mean_squared_error: 0.0848\n",
            "Epoch 154/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2578 - mean_squared_error: 0.1301\n",
            "Epoch 154: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2497 - mean_squared_error: 0.1293 - val_loss: 0.2442 - val_mean_squared_error: 0.0852\n",
            "Epoch 155/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2311 - mean_squared_error: 0.1141\n",
            "Epoch 155: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2513 - mean_squared_error: 0.1331 - val_loss: 0.2728 - val_mean_squared_error: 0.1114\n",
            "Epoch 156/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2530 - mean_squared_error: 0.1226\n",
            "Epoch 156: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2458 - mean_squared_error: 0.1280 - val_loss: 0.2433 - val_mean_squared_error: 0.0826\n",
            "Epoch 157/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.1403\n",
            "Epoch 157: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2697 - mean_squared_error: 0.1479 - val_loss: 0.2652 - val_mean_squared_error: 0.1066\n",
            "Epoch 158/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2533 - mean_squared_error: 0.1413\n",
            "Epoch 158: val_loss did not improve from 0.21859\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2522 - mean_squared_error: 0.1365 - val_loss: 0.2313 - val_mean_squared_error: 0.0738\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "Fold: 9\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 9\n",
            "Epoch 1/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 1.2034 - mean_squared_error: 2.5065 \n",
            "Epoch 1: val_loss improved from inf to 1.18838, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 6s 105ms/step - loss: 1.2140 - mean_squared_error: 2.4175 - val_loss: 1.1884 - val_mean_squared_error: 1.7905\n",
            "Epoch 2/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 1.0836 - mean_squared_error: 1.9834\n",
            "Epoch 2: val_loss improved from 1.18838 to 1.05691, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 1.0627 - mean_squared_error: 1.9136 - val_loss: 1.0569 - val_mean_squared_error: 1.5291\n",
            "Epoch 3/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.9406 - mean_squared_error: 1.5382\n",
            "Epoch 3: val_loss improved from 1.05691 to 0.92737, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.9520 - mean_squared_error: 1.5632 - val_loss: 0.9274 - val_mean_squared_error: 1.1130\n",
            "Epoch 4/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.8733 - mean_squared_error: 1.3300\n",
            "Epoch 4: val_loss improved from 0.92737 to 0.80639, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.8530 - mean_squared_error: 1.3022 - val_loss: 0.8064 - val_mean_squared_error: 0.9105\n",
            "Epoch 5/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7232 - mean_squared_error: 0.9344\n",
            "Epoch 5: val_loss improved from 0.80639 to 0.66358, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.7395 - mean_squared_error: 0.9916 - val_loss: 0.6636 - val_mean_squared_error: 0.5956\n",
            "Epoch 6/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.6583 - mean_squared_error: 0.7498\n",
            "Epoch 6: val_loss improved from 0.66358 to 0.52813, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.6537 - mean_squared_error: 0.7787 - val_loss: 0.5281 - val_mean_squared_error: 0.4428\n",
            "Epoch 7/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5920 - mean_squared_error: 0.7063\n",
            "Epoch 7: val_loss improved from 0.52813 to 0.44785, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.5815 - mean_squared_error: 0.6709 - val_loss: 0.4479 - val_mean_squared_error: 0.3138\n",
            "Epoch 8/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5328 - mean_squared_error: 0.5689\n",
            "Epoch 8: val_loss improved from 0.44785 to 0.42927, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.5341 - mean_squared_error: 0.5911 - val_loss: 0.4293 - val_mean_squared_error: 0.2855\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.5122 - mean_squared_error: 0.5583\n",
            "Epoch 9: val_loss improved from 0.42927 to 0.39767, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.5122 - mean_squared_error: 0.5583 - val_loss: 0.3977 - val_mean_squared_error: 0.2498\n",
            "Epoch 10/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.4945 - mean_squared_error: 0.4811\n",
            "Epoch 10: val_loss did not improve from 0.39767\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.5007 - mean_squared_error: 0.5349 - val_loss: 0.4061 - val_mean_squared_error: 0.2388\n",
            "Epoch 11/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4952 - mean_squared_error: 0.5119\n",
            "Epoch 11: val_loss did not improve from 0.39767\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.5015 - mean_squared_error: 0.5267 - val_loss: 0.4137 - val_mean_squared_error: 0.2507\n",
            "Epoch 12/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4656 - mean_squared_error: 0.4630\n",
            "Epoch 12: val_loss improved from 0.39767 to 0.38287, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.4739 - mean_squared_error: 0.4904 - val_loss: 0.3829 - val_mean_squared_error: 0.2236\n",
            "Epoch 13/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4600 - mean_squared_error: 0.4663\n",
            "Epoch 13: val_loss did not improve from 0.38287\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4581 - mean_squared_error: 0.4651 - val_loss: 0.4106 - val_mean_squared_error: 0.2461\n",
            "Epoch 14/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4619 - mean_squared_error: 0.4800\n",
            "Epoch 14: val_loss improved from 0.38287 to 0.37908, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.4481 - mean_squared_error: 0.4533 - val_loss: 0.3791 - val_mean_squared_error: 0.2183\n",
            "Epoch 15/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4342 - mean_squared_error: 0.4067\n",
            "Epoch 15: val_loss improved from 0.37908 to 0.35086, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.4499 - mean_squared_error: 0.4437 - val_loss: 0.3509 - val_mean_squared_error: 0.1854\n",
            "Epoch 16/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4301 - mean_squared_error: 0.4036\n",
            "Epoch 16: val_loss did not improve from 0.35086\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4300 - mean_squared_error: 0.4199 - val_loss: 0.3623 - val_mean_squared_error: 0.1858\n",
            "Epoch 17/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4178 - mean_squared_error: 0.3708\n",
            "Epoch 17: val_loss did not improve from 0.35086\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4312 - mean_squared_error: 0.4115 - val_loss: 0.3729 - val_mean_squared_error: 0.2020\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.4091 - mean_squared_error: 0.3918\n",
            "Epoch 18: val_loss did not improve from 0.35086\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4091 - mean_squared_error: 0.3918 - val_loss: 0.3511 - val_mean_squared_error: 0.1797\n",
            "Epoch 19/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4012 - mean_squared_error: 0.3739\n",
            "Epoch 19: val_loss improved from 0.35086 to 0.33307, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.4041 - mean_squared_error: 0.3812 - val_loss: 0.3331 - val_mean_squared_error: 0.1679\n",
            "Epoch 20/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3864 - mean_squared_error: 0.3695\n",
            "Epoch 20: val_loss improved from 0.33307 to 0.32399, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.3954 - mean_squared_error: 0.3643 - val_loss: 0.3240 - val_mean_squared_error: 0.1560\n",
            "Epoch 21/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3699 - mean_squared_error: 0.3138\n",
            "Epoch 21: val_loss did not improve from 0.32399\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3847 - mean_squared_error: 0.3458 - val_loss: 0.3499 - val_mean_squared_error: 0.1817\n",
            "Epoch 22/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3929 - mean_squared_error: 0.3438\n",
            "Epoch 22: val_loss improved from 0.32399 to 0.31311, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.3894 - mean_squared_error: 0.3448 - val_loss: 0.3131 - val_mean_squared_error: 0.1467\n",
            "Epoch 23/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3713 - mean_squared_error: 0.3213\n",
            "Epoch 23: val_loss did not improve from 0.31311\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3807 - mean_squared_error: 0.3360 - val_loss: 0.3155 - val_mean_squared_error: 0.1448\n",
            "Epoch 24/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3476 - mean_squared_error: 0.2753\n",
            "Epoch 24: val_loss improved from 0.31311 to 0.30464, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.3684 - mean_squared_error: 0.3171 - val_loss: 0.3046 - val_mean_squared_error: 0.1428\n",
            "Epoch 25/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3686 - mean_squared_error: 0.3218\n",
            "Epoch 25: val_loss improved from 0.30464 to 0.29086, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.3683 - mean_squared_error: 0.3140 - val_loss: 0.2909 - val_mean_squared_error: 0.1280\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3609 - mean_squared_error: 0.3270\n",
            "Epoch 26: val_loss did not improve from 0.29086\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3675 - mean_squared_error: 0.3062 - val_loss: 0.3029 - val_mean_squared_error: 0.1307\n",
            "Epoch 27/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3836 - mean_squared_error: 0.2989\n",
            "Epoch 27: val_loss did not improve from 0.29086\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3803 - mean_squared_error: 0.3039 - val_loss: 0.3478 - val_mean_squared_error: 0.1694\n",
            "Epoch 28/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3659 - mean_squared_error: 0.2979\n",
            "Epoch 28: val_loss did not improve from 0.29086\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3538 - mean_squared_error: 0.2854 - val_loss: 0.2987 - val_mean_squared_error: 0.1251\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3584 - mean_squared_error: 0.3117\n",
            "Epoch 29: val_loss improved from 0.29086 to 0.29058, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3462 - mean_squared_error: 0.2783 - val_loss: 0.2906 - val_mean_squared_error: 0.1211\n",
            "Epoch 30/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3236 - mean_squared_error: 0.2421\n",
            "Epoch 30: val_loss did not improve from 0.29058\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3413 - mean_squared_error: 0.2695 - val_loss: 0.2932 - val_mean_squared_error: 0.1277\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3389 - mean_squared_error: 0.2628\n",
            "Epoch 31: val_loss did not improve from 0.29058\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3389 - mean_squared_error: 0.2628 - val_loss: 0.2995 - val_mean_squared_error: 0.1375\n",
            "Epoch 32/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2873 - mean_squared_error: 0.1849\n",
            "Epoch 32: val_loss improved from 0.29058 to 0.27357, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.3272 - mean_squared_error: 0.2531 - val_loss: 0.2736 - val_mean_squared_error: 0.1101\n",
            "Epoch 33/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3379 - mean_squared_error: 0.2666\n",
            "Epoch 33: val_loss improved from 0.27357 to 0.26931, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3252 - mean_squared_error: 0.2468 - val_loss: 0.2693 - val_mean_squared_error: 0.1098\n",
            "Epoch 34/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3204 - mean_squared_error: 0.2463\n",
            "Epoch 34: val_loss did not improve from 0.26931\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3240 - mean_squared_error: 0.2432 - val_loss: 0.2776 - val_mean_squared_error: 0.1191\n",
            "Epoch 35/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3292 - mean_squared_error: 0.2480\n",
            "Epoch 35: val_loss did not improve from 0.26931\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3357 - mean_squared_error: 0.2474 - val_loss: 0.3125 - val_mean_squared_error: 0.1446\n",
            "Epoch 36/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3361 - mean_squared_error: 0.2649\n",
            "Epoch 36: val_loss improved from 0.26931 to 0.25814, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.3268 - mean_squared_error: 0.2405 - val_loss: 0.2581 - val_mean_squared_error: 0.1008\n",
            "Epoch 37/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3406 - mean_squared_error: 0.2578\n",
            "Epoch 37: val_loss improved from 0.25814 to 0.25814, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3296 - mean_squared_error: 0.2383 - val_loss: 0.2581 - val_mean_squared_error: 0.0992\n",
            "Epoch 38/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3378 - mean_squared_error: 0.2436\n",
            "Epoch 38: val_loss did not improve from 0.25814\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3233 - mean_squared_error: 0.2294 - val_loss: 0.2662 - val_mean_squared_error: 0.1114\n",
            "Epoch 39/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3142 - mean_squared_error: 0.2249\n",
            "Epoch 39: val_loss did not improve from 0.25814\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3189 - mean_squared_error: 0.2269 - val_loss: 0.3107 - val_mean_squared_error: 0.1459\n",
            "Epoch 40/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3167 - mean_squared_error: 0.2274\n",
            "Epoch 40: val_loss did not improve from 0.25814\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3283 - mean_squared_error: 0.2321 - val_loss: 0.2701 - val_mean_squared_error: 0.1084\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2983 - mean_squared_error: 0.1905\n",
            "Epoch 41: val_loss improved from 0.25814 to 0.25437, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3215 - mean_squared_error: 0.2255 - val_loss: 0.2544 - val_mean_squared_error: 0.1054\n",
            "Epoch 42/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2995 - mean_squared_error: 0.2020\n",
            "Epoch 42: val_loss did not improve from 0.25437\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3006 - mean_squared_error: 0.2078 - val_loss: 0.2615 - val_mean_squared_error: 0.1092\n",
            "Epoch 43/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2971 - mean_squared_error: 0.2062\n",
            "Epoch 43: val_loss improved from 0.25437 to 0.25388, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2990 - mean_squared_error: 0.2060 - val_loss: 0.2539 - val_mean_squared_error: 0.0950\n",
            "Epoch 44/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2868 - mean_squared_error: 0.1720\n",
            "Epoch 44: val_loss did not improve from 0.25388\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3072 - mean_squared_error: 0.2130 - val_loss: 0.2826 - val_mean_squared_error: 0.1254\n",
            "Epoch 45/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2916 - mean_squared_error: 0.1742\n",
            "Epoch 45: val_loss improved from 0.25388 to 0.24742, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.3040 - mean_squared_error: 0.2046 - val_loss: 0.2474 - val_mean_squared_error: 0.0959\n",
            "Epoch 46/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2748 - mean_squared_error: 0.1581\n",
            "Epoch 46: val_loss did not improve from 0.24742\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2853 - mean_squared_error: 0.1905 - val_loss: 0.2547 - val_mean_squared_error: 0.1063\n",
            "Epoch 47/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2915 - mean_squared_error: 0.1885\n",
            "Epoch 47: val_loss did not improve from 0.24742\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2868 - mean_squared_error: 0.1902 - val_loss: 0.2845 - val_mean_squared_error: 0.1259\n",
            "Epoch 48/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3132 - mean_squared_error: 0.2030\n",
            "Epoch 48: val_loss improved from 0.24742 to 0.24687, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 0.3053 - mean_squared_error: 0.1982 - val_loss: 0.2469 - val_mean_squared_error: 0.0899\n",
            "Epoch 49/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2906 - mean_squared_error: 0.1843\n",
            "Epoch 49: val_loss improved from 0.24687 to 0.24421, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2898 - mean_squared_error: 0.1845 - val_loss: 0.2442 - val_mean_squared_error: 0.0896\n",
            "Epoch 50/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2976 - mean_squared_error: 0.1966\n",
            "Epoch 50: val_loss improved from 0.24421 to 0.24136, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.2836 - mean_squared_error: 0.1799 - val_loss: 0.2414 - val_mean_squared_error: 0.0898\n",
            "Epoch 51/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2823 - mean_squared_error: 0.1749\n",
            "Epoch 51: val_loss did not improve from 0.24136\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2844 - mean_squared_error: 0.1793 - val_loss: 0.2859 - val_mean_squared_error: 0.1255\n",
            "Epoch 52/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2908 - mean_squared_error: 0.1856\n",
            "Epoch 52: val_loss did not improve from 0.24136\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.3079 - mean_squared_error: 0.1940 - val_loss: 0.2537 - val_mean_squared_error: 0.0986\n",
            "Epoch 53/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2733 - mean_squared_error: 0.1523\n",
            "Epoch 53: val_loss did not improve from 0.24136\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2759 - mean_squared_error: 0.1703 - val_loss: 0.2447 - val_mean_squared_error: 0.0924\n",
            "Epoch 54/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.1625\n",
            "Epoch 54: val_loss did not improve from 0.24136\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2768 - mean_squared_error: 0.1696 - val_loss: 0.2424 - val_mean_squared_error: 0.0930\n",
            "Epoch 55/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2943 - mean_squared_error: 0.1871\n",
            "Epoch 55: val_loss improved from 0.24136 to 0.23944, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2710 - mean_squared_error: 0.1667 - val_loss: 0.2394 - val_mean_squared_error: 0.0883\n",
            "Epoch 56/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2649 - mean_squared_error: 0.1601\n",
            "Epoch 56: val_loss did not improve from 0.23944\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2749 - mean_squared_error: 0.1654 - val_loss: 0.2572 - val_mean_squared_error: 0.1040\n",
            "Epoch 57/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2616 - mean_squared_error: 0.1408\n",
            "Epoch 57: val_loss improved from 0.23944 to 0.23574, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 0.2734 - mean_squared_error: 0.1613 - val_loss: 0.2357 - val_mean_squared_error: 0.0842\n",
            "Epoch 58/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2718 - mean_squared_error: 0.1597\n",
            "Epoch 58: val_loss improved from 0.23574 to 0.23497, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.2734 - mean_squared_error: 0.1600 - val_loss: 0.2350 - val_mean_squared_error: 0.0818\n",
            "Epoch 59/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2748 - mean_squared_error: 0.1573\n",
            "Epoch 59: val_loss improved from 0.23497 to 0.23454, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.2820 - mean_squared_error: 0.1629 - val_loss: 0.2345 - val_mean_squared_error: 0.0876\n",
            "Epoch 60/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.1445\n",
            "Epoch 60: val_loss did not improve from 0.23454\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2689 - mean_squared_error: 0.1536 - val_loss: 0.2551 - val_mean_squared_error: 0.0996\n",
            "Epoch 61/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2775 - mean_squared_error: 0.1436\n",
            "Epoch 61: val_loss did not improve from 0.23454\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2735 - mean_squared_error: 0.1566 - val_loss: 0.2355 - val_mean_squared_error: 0.0812\n",
            "Epoch 62/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2593 - mean_squared_error: 0.1389\n",
            "Epoch 62: val_loss improved from 0.23454 to 0.23249, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.2659 - mean_squared_error: 0.1495 - val_loss: 0.2325 - val_mean_squared_error: 0.0847\n",
            "Epoch 63/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2663 - mean_squared_error: 0.1505\n",
            "Epoch 63: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2669 - mean_squared_error: 0.1482 - val_loss: 0.2385 - val_mean_squared_error: 0.0817\n",
            "Epoch 64/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2612 - mean_squared_error: 0.1349\n",
            "Epoch 64: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2816 - mean_squared_error: 0.1587 - val_loss: 0.2468 - val_mean_squared_error: 0.0936\n",
            "Epoch 65/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2743 - mean_squared_error: 0.1503\n",
            "Epoch 65: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2802 - mean_squared_error: 0.1553 - val_loss: 0.2427 - val_mean_squared_error: 0.0866\n",
            "Epoch 66/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.1684\n",
            "Epoch 66: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2659 - mean_squared_error: 0.1468 - val_loss: 0.2386 - val_mean_squared_error: 0.0835\n",
            "Epoch 67/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2574 - mean_squared_error: 0.1227\n",
            "Epoch 67: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2649 - mean_squared_error: 0.1427 - val_loss: 0.2394 - val_mean_squared_error: 0.0892\n",
            "Epoch 68/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2539 - mean_squared_error: 0.1405\n",
            "Epoch 68: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2577 - mean_squared_error: 0.1395 - val_loss: 0.2469 - val_mean_squared_error: 0.0935\n",
            "Epoch 69/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2779 - mean_squared_error: 0.1520\n",
            "Epoch 69: val_loss did not improve from 0.23249\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2693 - mean_squared_error: 0.1410 - val_loss: 0.2360 - val_mean_squared_error: 0.0828\n",
            "Epoch 70/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2664 - mean_squared_error: 0.1477\n",
            "Epoch 70: val_loss improved from 0.23249 to 0.22886, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2650 - mean_squared_error: 0.1429 - val_loss: 0.2289 - val_mean_squared_error: 0.0788\n",
            "Epoch 71/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2632 - mean_squared_error: 0.1276\n",
            "Epoch 71: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2625 - mean_squared_error: 0.1390 - val_loss: 0.2647 - val_mean_squared_error: 0.1098\n",
            "Epoch 72/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2690 - mean_squared_error: 0.1492\n",
            "Epoch 72: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2591 - mean_squared_error: 0.1411 - val_loss: 0.2348 - val_mean_squared_error: 0.0781\n",
            "Epoch 73/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2576 - mean_squared_error: 0.1308\n",
            "Epoch 73: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2622 - mean_squared_error: 0.1371 - val_loss: 0.2322 - val_mean_squared_error: 0.0796\n",
            "Epoch 74/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.1450\n",
            "Epoch 74: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2595 - mean_squared_error: 0.1355 - val_loss: 0.2586 - val_mean_squared_error: 0.1065\n",
            "Epoch 75/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2703 - mean_squared_error: 0.1466\n",
            "Epoch 75: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2758 - mean_squared_error: 0.1458 - val_loss: 0.2361 - val_mean_squared_error: 0.0782\n",
            "Epoch 76/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2559 - mean_squared_error: 0.1345\n",
            "Epoch 76: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2591 - mean_squared_error: 0.1367 - val_loss: 0.2364 - val_mean_squared_error: 0.0783\n",
            "Epoch 77/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2556 - mean_squared_error: 0.1423\n",
            "Epoch 77: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2520 - mean_squared_error: 0.1304 - val_loss: 0.2613 - val_mean_squared_error: 0.1092\n",
            "Epoch 78/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2614 - mean_squared_error: 0.1334\n",
            "Epoch 78: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2693 - mean_squared_error: 0.1383 - val_loss: 0.2426 - val_mean_squared_error: 0.0842\n",
            "Epoch 79/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2769 - mean_squared_error: 0.1419\n",
            "Epoch 79: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2735 - mean_squared_error: 0.1444 - val_loss: 0.2489 - val_mean_squared_error: 0.0975\n",
            "Epoch 80/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2745 - mean_squared_error: 0.1477\n",
            "Epoch 80: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2598 - mean_squared_error: 0.1290 - val_loss: 0.2416 - val_mean_squared_error: 0.0865\n",
            "Epoch 81/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2477 - mean_squared_error: 0.1270\n",
            "Epoch 81: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2510 - mean_squared_error: 0.1238 - val_loss: 0.2312 - val_mean_squared_error: 0.0781\n",
            "Epoch 82/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2512 - mean_squared_error: 0.1230\n",
            "Epoch 82: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2529 - mean_squared_error: 0.1262 - val_loss: 0.2321 - val_mean_squared_error: 0.0776\n",
            "Epoch 83/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2291 - mean_squared_error: 0.0989\n",
            "Epoch 83: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2498 - mean_squared_error: 0.1230 - val_loss: 0.2660 - val_mean_squared_error: 0.1134\n",
            "Epoch 84/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2489 - mean_squared_error: 0.1227\n",
            "Epoch 84: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2594 - mean_squared_error: 0.1321 - val_loss: 0.2314 - val_mean_squared_error: 0.0771\n",
            "Epoch 85/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2727 - mean_squared_error: 0.1409\n",
            "Epoch 85: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2626 - mean_squared_error: 0.1323 - val_loss: 0.2400 - val_mean_squared_error: 0.0869\n",
            "Epoch 86/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2689 - mean_squared_error: 0.1416\n",
            "Epoch 86: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2699 - mean_squared_error: 0.1391 - val_loss: 0.2729 - val_mean_squared_error: 0.1237\n",
            "Epoch 87/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2555 - mean_squared_error: 0.1222\n",
            "Epoch 87: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2551 - mean_squared_error: 0.1249 - val_loss: 0.2303 - val_mean_squared_error: 0.0801\n",
            "Epoch 88/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2421 - mean_squared_error: 0.1085\n",
            "Epoch 88: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2534 - mean_squared_error: 0.1258 - val_loss: 0.2378 - val_mean_squared_error: 0.0805\n",
            "Epoch 89/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2688 - mean_squared_error: 0.1411\n",
            "Epoch 89: val_loss did not improve from 0.22886\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2475 - mean_squared_error: 0.1203 - val_loss: 0.2383 - val_mean_squared_error: 0.0909\n",
            "Epoch 90/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2378 - mean_squared_error: 0.1161\n",
            "Epoch 90: val_loss improved from 0.22886 to 0.22363, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.2458 - mean_squared_error: 0.1181 - val_loss: 0.2236 - val_mean_squared_error: 0.0790\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2433 - mean_squared_error: 0.1203\n",
            "Epoch 91: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2433 - mean_squared_error: 0.1203 - val_loss: 0.2290 - val_mean_squared_error: 0.0754\n",
            "Epoch 92/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2400 - mean_squared_error: 0.1163\n",
            "Epoch 92: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2381 - mean_squared_error: 0.1135 - val_loss: 0.2343 - val_mean_squared_error: 0.0889\n",
            "Epoch 93/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2340 - mean_squared_error: 0.1080\n",
            "Epoch 93: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2448 - mean_squared_error: 0.1155 - val_loss: 0.2357 - val_mean_squared_error: 0.0800\n",
            "Epoch 94/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2435 - mean_squared_error: 0.1154\n",
            "Epoch 94: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2498 - mean_squared_error: 0.1234 - val_loss: 0.2725 - val_mean_squared_error: 0.1296\n",
            "Epoch 95/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2402 - mean_squared_error: 0.1167\n",
            "Epoch 95: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2502 - mean_squared_error: 0.1186 - val_loss: 0.2297 - val_mean_squared_error: 0.0782\n",
            "Epoch 96/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2563 - mean_squared_error: 0.1246\n",
            "Epoch 96: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2648 - mean_squared_error: 0.1305 - val_loss: 0.2569 - val_mean_squared_error: 0.1102\n",
            "Epoch 97/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2447 - mean_squared_error: 0.1147\n",
            "Epoch 97: val_loss did not improve from 0.22363\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2470 - mean_squared_error: 0.1139 - val_loss: 0.2400 - val_mean_squared_error: 0.0899\n",
            "Epoch 98/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2342 - mean_squared_error: 0.1112\n",
            "Epoch 98: val_loss improved from 0.22363 to 0.22223, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2343 - mean_squared_error: 0.1083 - val_loss: 0.2222 - val_mean_squared_error: 0.0777\n",
            "Epoch 99/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2362 - mean_squared_error: 0.1100\n",
            "Epoch 99: val_loss did not improve from 0.22223\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2294 - mean_squared_error: 0.1062 - val_loss: 0.2503 - val_mean_squared_error: 0.1046\n",
            "Epoch 100/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2419 - mean_squared_error: 0.1171\n",
            "Epoch 100: val_loss did not improve from 0.22223\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2329 - mean_squared_error: 0.1100 - val_loss: 0.2270 - val_mean_squared_error: 0.0766\n",
            "Epoch 101/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2397 - mean_squared_error: 0.1098\n",
            "Epoch 101: val_loss did not improve from 0.22223\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2354 - mean_squared_error: 0.1074 - val_loss: 0.2295 - val_mean_squared_error: 0.0838\n",
            "Epoch 102/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.0947\n",
            "Epoch 102: val_loss improved from 0.22223 to 0.21971, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2281 - mean_squared_error: 0.1037 - val_loss: 0.2197 - val_mean_squared_error: 0.0800\n",
            "Epoch 103/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2329 - mean_squared_error: 0.1106\n",
            "Epoch 103: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2277 - mean_squared_error: 0.1041 - val_loss: 0.2239 - val_mean_squared_error: 0.0805\n",
            "Epoch 104/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2316 - mean_squared_error: 0.1074\n",
            "Epoch 104: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2290 - mean_squared_error: 0.1038 - val_loss: 0.2234 - val_mean_squared_error: 0.0829\n",
            "Epoch 105/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2247 - mean_squared_error: 0.0992\n",
            "Epoch 105: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2253 - mean_squared_error: 0.1015 - val_loss: 0.2232 - val_mean_squared_error: 0.0798\n",
            "Epoch 106/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2312 - mean_squared_error: 0.1017\n",
            "Epoch 106: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2249 - mean_squared_error: 0.1014 - val_loss: 0.2245 - val_mean_squared_error: 0.0792\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2257 - mean_squared_error: 0.1028\n",
            "Epoch 107: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2257 - mean_squared_error: 0.1028 - val_loss: 0.2338 - val_mean_squared_error: 0.0897\n",
            "Epoch 108/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2236 - mean_squared_error: 0.0898\n",
            "Epoch 108: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2325 - mean_squared_error: 0.1046 - val_loss: 0.2339 - val_mean_squared_error: 0.0939\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2339 - mean_squared_error: 0.1036\n",
            "Epoch 109: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2339 - mean_squared_error: 0.1036 - val_loss: 0.2310 - val_mean_squared_error: 0.0783\n",
            "Epoch 110/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2394 - mean_squared_error: 0.1130\n",
            "Epoch 110: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2341 - mean_squared_error: 0.1079 - val_loss: 0.2235 - val_mean_squared_error: 0.0840\n",
            "Epoch 111/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2245 - mean_squared_error: 0.0973\n",
            "Epoch 111: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2282 - mean_squared_error: 0.1012 - val_loss: 0.2486 - val_mean_squared_error: 0.1105\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2476 - mean_squared_error: 0.1161\n",
            "Epoch 112: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2476 - mean_squared_error: 0.1161 - val_loss: 0.2394 - val_mean_squared_error: 0.0866\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2409 - mean_squared_error: 0.1055\n",
            "Epoch 113: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2409 - mean_squared_error: 0.1055 - val_loss: 0.2993 - val_mean_squared_error: 0.1631\n",
            "Epoch 114/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2404 - mean_squared_error: 0.1172\n",
            "Epoch 114: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2475 - mean_squared_error: 0.1208 - val_loss: 0.2271 - val_mean_squared_error: 0.0770\n",
            "Epoch 115/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2319 - mean_squared_error: 0.1021\n",
            "Epoch 115: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2323 - mean_squared_error: 0.1015 - val_loss: 0.2315 - val_mean_squared_error: 0.0899\n",
            "Epoch 116/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2227 - mean_squared_error: 0.0944\n",
            "Epoch 116: val_loss did not improve from 0.21971\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2277 - mean_squared_error: 0.0997 - val_loss: 0.2291 - val_mean_squared_error: 0.0851\n",
            "Epoch 117/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2256 - mean_squared_error: 0.0968\n",
            "Epoch 117: val_loss improved from 0.21971 to 0.21817, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2261 - mean_squared_error: 0.0991 - val_loss: 0.2182 - val_mean_squared_error: 0.0838\n",
            "Epoch 118/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2164 - mean_squared_error: 0.0970\n",
            "Epoch 118: val_loss did not improve from 0.21817\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2232 - mean_squared_error: 0.0984 - val_loss: 0.2215 - val_mean_squared_error: 0.0814\n",
            "Epoch 119/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2234 - mean_squared_error: 0.0973\n",
            "Epoch 119: val_loss improved from 0.21817 to 0.21810, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 37ms/step - loss: 0.2232 - mean_squared_error: 0.0967 - val_loss: 0.2181 - val_mean_squared_error: 0.0771\n",
            "Epoch 120/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2223 - mean_squared_error: 0.0984\n",
            "Epoch 120: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2241 - mean_squared_error: 0.0968 - val_loss: 0.2188 - val_mean_squared_error: 0.0747\n",
            "Epoch 121/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2256 - mean_squared_error: 0.1013\n",
            "Epoch 121: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2251 - mean_squared_error: 0.0994 - val_loss: 0.2266 - val_mean_squared_error: 0.0791\n",
            "Epoch 122/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2369 - mean_squared_error: 0.1081\n",
            "Epoch 122: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2288 - mean_squared_error: 0.1011 - val_loss: 0.2235 - val_mean_squared_error: 0.0775\n",
            "Epoch 123/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2202 - mean_squared_error: 0.0967\n",
            "Epoch 123: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2254 - mean_squared_error: 0.0996 - val_loss: 0.2189 - val_mean_squared_error: 0.0770\n",
            "Epoch 124/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2125 - mean_squared_error: 0.0882\n",
            "Epoch 124: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2214 - mean_squared_error: 0.0968 - val_loss: 0.2209 - val_mean_squared_error: 0.0772\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2240 - mean_squared_error: 0.0970\n",
            "Epoch 125: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2240 - mean_squared_error: 0.0970 - val_loss: 0.2193 - val_mean_squared_error: 0.0763\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2175 - mean_squared_error: 0.0943\n",
            "Epoch 126: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2175 - mean_squared_error: 0.0943 - val_loss: 0.2423 - val_mean_squared_error: 0.1013\n",
            "Epoch 127/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2365 - mean_squared_error: 0.1074\n",
            "Epoch 127: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2350 - mean_squared_error: 0.1046 - val_loss: 0.2218 - val_mean_squared_error: 0.0754\n",
            "Epoch 128/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2099 - mean_squared_error: 0.0842\n",
            "Epoch 128: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2233 - mean_squared_error: 0.0954 - val_loss: 0.2187 - val_mean_squared_error: 0.0807\n",
            "Epoch 129/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2237 - mean_squared_error: 0.0925\n",
            "Epoch 129: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2292 - mean_squared_error: 0.0973 - val_loss: 0.2351 - val_mean_squared_error: 0.0813\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2410 - mean_squared_error: 0.1060\n",
            "Epoch 130: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2410 - mean_squared_error: 0.1060 - val_loss: 0.2307 - val_mean_squared_error: 0.0905\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2281 - mean_squared_error: 0.0951\n",
            "Epoch 131: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2281 - mean_squared_error: 0.0951 - val_loss: 0.2342 - val_mean_squared_error: 0.0918\n",
            "Epoch 132/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2198 - mean_squared_error: 0.0876\n",
            "Epoch 132: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2247 - mean_squared_error: 0.0947 - val_loss: 0.2320 - val_mean_squared_error: 0.0787\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2335 - mean_squared_error: 0.1008\n",
            "Epoch 133: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2335 - mean_squared_error: 0.1008 - val_loss: 0.2192 - val_mean_squared_error: 0.0818\n",
            "Epoch 134/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2317 - mean_squared_error: 0.1004\n",
            "Epoch 134: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2337 - mean_squared_error: 0.1030 - val_loss: 0.2317 - val_mean_squared_error: 0.0938\n",
            "Epoch 135/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2347 - mean_squared_error: 0.1043\n",
            "Epoch 135: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2259 - mean_squared_error: 0.0972 - val_loss: 0.2197 - val_mean_squared_error: 0.0778\n",
            "Epoch 136/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2168 - mean_squared_error: 0.0946\n",
            "Epoch 136: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2191 - mean_squared_error: 0.0940 - val_loss: 0.2193 - val_mean_squared_error: 0.0762\n",
            "Epoch 137/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2145 - mean_squared_error: 0.0924\n",
            "Epoch 137: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2164 - mean_squared_error: 0.0922 - val_loss: 0.2182 - val_mean_squared_error: 0.0769\n",
            "Epoch 138/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2245 - mean_squared_error: 0.0979\n",
            "Epoch 138: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2189 - mean_squared_error: 0.0933 - val_loss: 0.2191 - val_mean_squared_error: 0.0752\n",
            "Epoch 139/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2103 - mean_squared_error: 0.0888\n",
            "Epoch 139: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2218 - mean_squared_error: 0.0955 - val_loss: 0.2289 - val_mean_squared_error: 0.0882\n",
            "Epoch 140/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2270 - mean_squared_error: 0.0963\n",
            "Epoch 140: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2281 - mean_squared_error: 0.0960 - val_loss: 0.2328 - val_mean_squared_error: 0.0949\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2284 - mean_squared_error: 0.0993\n",
            "Epoch 141: val_loss did not improve from 0.21810\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.2284 - mean_squared_error: 0.0993 - val_loss: 0.2236 - val_mean_squared_error: 0.0790\n",
            "Epoch 142/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2358 - mean_squared_error: 0.1032\n",
            "Epoch 142: val_loss improved from 0.21810 to 0.21727, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2270 - mean_squared_error: 0.0960 - val_loss: 0.2173 - val_mean_squared_error: 0.0733\n",
            "Epoch 143/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2188 - mean_squared_error: 0.0925\n",
            "Epoch 143: val_loss did not improve from 0.21727\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2206 - mean_squared_error: 0.0926 - val_loss: 0.2260 - val_mean_squared_error: 0.0844\n",
            "Epoch 144/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.1003\n",
            "Epoch 144: val_loss improved from 0.21727 to 0.21596, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2191 - mean_squared_error: 0.0918 - val_loss: 0.2160 - val_mean_squared_error: 0.0751\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2159 - mean_squared_error: 0.0910\n",
            "Epoch 145: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.2159 - mean_squared_error: 0.0910 - val_loss: 0.2176 - val_mean_squared_error: 0.0768\n",
            "Epoch 146/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2119 - mean_squared_error: 0.0863\n",
            "Epoch 146: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2164 - mean_squared_error: 0.0923 - val_loss: 0.2173 - val_mean_squared_error: 0.0754\n",
            "Epoch 147/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2158 - mean_squared_error: 0.0907\n",
            "Epoch 147: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2181 - mean_squared_error: 0.0931 - val_loss: 0.2235 - val_mean_squared_error: 0.0847\n",
            "Epoch 148/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2208 - mean_squared_error: 0.0979\n",
            "Epoch 148: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2157 - mean_squared_error: 0.0917 - val_loss: 0.2233 - val_mean_squared_error: 0.0765\n",
            "Epoch 149/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2142 - mean_squared_error: 0.0920\n",
            "Epoch 149: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2167 - mean_squared_error: 0.0939 - val_loss: 0.2308 - val_mean_squared_error: 0.0968\n",
            "Epoch 150/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2154 - mean_squared_error: 0.0898\n",
            "Epoch 150: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2177 - mean_squared_error: 0.0928 - val_loss: 0.2270 - val_mean_squared_error: 0.0793\n",
            "Epoch 151/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2299 - mean_squared_error: 0.1064\n",
            "Epoch 151: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2249 - mean_squared_error: 0.0980 - val_loss: 0.2194 - val_mean_squared_error: 0.0770\n",
            "Epoch 152/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2118 - mean_squared_error: 0.0897\n",
            "Epoch 152: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2184 - mean_squared_error: 0.0923 - val_loss: 0.2199 - val_mean_squared_error: 0.0730\n",
            "Epoch 153/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2322 - mean_squared_error: 0.0993\n",
            "Epoch 153: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2275 - mean_squared_error: 0.0953 - val_loss: 0.2494 - val_mean_squared_error: 0.1111\n",
            "Epoch 154/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2303 - mean_squared_error: 0.0984\n",
            "Epoch 154: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2279 - mean_squared_error: 0.0961 - val_loss: 0.2180 - val_mean_squared_error: 0.0735\n",
            "Epoch 155/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2416 - mean_squared_error: 0.1084\n",
            "Epoch 155: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2427 - mean_squared_error: 0.1083 - val_loss: 0.2222 - val_mean_squared_error: 0.0767\n",
            "Epoch 156/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2193 - mean_squared_error: 0.0862\n",
            "Epoch 156: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2193 - mean_squared_error: 0.0906 - val_loss: 0.2261 - val_mean_squared_error: 0.0821\n",
            "Epoch 157/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2161 - mean_squared_error: 0.0906\n",
            "Epoch 157: val_loss did not improve from 0.21596\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2173 - mean_squared_error: 0.0915 - val_loss: 0.2193 - val_mean_squared_error: 0.0752\n",
            "Epoch 158/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2221 - mean_squared_error: 0.0986\n",
            "Epoch 158: val_loss improved from 0.21596 to 0.21535, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.2151 - mean_squared_error: 0.0906 - val_loss: 0.2153 - val_mean_squared_error: 0.0777\n",
            "Epoch 159/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2083 - mean_squared_error: 0.0899\n",
            "Epoch 159: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2157 - mean_squared_error: 0.0906 - val_loss: 0.2160 - val_mean_squared_error: 0.0752\n",
            "Epoch 160/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2215 - mean_squared_error: 0.0911\n",
            "Epoch 160: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2149 - mean_squared_error: 0.0900 - val_loss: 0.2201 - val_mean_squared_error: 0.0724\n",
            "Epoch 161/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2261 - mean_squared_error: 0.0917\n",
            "Epoch 161: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2284 - mean_squared_error: 0.0985 - val_loss: 0.2488 - val_mean_squared_error: 0.1056\n",
            "Epoch 162/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2385 - mean_squared_error: 0.1073\n",
            "Epoch 162: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2291 - mean_squared_error: 0.0956 - val_loss: 0.2308 - val_mean_squared_error: 0.0875\n",
            "Epoch 163/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2070 - mean_squared_error: 0.0900\n",
            "Epoch 163: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2171 - mean_squared_error: 0.0920 - val_loss: 0.2189 - val_mean_squared_error: 0.0730\n",
            "Epoch 164/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2225 - mean_squared_error: 0.0963\n",
            "Epoch 164: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2146 - mean_squared_error: 0.0902 - val_loss: 0.2247 - val_mean_squared_error: 0.0822\n",
            "Epoch 165/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2245 - mean_squared_error: 0.1009\n",
            "Epoch 165: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2124 - mean_squared_error: 0.0895 - val_loss: 0.2183 - val_mean_squared_error: 0.0746\n",
            "Epoch 166/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2226 - mean_squared_error: 0.0942\n",
            "Epoch 166: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2167 - mean_squared_error: 0.0910 - val_loss: 0.2208 - val_mean_squared_error: 0.0749\n",
            "Epoch 167/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2220 - mean_squared_error: 0.0913\n",
            "Epoch 167: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2198 - mean_squared_error: 0.0910 - val_loss: 0.2250 - val_mean_squared_error: 0.0844\n",
            "Epoch 168/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2112 - mean_squared_error: 0.0841\n",
            "Epoch 168: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2095 - mean_squared_error: 0.0886 - val_loss: 0.2205 - val_mean_squared_error: 0.0755\n",
            "Epoch 169/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2154 - mean_squared_error: 0.0960\n",
            "Epoch 169: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2123 - mean_squared_error: 0.0890 - val_loss: 0.2227 - val_mean_squared_error: 0.0822\n",
            "Epoch 170/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2074 - mean_squared_error: 0.0841\n",
            "Epoch 170: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2172 - mean_squared_error: 0.0908 - val_loss: 0.2184 - val_mean_squared_error: 0.0771\n",
            "Epoch 171/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2301 - mean_squared_error: 0.1000\n",
            "Epoch 171: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2206 - mean_squared_error: 0.0925 - val_loss: 0.2176 - val_mean_squared_error: 0.0753\n",
            "Epoch 172/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2061 - mean_squared_error: 0.0794\n",
            "Epoch 172: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2141 - mean_squared_error: 0.0878 - val_loss: 0.2256 - val_mean_squared_error: 0.0761\n",
            "Epoch 173/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.0931\n",
            "Epoch 173: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2304 - mean_squared_error: 0.0969 - val_loss: 0.2315 - val_mean_squared_error: 0.0919\n",
            "Epoch 174/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2345 - mean_squared_error: 0.0993\n",
            "Epoch 174: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2424 - mean_squared_error: 0.1051 - val_loss: 0.2244 - val_mean_squared_error: 0.0786\n",
            "Epoch 175/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2285 - mean_squared_error: 0.0976\n",
            "Epoch 175: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2256 - mean_squared_error: 0.0967 - val_loss: 0.2261 - val_mean_squared_error: 0.0780\n",
            "Epoch 176/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2050 - mean_squared_error: 0.0777\n",
            "Epoch 176: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2194 - mean_squared_error: 0.0907 - val_loss: 0.2245 - val_mean_squared_error: 0.0761\n",
            "Epoch 177/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2132 - mean_squared_error: 0.0932\n",
            "Epoch 177: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2188 - mean_squared_error: 0.0919 - val_loss: 0.2266 - val_mean_squared_error: 0.0827\n",
            "Epoch 178/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2301 - mean_squared_error: 0.0960\n",
            "Epoch 178: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2204 - mean_squared_error: 0.0896 - val_loss: 0.2288 - val_mean_squared_error: 0.0839\n",
            "Epoch 179/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2194 - mean_squared_error: 0.0960\n",
            "Epoch 179: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2145 - mean_squared_error: 0.0905 - val_loss: 0.2223 - val_mean_squared_error: 0.0812\n",
            "Epoch 180/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2253 - mean_squared_error: 0.0928\n",
            "Epoch 180: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2130 - mean_squared_error: 0.0896 - val_loss: 0.2227 - val_mean_squared_error: 0.0750\n",
            "Epoch 181/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2065 - mean_squared_error: 0.0835\n",
            "Epoch 181: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2143 - mean_squared_error: 0.0890 - val_loss: 0.2392 - val_mean_squared_error: 0.0965\n",
            "Epoch 182/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2160 - mean_squared_error: 0.0869\n",
            "Epoch 182: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2167 - mean_squared_error: 0.0882 - val_loss: 0.2236 - val_mean_squared_error: 0.0736\n",
            "Epoch 183/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2113 - mean_squared_error: 0.0817\n",
            "Epoch 183: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.2161 - mean_squared_error: 0.0900 - val_loss: 0.2191 - val_mean_squared_error: 0.0777\n",
            "Epoch 184/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2037 - mean_squared_error: 0.0836\n",
            "Epoch 184: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2111 - mean_squared_error: 0.0882 - val_loss: 0.2231 - val_mean_squared_error: 0.0758\n",
            "Epoch 185/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2298 - mean_squared_error: 0.1044\n",
            "Epoch 185: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2211 - mean_squared_error: 0.0921 - val_loss: 0.2225 - val_mean_squared_error: 0.0763\n",
            "Epoch 186/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2231 - mean_squared_error: 0.0979\n",
            "Epoch 186: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2252 - mean_squared_error: 0.0943 - val_loss: 0.2374 - val_mean_squared_error: 0.1000\n",
            "Epoch 187/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2185 - mean_squared_error: 0.0880\n",
            "Epoch 187: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2197 - mean_squared_error: 0.0921 - val_loss: 0.2184 - val_mean_squared_error: 0.0786\n",
            "Epoch 188/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2170 - mean_squared_error: 0.0969\n",
            "Epoch 188: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2117 - mean_squared_error: 0.0875 - val_loss: 0.2224 - val_mean_squared_error: 0.0768\n",
            "Epoch 189/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.0966\n",
            "Epoch 189: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2233 - mean_squared_error: 0.0936 - val_loss: 0.2394 - val_mean_squared_error: 0.1010\n",
            "Epoch 190/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2140 - mean_squared_error: 0.0911\n",
            "Epoch 190: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2222 - mean_squared_error: 0.0949 - val_loss: 0.2250 - val_mean_squared_error: 0.0760\n",
            "Epoch 191/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2128 - mean_squared_error: 0.0889\n",
            "Epoch 191: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2237 - mean_squared_error: 0.0932 - val_loss: 0.2197 - val_mean_squared_error: 0.0777\n",
            "Epoch 192/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2083 - mean_squared_error: 0.0765\n",
            "Epoch 192: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2174 - mean_squared_error: 0.0905 - val_loss: 0.2525 - val_mean_squared_error: 0.1109\n",
            "Epoch 193/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.0898\n",
            "Epoch 193: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2266 - mean_squared_error: 0.0952 - val_loss: 0.2192 - val_mean_squared_error: 0.0735\n",
            "Epoch 194/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.1932 - mean_squared_error: 0.0676\n",
            "Epoch 194: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2143 - mean_squared_error: 0.0890 - val_loss: 0.2236 - val_mean_squared_error: 0.0791\n",
            "Epoch 195/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.1986 - mean_squared_error: 0.0825\n",
            "Epoch 195: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2124 - mean_squared_error: 0.0872 - val_loss: 0.2187 - val_mean_squared_error: 0.0721\n",
            "Epoch 196/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2031 - mean_squared_error: 0.0812\n",
            "Epoch 196: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2140 - mean_squared_error: 0.0906 - val_loss: 0.2253 - val_mean_squared_error: 0.0836\n",
            "Epoch 197/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2179 - mean_squared_error: 0.0914\n",
            "Epoch 197: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2133 - mean_squared_error: 0.0885 - val_loss: 0.2249 - val_mean_squared_error: 0.0847\n",
            "Epoch 198/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2013 - mean_squared_error: 0.0868\n",
            "Epoch 198: val_loss did not improve from 0.21535\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2188 - mean_squared_error: 0.0923 - val_loss: 0.2214 - val_mean_squared_error: 0.0746\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "Fold: 10\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 98, 98, 16)   160         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 47, 47, 16)   2320        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8464)         0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           270880      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16)           528         ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            17          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            65          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 274,402\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Start fitting - Fold: 10\n",
            "Epoch 1/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 4.8040 - mean_squared_error: 39.8787 \n",
            "Epoch 1: val_loss improved from inf to 2.78127, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 5s 109ms/step - loss: 4.5582 - mean_squared_error: 36.4669 - val_loss: 2.7813 - val_mean_squared_error: 16.1327\n",
            "Epoch 2/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 1.2715 - mean_squared_error: 3.1158\n",
            "Epoch 2: val_loss improved from 2.78127 to 1.23150, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 1.2998 - mean_squared_error: 2.9890 - val_loss: 1.2315 - val_mean_squared_error: 2.6225\n",
            "Epoch 3/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.7273 - mean_squared_error: 1.0548\n",
            "Epoch 3: val_loss improved from 1.23150 to 0.68242, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.7231 - mean_squared_error: 1.0229 - val_loss: 0.6824 - val_mean_squared_error: 0.5977\n",
            "Epoch 4/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.6927 - mean_squared_error: 0.8395\n",
            "Epoch 4: val_loss improved from 0.68242 to 0.67393, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.6775 - mean_squared_error: 0.8244 - val_loss: 0.6739 - val_mean_squared_error: 1.0796\n",
            "Epoch 5/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5555 - mean_squared_error: 0.6250\n",
            "Epoch 5: val_loss improved from 0.67393 to 0.46230, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.5508 - mean_squared_error: 0.6216 - val_loss: 0.4623 - val_mean_squared_error: 0.3374\n",
            "Epoch 6/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.5011 - mean_squared_error: 0.4809\n",
            "Epoch 6: val_loss improved from 0.46230 to 0.40708, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.4846 - mean_squared_error: 0.4484 - val_loss: 0.4071 - val_mean_squared_error: 0.2758\n",
            "Epoch 7/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4801 - mean_squared_error: 0.4320\n",
            "Epoch 7: val_loss improved from 0.40708 to 0.34155, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.4686 - mean_squared_error: 0.4082 - val_loss: 0.3416 - val_mean_squared_error: 0.1941\n",
            "Epoch 8/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4506 - mean_squared_error: 0.3903\n",
            "Epoch 8: val_loss did not improve from 0.34155\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4396 - mean_squared_error: 0.3757 - val_loss: 0.4212 - val_mean_squared_error: 0.3596\n",
            "Epoch 9/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.4296 - mean_squared_error: 0.3845\n",
            "Epoch 9: val_loss did not improve from 0.34155\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4205 - mean_squared_error: 0.3596 - val_loss: 0.3604 - val_mean_squared_error: 0.1963\n",
            "Epoch 10/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3886 - mean_squared_error: 0.2961\n",
            "Epoch 10: val_loss improved from 0.34155 to 0.22587, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 0.3877 - mean_squared_error: 0.2959 - val_loss: 0.2259 - val_mean_squared_error: 0.1114\n",
            "Epoch 11/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3343 - mean_squared_error: 0.2230\n",
            "Epoch 11: val_loss did not improve from 0.22587\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.3584 - mean_squared_error: 0.2589 - val_loss: 0.2395 - val_mean_squared_error: 0.1145\n",
            "Epoch 12/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3346 - mean_squared_error: 0.2413\n",
            "Epoch 12: val_loss did not improve from 0.22587\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3351 - mean_squared_error: 0.2439 - val_loss: 0.2611 - val_mean_squared_error: 0.1326\n",
            "Epoch 13/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3446 - mean_squared_error: 0.2372\n",
            "Epoch 13: val_loss improved from 0.22587 to 0.21224, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.3427 - mean_squared_error: 0.2318 - val_loss: 0.2122 - val_mean_squared_error: 0.0937\n",
            "Epoch 14/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3375 - mean_squared_error: 0.2322\n",
            "Epoch 14: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.3310 - mean_squared_error: 0.2186 - val_loss: 0.2497 - val_mean_squared_error: 0.0936\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.3158 - mean_squared_error: 0.1991\n",
            "Epoch 15: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3158 - mean_squared_error: 0.1991 - val_loss: 0.2149 - val_mean_squared_error: 0.0684\n",
            "Epoch 16/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3381 - mean_squared_error: 0.2063\n",
            "Epoch 16: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3351 - mean_squared_error: 0.2044 - val_loss: 0.2205 - val_mean_squared_error: 0.1304\n",
            "Epoch 17/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3076 - mean_squared_error: 0.1809\n",
            "Epoch 17: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.3017 - mean_squared_error: 0.1743 - val_loss: 0.2300 - val_mean_squared_error: 0.1196\n",
            "Epoch 18/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2936 - mean_squared_error: 0.1794\n",
            "Epoch 18: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.2859 - mean_squared_error: 0.1688 - val_loss: 0.2143 - val_mean_squared_error: 0.0698\n",
            "Epoch 19/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.3035 - mean_squared_error: 0.1835\n",
            "Epoch 19: val_loss did not improve from 0.21224\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.3054 - mean_squared_error: 0.1821 - val_loss: 0.2275 - val_mean_squared_error: 0.0655\n",
            "Epoch 20/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2831 - mean_squared_error: 0.1667\n",
            "Epoch 20: val_loss improved from 0.21224 to 0.20135, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.2917 - mean_squared_error: 0.1706 - val_loss: 0.2014 - val_mean_squared_error: 0.0637\n",
            "Epoch 21/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2961 - mean_squared_error: 0.1702\n",
            "Epoch 21: val_loss improved from 0.20135 to 0.18574, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.2932 - mean_squared_error: 0.1668 - val_loss: 0.1857 - val_mean_squared_error: 0.0484\n",
            "Epoch 22/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2984 - mean_squared_error: 0.1687\n",
            "Epoch 22: val_loss improved from 0.18574 to 0.18330, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.2902 - mean_squared_error: 0.1609 - val_loss: 0.1833 - val_mean_squared_error: 0.0499\n",
            "Epoch 23/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.1395\n",
            "Epoch 23: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2833 - mean_squared_error: 0.1542 - val_loss: 0.2057 - val_mean_squared_error: 0.0648\n",
            "Epoch 24/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2853 - mean_squared_error: 0.1520\n",
            "Epoch 24: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2982 - mean_squared_error: 0.1661 - val_loss: 0.2035 - val_mean_squared_error: 0.0630\n",
            "Epoch 25/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.1417\n",
            "Epoch 25: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2856 - mean_squared_error: 0.1605 - val_loss: 0.2495 - val_mean_squared_error: 0.0903\n",
            "Epoch 26/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3432 - mean_squared_error: 0.1981\n",
            "Epoch 26: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3220 - mean_squared_error: 0.1776 - val_loss: 0.2466 - val_mean_squared_error: 0.0873\n",
            "Epoch 27/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.3008 - mean_squared_error: 0.1612\n",
            "Epoch 27: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3035 - mean_squared_error: 0.1654 - val_loss: 0.1875 - val_mean_squared_error: 0.0500\n",
            "Epoch 28/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3051 - mean_squared_error: 0.1849\n",
            "Epoch 28: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2977 - mean_squared_error: 0.1664 - val_loss: 0.1996 - val_mean_squared_error: 0.0619\n",
            "Epoch 29/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3322 - mean_squared_error: 0.1919\n",
            "Epoch 29: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3292 - mean_squared_error: 0.1832 - val_loss: 0.3522 - val_mean_squared_error: 0.3258\n",
            "Epoch 30/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3100 - mean_squared_error: 0.1655\n",
            "Epoch 30: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3045 - mean_squared_error: 0.1676 - val_loss: 0.2556 - val_mean_squared_error: 0.1290\n",
            "Epoch 31/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2742 - mean_squared_error: 0.1593\n",
            "Epoch 31: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2793 - mean_squared_error: 0.1531 - val_loss: 0.2083 - val_mean_squared_error: 0.0716\n",
            "Epoch 32/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2598 - mean_squared_error: 0.1200\n",
            "Epoch 32: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2791 - mean_squared_error: 0.1487 - val_loss: 0.1887 - val_mean_squared_error: 0.0523\n",
            "Epoch 33/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2652 - mean_squared_error: 0.1389\n",
            "Epoch 33: val_loss did not improve from 0.18330\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2695 - mean_squared_error: 0.1406 - val_loss: 0.2034 - val_mean_squared_error: 0.0602\n",
            "Epoch 34/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2537 - mean_squared_error: 0.1187\n",
            "Epoch 34: val_loss improved from 0.18330 to 0.18043, saving model to best_cnn_gap.h5\n",
            "8/8 [==============================] - 0s 38ms/step - loss: 0.2671 - mean_squared_error: 0.1351 - val_loss: 0.1804 - val_mean_squared_error: 0.0489\n",
            "Epoch 35/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2783 - mean_squared_error: 0.1542\n",
            "Epoch 35: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2681 - mean_squared_error: 0.1427 - val_loss: 0.2060 - val_mean_squared_error: 0.0603\n",
            "Epoch 36/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2563 - mean_squared_error: 0.1263\n",
            "Epoch 36: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2610 - mean_squared_error: 0.1372 - val_loss: 0.2452 - val_mean_squared_error: 0.1011\n",
            "Epoch 37/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2945 - mean_squared_error: 0.1559\n",
            "Epoch 37: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2712 - mean_squared_error: 0.1398 - val_loss: 0.2089 - val_mean_squared_error: 0.0616\n",
            "Epoch 38/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2440 - mean_squared_error: 0.1221\n",
            "Epoch 38: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2576 - mean_squared_error: 0.1361 - val_loss: 0.2519 - val_mean_squared_error: 0.1112\n",
            "Epoch 39/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2631 - mean_squared_error: 0.1361\n",
            "Epoch 39: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2668 - mean_squared_error: 0.1356 - val_loss: 0.2598 - val_mean_squared_error: 0.1230\n",
            "Epoch 40/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2876 - mean_squared_error: 0.1506\n",
            "Epoch 40: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.2788 - mean_squared_error: 0.1429 - val_loss: 0.2102 - val_mean_squared_error: 0.0637\n",
            "Epoch 41/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2595 - mean_squared_error: 0.1229\n",
            "Epoch 41: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2704 - mean_squared_error: 0.1336 - val_loss: 0.2221 - val_mean_squared_error: 0.0638\n",
            "Epoch 42/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2559 - mean_squared_error: 0.1342\n",
            "Epoch 42: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2507 - mean_squared_error: 0.1263 - val_loss: 0.2423 - val_mean_squared_error: 0.0896\n",
            "Epoch 43/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2558 - mean_squared_error: 0.1248\n",
            "Epoch 43: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2528 - mean_squared_error: 0.1232 - val_loss: 0.2250 - val_mean_squared_error: 0.0674\n",
            "Epoch 44/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2496 - mean_squared_error: 0.1213\n",
            "Epoch 44: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2491 - mean_squared_error: 0.1241 - val_loss: 0.2551 - val_mean_squared_error: 0.0969\n",
            "Epoch 45/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2200 - mean_squared_error: 0.0952\n",
            "Epoch 45: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2465 - mean_squared_error: 0.1225 - val_loss: 0.2250 - val_mean_squared_error: 0.0676\n",
            "Epoch 46/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2306 - mean_squared_error: 0.1102\n",
            "Epoch 46: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2445 - mean_squared_error: 0.1207 - val_loss: 0.2108 - val_mean_squared_error: 0.0607\n",
            "Epoch 47/500\n",
            "5/8 [=================>............] - ETA: 0s - loss: 0.2439 - mean_squared_error: 0.1199\n",
            "Epoch 47: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2434 - mean_squared_error: 0.1163 - val_loss: 0.2635 - val_mean_squared_error: 0.0926\n",
            "Epoch 48/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2469 - mean_squared_error: 0.1131\n",
            "Epoch 48: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2568 - mean_squared_error: 0.1202 - val_loss: 0.2013 - val_mean_squared_error: 0.0603\n",
            "Epoch 49/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2419 - mean_squared_error: 0.1212\n",
            "Epoch 49: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2434 - mean_squared_error: 0.1166 - val_loss: 0.1922 - val_mean_squared_error: 0.0673\n",
            "Epoch 50/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.3105 - mean_squared_error: 0.1594\n",
            "Epoch 50: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.3009 - mean_squared_error: 0.1509 - val_loss: 0.2274 - val_mean_squared_error: 0.0791\n",
            "Epoch 51/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.1271\n",
            "Epoch 51: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2710 - mean_squared_error: 0.1294 - val_loss: 0.1942 - val_mean_squared_error: 0.0582\n",
            "Epoch 52/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2778 - mean_squared_error: 0.1327\n",
            "Epoch 52: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2728 - mean_squared_error: 0.1260 - val_loss: 0.2174 - val_mean_squared_error: 0.0682\n",
            "Epoch 53/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2355 - mean_squared_error: 0.1153\n",
            "Epoch 53: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2342 - mean_squared_error: 0.1095 - val_loss: 0.2454 - val_mean_squared_error: 0.0857\n",
            "Epoch 54/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2471 - mean_squared_error: 0.1070\n",
            "Epoch 54: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2454 - mean_squared_error: 0.1130 - val_loss: 0.2412 - val_mean_squared_error: 0.0871\n",
            "Epoch 55/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2586 - mean_squared_error: 0.1255\n",
            "Epoch 55: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.2514 - mean_squared_error: 0.1185 - val_loss: 0.2402 - val_mean_squared_error: 0.0837\n",
            "Epoch 56/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2537 - mean_squared_error: 0.1211\n",
            "Epoch 56: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2528 - mean_squared_error: 0.1190 - val_loss: 0.2110 - val_mean_squared_error: 0.0716\n",
            "Epoch 57/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2314 - mean_squared_error: 0.1069\n",
            "Epoch 57: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2286 - mean_squared_error: 0.1022 - val_loss: 0.2254 - val_mean_squared_error: 0.0850\n",
            "Epoch 58/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2410 - mean_squared_error: 0.1105\n",
            "Epoch 58: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2410 - mean_squared_error: 0.1090 - val_loss: 0.2278 - val_mean_squared_error: 0.0770\n",
            "Epoch 59/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2398 - mean_squared_error: 0.1050\n",
            "Epoch 59: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2481 - mean_squared_error: 0.1103 - val_loss: 0.2032 - val_mean_squared_error: 0.0707\n",
            "Epoch 60/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2590 - mean_squared_error: 0.1191\n",
            "Epoch 60: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2565 - mean_squared_error: 0.1164 - val_loss: 0.2536 - val_mean_squared_error: 0.0982\n",
            "Epoch 61/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2936 - mean_squared_error: 0.1593\n",
            "Epoch 61: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2861 - mean_squared_error: 0.1501 - val_loss: 0.2665 - val_mean_squared_error: 0.1182\n",
            "Epoch 62/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.1378\n",
            "Epoch 62: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2653 - mean_squared_error: 0.1245 - val_loss: 0.2244 - val_mean_squared_error: 0.0905\n",
            "Epoch 63/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2417 - mean_squared_error: 0.1145\n",
            "Epoch 63: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2396 - mean_squared_error: 0.1097 - val_loss: 0.2180 - val_mean_squared_error: 0.0746\n",
            "Epoch 64/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2397 - mean_squared_error: 0.1132\n",
            "Epoch 64: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2393 - mean_squared_error: 0.1083 - val_loss: 0.2171 - val_mean_squared_error: 0.0909\n",
            "Epoch 65/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2209 - mean_squared_error: 0.0984\n",
            "Epoch 65: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.2184 - mean_squared_error: 0.0962 - val_loss: 0.2106 - val_mean_squared_error: 0.0820\n",
            "Epoch 66/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2174 - mean_squared_error: 0.0987\n",
            "Epoch 66: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.2170 - mean_squared_error: 0.0950 - val_loss: 0.2164 - val_mean_squared_error: 0.0901\n",
            "Epoch 67/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2202 - mean_squared_error: 0.0973\n",
            "Epoch 67: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2269 - mean_squared_error: 0.1048 - val_loss: 0.2709 - val_mean_squared_error: 0.1152\n",
            "Epoch 68/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1277\n",
            "Epoch 68: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2547 - mean_squared_error: 0.1195 - val_loss: 0.2952 - val_mean_squared_error: 0.1312\n",
            "Epoch 69/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2403 - mean_squared_error: 0.1151\n",
            "Epoch 69: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.2373 - mean_squared_error: 0.1126 - val_loss: 0.2074 - val_mean_squared_error: 0.0889\n",
            "Epoch 70/500\n",
            "6/8 [=====================>........] - ETA: 0s - loss: 0.2702 - mean_squared_error: 0.1265\n",
            "Epoch 70: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.2547 - mean_squared_error: 0.1124 - val_loss: 0.2349 - val_mean_squared_error: 0.1054\n",
            "Epoch 71/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2313 - mean_squared_error: 0.1069\n",
            "Epoch 71: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2342 - mean_squared_error: 0.1076 - val_loss: 0.2118 - val_mean_squared_error: 0.0829\n",
            "Epoch 72/500\n",
            "7/8 [=========================>....] - ETA: 0s - loss: 0.2590 - mean_squared_error: 0.1159\n",
            "Epoch 72: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2476 - mean_squared_error: 0.1074 - val_loss: 0.2697 - val_mean_squared_error: 0.1508\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2331 - mean_squared_error: 0.1121\n",
            "Epoch 73: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2331 - mean_squared_error: 0.1121 - val_loss: 0.2168 - val_mean_squared_error: 0.0829\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - ETA: 0s - loss: 0.2187 - mean_squared_error: 0.0970\n",
            "Epoch 74: val_loss did not improve from 0.18043\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.2187 - mean_squared_error: 0.0970 - val_loss: 0.2177 - val_mean_squared_error: 0.0917\n",
            "1/1 [==============================] - 0s 156ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PGcP4hneFh24",
        "2J82C1fy_PFD",
        "Cth4q9IM6NDl",
        "KHBecwB0_2_I",
        "khEOzNYf40co"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}