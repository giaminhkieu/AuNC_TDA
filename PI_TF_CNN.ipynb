{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giaminhkieu/AuNC_TDA/blob/main/PI_TF_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0CdEEiyG2GJ"
      },
      "source": [
        "# Generating Persistence Images and Simplical complex features to predict AuNC properties\n",
        "\n",
        "1. This notebook focuses on generating persistence images and simplicial complex count and using them to build a Kernel Ridge Regression and Random Forest Regression model to predict the HOMO-LUMO gap, internal energies, and dipole moment of AuNCs.\n",
        "2. Persistence Images were generated using the code in Townsend *et. al.* 2020 publication, publicly available at: https://gitlab.com/voglab/PersistentImages_Chemistry\n",
        "3. Simplex analysis were done using GUDHI package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki4_nk4hFq0F"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXKRJ4SdVsQP",
        "outputId": "0b226a68-832e-402a-ce91-38abd4ad3ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gudhi\n",
            "  Downloading gudhi-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from gudhi) (1.23.5)\n",
            "Installing collected packages: gudhi\n",
            "Successfully installed gudhi-3.8.0\n",
            "Collecting ripser\n",
            "  Downloading ripser-0.6.4.tar.gz (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from ripser) (0.29.36)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ripser) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ripser) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ripser) (1.2.2)\n",
            "Collecting persim (from ripser)\n",
            "  Downloading persim-0.3.1-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from persim->ripser) (3.7.1)\n",
            "Collecting hopcroftkarp (from persim->ripser)\n",
            "  Downloading hopcroftkarp-1.2.5.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from persim->ripser)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from persim->ripser) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ripser) (3.2.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->persim->ripser) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->persim->ripser) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->persim->ripser) (1.16.0)\n",
            "Building wheels for collected packages: ripser, hopcroftkarp\n",
            "  Building wheel for ripser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ripser: filename=ripser-0.6.4-cp310-cp310-linux_x86_64.whl size=752979 sha256=2022a4c67fe3e7547b46ec790f6f82e50d1e4760dc51cccc8eb8416a4e82eb06\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/f5/66/f41f708b049057431155934f74e20ca6001a085fcd2e615150\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18103 sha256=35a7a363eb0c54a144b49745914dc91876bc719546eb826585988d76ccbbb594\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/0f/3b/0f931844eecc34addd90e72d54cd39c08b7066c5f25c00b9a4\n",
            "Successfully built ripser hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, deprecated, persim, ripser\n",
            "Successfully installed deprecated-1.2.14 hopcroftkarp-1.2.5 persim-0.3.1 ripser-0.6.4\n",
            "Collecting elements\n",
            "  Downloading elements-1.0.0.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from elements) (1.23.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from elements) (2.31.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->elements) (9.4.0)\n",
            "Building wheels for collected packages: elements\n",
            "  Building wheel for elements (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elements: filename=elements-1.0.0-py3-none-any.whl size=21892 sha256=1dde5a55d45875d4c3047df24ecfce01b63bbd2a16b439a286821b9c02b92129\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b2/1d/b417ced7c50363975f73c922903470bb743dfe21f97051353f\n",
            "Successfully built elements\n",
            "Installing collected packages: elements\n",
            "Successfully installed elements-1.0.0\n",
            "Collecting qml\n",
            "  Downloading qml-0.4.0.27.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: qml\n",
            "  Building wheel for qml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qml: filename=qml-0.4.0.27-cp310-cp310-linux_x86_64.whl size=1211677 sha256=0bd5ffc7ff6b56cdd7f26e339c20942be97e9af05a1f260edcbd4733800c8bd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/57/2f/cdd885bb28ee050be4f41882d74d716e1fc0ff0745a657b39a\n",
            "Successfully built qml\n",
            "Installing collected packages: qml\n",
            "Successfully installed qml-0.4.0.27\n",
            "Collecting dscribe\n",
            "  Downloading dscribe-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.4 (from dscribe)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.10.1)\n",
            "Collecting ase>=3.19.0 (from dscribe)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from dscribe) (1.3.2)\n",
            "Collecting sparse (from dscribe)\n",
            "  Downloading sparse-0.14.0-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase>=3.19.0->dscribe) (3.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dscribe) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from sparse->dscribe) (0.56.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase>=3.19.0->dscribe) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse->dscribe) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->sparse->dscribe) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase>=3.19.0->dscribe) (1.16.0)\n",
            "Installing collected packages: pybind11, sparse, ase, dscribe\n",
            "Successfully installed ase-3.22.1 dscribe-2.0.1 pybind11-2.11.1 sparse-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gudhi\n",
        "!pip install ripser\n",
        "!pip install elements\n",
        "!pip install qml\n",
        "!pip install dscribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i6jvju4aUDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b43e387-e28c-4b84-f901-5c29e9c0e0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2ZB4QckCd5bA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/URECA_22')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import qml\n",
        "from qml.representations import *\n",
        "from natsort import natsorted\n",
        "from dscribe.descriptors import SOAP\n",
        "from ase.io import read\n",
        "from copy import deepcopy\n",
        "from ElementsInfo import *\n",
        "from PersistentImageCode import *\n",
        "from VariancePersistCode import *\n",
        "from GenerateImagePI import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGcP4hneFh24"
      },
      "source": [
        "# Process data and store them to pickle files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "dlkMTF4K1eyZ",
        "outputId": "01689b48-cf0b-4739-c662-cecc689b713b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-4a60f256-b723-423c-85d3-a1da71a3c4c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clusters</th>\n",
              "      <th>Filename</th>\n",
              "      <th>Charge</th>\n",
              "      <th>Ligands1</th>\n",
              "      <th>Type1</th>\n",
              "      <th>Quantity1</th>\n",
              "      <th>Ligands2</th>\n",
              "      <th>Type2</th>\n",
              "      <th>Quantity2</th>\n",
              "      <th>Ligands3</th>\n",
              "      <th>Type3</th>\n",
              "      <th>Quantity3</th>\n",
              "      <th>u298</th>\n",
              "      <th>gap</th>\n",
              "      <th>dipX</th>\n",
              "      <th>dipY</th>\n",
              "      <th>dipZ</th>\n",
              "      <th>mu</th>\n",
              "      <th>core</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Au(PPh3)(CH3)</td>\n",
              "      <td>Au_PPh3_CH3_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>CH3</td>\n",
              "      <td>others</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.400818</td>\n",
              "      <td>1.946276</td>\n",
              "      <td>-0.4391</td>\n",
              "      <td>2.4436</td>\n",
              "      <td>-1.3932</td>\n",
              "      <td>7.236</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Au(SbPh3)4</td>\n",
              "      <td>Au_SbPh3_4_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-198.398732</td>\n",
              "      <td>2.058591</td>\n",
              "      <td>0.5488</td>\n",
              "      <td>0.0287</td>\n",
              "      <td>0.1793</td>\n",
              "      <td>1.469</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Au(SbPh3)Cl</td>\n",
              "      <td>Au_SbPh3_Cl_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>Cl</td>\n",
              "      <td>Halogens</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.829549</td>\n",
              "      <td>3.519847</td>\n",
              "      <td>0.3156</td>\n",
              "      <td>3.6059</td>\n",
              "      <td>-1.8031</td>\n",
              "      <td>10.279</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Au(SbTol3)3(SbCl2Tol2)</td>\n",
              "      <td>Au_SbTol3_3_SbCl2Tol2_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>SbTol3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>3</td>\n",
              "      <td>SbCl2Tol2</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-226.946163</td>\n",
              "      <td>3.277166</td>\n",
              "      <td>-0.6295</td>\n",
              "      <td>-2.4385</td>\n",
              "      <td>1.0070</td>\n",
              "      <td>6.894</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Au2(PET)(PPh3)2]+</td>\n",
              "      <td>Au2_PET_PPh3_2_c1</td>\n",
              "      <td>1</td>\n",
              "      <td>PET</td>\n",
              "      <td>SX</td>\n",
              "      <td>1</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-130.543475</td>\n",
              "      <td>3.157827</td>\n",
              "      <td>2.3911</td>\n",
              "      <td>0.2794</td>\n",
              "      <td>32.0597</td>\n",
              "      <td>81.717</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>Au103S2(S-Nap)41</td>\n",
              "      <td>Au103_S2_SNap_41_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>S-Nap</td>\n",
              "      <td>SX</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1594.470972</td>\n",
              "      <td>0.324921</td>\n",
              "      <td>-0.1763</td>\n",
              "      <td>-1.6961</td>\n",
              "      <td>-0.2783</td>\n",
              "      <td>4.392</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>Au108S24(PPh3)16</td>\n",
              "      <td>Au108_S24_PPh3_16_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>PPh3</td>\n",
              "      <td>16</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1302.446940</td>\n",
              "      <td>0.485519</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>-0.0230</td>\n",
              "      <td>-0.0275</td>\n",
              "      <td>0.103</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>[Au110(p-CF3C6H4CC)48]2-</td>\n",
              "      <td>Au110_CCPhCF3_48_c-2</td>\n",
              "      <td>-2</td>\n",
              "      <td>p-CF3C6H4CC</td>\n",
              "      <td>CCR</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-2195.283841</td>\n",
              "      <td>0.153487</td>\n",
              "      <td>-57.1284</td>\n",
              "      <td>-66.4725</td>\n",
              "      <td>-44.5142</td>\n",
              "      <td>249.865</td>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>Au133(TBBT)52</td>\n",
              "      <td>Au133_TBBT_52_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>TBBT</td>\n",
              "      <td>SX</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-2192.276830</td>\n",
              "      <td>0.068608</td>\n",
              "      <td>0.0956</td>\n",
              "      <td>-1.9440</td>\n",
              "      <td>-0.0162</td>\n",
              "      <td>4.947</td>\n",
              "      <td>133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>Au144(PET)60</td>\n",
              "      <td>Au144_PET_60_c0</td>\n",
              "      <td>0</td>\n",
              "      <td>PET</td>\n",
              "      <td>SX</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>others</td>\n",
              "      <td>0</td>\n",
              "      <td>-1914.637625</td>\n",
              "      <td>0.095082</td>\n",
              "      <td>-0.9166</td>\n",
              "      <td>-0.5340</td>\n",
              "      <td>-0.0097</td>\n",
              "      <td>2.696</td>\n",
              "      <td>144</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>212 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a60f256-b723-423c-85d3-a1da71a3c4c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-fe8c437d-19ed-4205-929f-7099b7072852\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe8c437d-19ed-4205-929f-7099b7072852')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-fe8c437d-19ed-4205-929f-7099b7072852 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a60f256-b723-423c-85d3-a1da71a3c4c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a60f256-b723-423c-85d3-a1da71a3c4c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                     Clusters                  Filename  Charge     Ligands1  \\\n",
              "0               Au(PPh3)(CH3)            Au_PPh3_CH3_c0       0         PPh3   \n",
              "1                  Au(SbPh3)4             Au_SbPh3_4_c0       0        SbPh3   \n",
              "2                 Au(SbPh3)Cl            Au_SbPh3_Cl_c0       0        SbPh3   \n",
              "3      Au(SbTol3)3(SbCl2Tol2)  Au_SbTol3_3_SbCl2Tol2_c0       0       SbTol3   \n",
              "4          [Au2(PET)(PPh3)2]+         Au2_PET_PPh3_2_c1       1          PET   \n",
              "..                        ...                       ...     ...          ...   \n",
              "207          Au103S2(S-Nap)41       Au103_S2_SNap_41_c0       0            S   \n",
              "208          Au108S24(PPh3)16      Au108_S24_PPh3_16_c0       0         PPh3   \n",
              "209  [Au110(p-CF3C6H4CC)48]2-      Au110_CCPhCF3_48_c-2      -2  p-CF3C6H4CC   \n",
              "210             Au133(TBBT)52          Au133_TBBT_52_c0       0         TBBT   \n",
              "211              Au144(PET)60           Au144_PET_60_c0       0          PET   \n",
              "\n",
              "    Type1  Quantity1   Ligands2     Type2  Quantity2 Ligands3   Type3  \\\n",
              "0    PPh3          1        CH3    others          1        0  others   \n",
              "1    PPh3          4          0    others          0        0  others   \n",
              "2    PPh3          1         Cl  Halogens          1        0  others   \n",
              "3    PPh3          3  SbCl2Tol2      PPh3          1        0  others   \n",
              "4      SX          1       PPh3      PPh3          2        0  others   \n",
              "..    ...        ...        ...       ...        ...      ...     ...   \n",
              "207     S          2      S-Nap        SX         41        0  others   \n",
              "208  PPh3         16          S         S         24        0  others   \n",
              "209   CCR         48          0    others          0        0  others   \n",
              "210    SX         52          0    others          0        0  others   \n",
              "211    SX         60          0    others          0        0  others   \n",
              "\n",
              "     Quantity3         u298       gap     dipX     dipY     dipZ       mu  \\\n",
              "0            0   -56.400818  1.946276  -0.4391   2.4436  -1.3932    7.236   \n",
              "1            0  -198.398732  2.058591   0.5488   0.0287   0.1793    1.469   \n",
              "2            0   -56.829549  3.519847   0.3156   3.6059  -1.8031   10.279   \n",
              "3            0  -226.946163  3.277166  -0.6295  -2.4385   1.0070    6.894   \n",
              "4            0  -130.543475  3.157827   2.3911   0.2794  32.0597   81.717   \n",
              "..         ...          ...       ...      ...      ...      ...      ...   \n",
              "207          0 -1594.470972  0.324921  -0.1763  -1.6961  -0.2783    4.392   \n",
              "208          0 -1302.446940  0.485519   0.0193  -0.0230  -0.0275    0.103   \n",
              "209          0 -2195.283841  0.153487 -57.1284 -66.4725 -44.5142  249.865   \n",
              "210          0 -2192.276830  0.068608   0.0956  -1.9440  -0.0162    4.947   \n",
              "211          0 -1914.637625  0.095082  -0.9166  -0.5340  -0.0097    2.696   \n",
              "\n",
              "     core  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       2  \n",
              "..    ...  \n",
              "207   103  \n",
              "208   108  \n",
              "209   110  \n",
              "210   133  \n",
              "211   144  \n",
              "\n",
              "[212 rows x 19 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import dataframe\n",
        "os.chdir('/content/drive/MyDrive/AuNC database')\n",
        "DatasetAuNC = pd.read_excel(\"DatasetAuNC_XTB_290723.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3zc5WFR2X8B",
        "outputId": "d2e00d97-7cc7-4161-a022-e72a6bc614ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 212/212 [00:00<00:00, 376.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# Appending xyz\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/AuNC database/Optimized_geometries')\n",
        "\n",
        "xyz_list = []\n",
        "files = []\n",
        "for i in tqdm(range(len(DatasetAuNC))):\n",
        "    try:\n",
        "        xyz_filename = DatasetAuNC[\"Filename\"][i] + \".xyz\"\n",
        "        with open(xyz_filename) as f:\n",
        "            lines = f.readlines()\n",
        "            files.append(xyz_filename)\n",
        "            xyz_list.append(lines)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: File '{xyz_filename}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error at index {i}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "DatasetAuNC[\"xyz\"] = xyz_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JQVzTqV_UX9"
      },
      "outputs": [],
      "source": [
        "# Calculating Betti feature count for overview\n",
        "\n",
        "Error = []\n",
        "B0_list = []\n",
        "B1_list = []\n",
        "B2_list = []\n",
        "\n",
        "for i in range(len(DatasetAuNC)):\n",
        "  try:\n",
        "    D, elements = Makexyzdistance(DatasetAuNC[\"xyz\"][i])\n",
        "    persistent_homology_features = ripser(D,distance_matrix=True, maxdim = 2)\n",
        "    B0_list.append(len(persistent_homology_features['dgms'][0])-1)\n",
        "    B1_list.append(len(persistent_homology_features['dgms'][1]))\n",
        "    B2_list.append(len(persistent_homology_features['dgms'][2]))\n",
        "\n",
        "  except:\n",
        "    Error.append(DatasetAuNC[\"Number\"][i])\n",
        "    continue\n",
        "\n",
        "DatasetAuNC[\"Betti0count\"] = B0_list\n",
        "DatasetAuNC[\"Betti1count\"] = B1_list\n",
        "DatasetAuNC[\"Betti2count\"] = B2_list\n",
        "\n",
        "print(\"Finished appending Betti feature count!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1T4Cxhe7QYq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Calculating and appending VariancePersist arrays to pandas dataframe\n",
        "VariancePersistv1 is a modified PI function with added buffer values\n",
        "(0.5, 0.05) for Betti 1 and 2 features.\n",
        "Except block is added due to some xyz file having values written in different notations (e.g. 1e-4)\n",
        "and cannot be read as a float. (current AuNC database does not have this issue)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def append_persistence_image(Dataframe):\n",
        "  Error = []\n",
        "  PersImgArr = []\n",
        "\n",
        "  for idx in range(len(Dataframe)):\n",
        "    try:\n",
        "        persistent_image_matrix = VariancePersistv1(\n",
        "                              Dataframe[\"xyz\"][idx],\n",
        "                              pixelx=resolution,\n",
        "                              pixely=resolution,\n",
        "                              myspread=myspread ,\n",
        "                              myspecs={\"maxBD\": max_bound, \"minBD\":min_bound},\n",
        "                              electroneg_addition=electroneg_addition,\n",
        "                              electroneg_division=electroneg_division,\n",
        "                              B1_buffer=B1_buffer,\n",
        "                              B2_buffer=B2_buffer,\n",
        "                              showplot = False\n",
        "                              )\n",
        "\n",
        "        PersImgArr.append(persistent_image_matrix)\n",
        "    except:\n",
        "        Error.append(idx)\n",
        "        print(\"Error at index: \", idx)\n",
        "        PersImgArr.append(np.zeros(resolution*resolution,))\n",
        "\n",
        "  Dataframe[\"PersImg\"] = PersImgArr\n",
        "  print(\"Finished appending Persistence Images!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Persistence Image Parameters\n",
        "resolution=100\n",
        "myspread=0.3\n",
        "min_bound=-0.3\n",
        "max_bound=7\n",
        "electroneg_addition=+0.4\n",
        "electroneg_division=10\n",
        "B1_buffer=0.5\n",
        "B2_buffer=0.05\n",
        "\n",
        "append_persistence_image(DatasetAuNC)"
      ],
      "metadata": {
        "id": "19TJqyHm6n6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGVd8emwtP1Y"
      },
      "outputs": [],
      "source": [
        "# Simplcial complex analysis\n",
        "def Simplex_analyze(Core_coordinates, bond_length_limit):\n",
        "  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)\n",
        "  simplex_tree = rips.create_simplex_tree(max_dimension=3)\n",
        "  simplex_generator = simplex_tree.get_skeleton(3)\n",
        "\n",
        "  Tetrahedra_list = []\n",
        "  Vertices_of_tetrahedra = []\n",
        "  Overlapping_triangles = []\n",
        "  Unconnected_triangles = []\n",
        "  Triangles_with_1_shared_vertex = []\n",
        "  Triangles_with_2_shared_vertices = []\n",
        "\n",
        "  for simplex in simplex_generator:\n",
        "    if simplex[1] >= bond_length_limit: continue\n",
        "    if len(simplex[0]) == 4: #simplex[0]: list of vertices, simplex[1]: birth filtration\n",
        "      Tetrahedra_list.append(simplex)\n",
        "      for vertex in simplex[0]:\n",
        "        if vertex not in Vertices_of_tetrahedra:\n",
        "          Vertices_of_tetrahedra.append(vertex)\n",
        "\n",
        "  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)\n",
        "  simplex_tree = rips.create_simplex_tree(max_dimension=3)\n",
        "  simplex_generator = simplex_tree.get_skeleton(3)\n",
        "\n",
        "  for simplex in simplex_generator:\n",
        "    Shared_count_arr = []\n",
        "\n",
        "    if simplex[1] >= bond_length_limit: continue\n",
        "    if len(simplex[0]) != 3: continue\n",
        "\n",
        "    if len(Tetrahedra_list) == 0: Unconnected_triangles.append(simplex[0])\n",
        "    else:\n",
        "      for tetrahedra in Tetrahedra_list:\n",
        "        Shared_count_arr.append(len(set(simplex[0]) & set(tetrahedra[0])))\n",
        "\n",
        "      if max(Shared_count_arr) == 3: Overlapping_triangles.append(simplex)\n",
        "      elif max(Shared_count_arr) == 2: Triangles_with_2_shared_vertices.append(simplex)\n",
        "      elif max(Shared_count_arr) == 1: Triangles_with_1_shared_vertex.append(simplex)\n",
        "      else: Unconnected_triangles.append(simplex)\n",
        "\n",
        "  return {\"Tetrahedral_count\": len(Tetrahedra_list), \"Unconnected_triangles_count\": len(Unconnected_triangles),\n",
        "          \"Triangles_with_1_shared_vertex_count\":len(Triangles_with_1_shared_vertex),\n",
        "          \"Triangles_with_2_shared_vertices_count\": len(Triangles_with_2_shared_vertices)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RZhjH8ethJo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Simplex analysis\n",
        "\"\"\"\n",
        "\n",
        "Tetrahedral_count_arr = []\n",
        "Unconnected_triangles_count_arr = []\n",
        "Triangles_with_1_shared_vertex_count_arr = []\n",
        "Triangles_with_2_shared_vertices_count_arr = []\n",
        "files = []\n",
        "\n",
        "for idx in range(len(DatasetAuNC)):\n",
        "  try:\n",
        "    filename = DatasetAuNC[\"Filename\"][idx] + \".xyz\"\n",
        "    df=pd.read_table(filename, delim_whitespace=True, names=['a','b','c','d'],skiprows = 2) # skip the first 2 lines of xyz files\n",
        "    mat = df[['b','c','d']].to_numpy()\n",
        "    ElementArr = df['a'].to_numpy()\n",
        "\n",
        "    Core_coordinates = []\n",
        "\n",
        "    for index in range(len(ElementArr)):\n",
        "      if ElementArr[index] != \"Au\": continue\n",
        "      Core_coordinates.append(mat[index])\n",
        "\n",
        "    Dict = Simplex_analyze(Core_coordinates, bond_length_limit = 4.0)\n",
        "\n",
        "    Tetrahedral_count_arr.append(Dict[\"Tetrahedral_count\"])\n",
        "    Unconnected_triangles_count_arr.append(Dict[\"Unconnected_triangles_count\"])\n",
        "    Triangles_with_1_shared_vertex_count_arr.append(Dict[\"Triangles_with_1_shared_vertex_count\"])\n",
        "    Triangles_with_2_shared_vertices_count_arr.append(Dict[\"Triangles_with_2_shared_vertices_count\"])\n",
        "\n",
        "    print(\"Done: \", i)\n",
        "    print(len(Tetrahedral_count_arr))\n",
        "\n",
        "\n",
        "  except:\n",
        "    print(\"error at: \", i)\n",
        "    continue\n",
        "\n",
        "DatasetAuNC[\"Tetrahedral_count\"] = Tetrahedral_count_arr\n",
        "DatasetAuNC[\"Unconnected_triangles_count\"] = Unconnected_triangles_count_arr\n",
        "DatasetAuNC[\"Triangles_with_1_shared_vertex_count\"] = Triangles_with_1_shared_vertex_count_arr\n",
        "DatasetAuNC[\"Triangles_with_2_shared_vertices_count\"] = Triangles_with_2_shared_vertices_count_arr\n",
        "\n",
        "print(\"Finished appending Simplex count!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculating and appending Coulomb Matrix arrays to pandas dataframe\n",
        "\"\"\"\n",
        "def append_cm(dataframe, xyz_root):\n",
        "  CoulMatArr = []\n",
        "  for f in natsorted(os.listdir(xyz_root)):\n",
        "    try:\n",
        "      mol = qml.Compound(xyz=f)\n",
        "      mol.generate_coulomb_matrix(size=2000, sorting=\"row-norm\")\n",
        "      A = mol.representation\n",
        "      CoulMatArr.append([f,A])\n",
        "    except:\n",
        "      print(\"Error at: \", f)\n",
        "\n",
        "  # Sample array containing the format [\"Filename.xyz\", property array]\n",
        "  property_arrays = np.empty(len(dataframe), dtype=object)\n",
        "\n",
        "  for filename, property_array in CoulMatArr:\n",
        "      filename_without_extension = filename.split('.xyz')[0]\n",
        "      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]\n",
        "\n",
        "      if len(row_index) == 1:\n",
        "          property_arrays[row_index[0]] = property_array\n",
        "      else:\n",
        "          print(f\"Filename '{filename_without_extension}' not found in the DataFrame.\")\n",
        "\n",
        "  dataframe['Coulomb_Matrix'] = property_arrays\n",
        "  print(\"Finished appending Coulomb Matrices!\")\n",
        "\n",
        "append_cm(DatasetAuNC, \"/content/drive/MyDrive/AuNC database/Optimized_geometries\")"
      ],
      "metadata": {
        "id": "qBW1TrJRKmos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculating and appending SOAP arrays to pandas dataframe\n",
        "\"\"\"\n",
        "def append_soap(dataframe, xyz_root):\n",
        "  species = [\"Au\", \"P\", \"S\", \"Sb\", \"Se\", \"Cl\", \"Br\", \"F\", \"C\", \"N\", \"O\", \"H\", \"I\", \"Fe\"]\n",
        "  r_cut = 6.0\n",
        "  n_max = 8\n",
        "  l_max = 6\n",
        "\n",
        "  soap = SOAP(\n",
        "      species=species,\n",
        "      periodic=False,\n",
        "      r_cut=r_cut,\n",
        "      n_max=n_max,\n",
        "      l_max=l_max,\n",
        "  )\n",
        "\n",
        "  SOAPArr = []\n",
        "  for f in natsorted(os.listdir(xyz_root)):\n",
        "    try:\n",
        "      mol = read(f)\n",
        "      a = soap.create(system=mol,centers=[0])\n",
        "      A = a.flatten()\n",
        "      SOAPArr.append([f,A])\n",
        "    except:\n",
        "      print(\"error at: \", f)\n",
        "\n",
        "  property_arrays = np.empty(len(dataframe), dtype=object)\n",
        "\n",
        "  for filename, property_array in SOAPArr:\n",
        "      filename_without_extension = filename.split('.xyz')[0]\n",
        "      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]\n",
        "\n",
        "      if len(row_index) == 1:\n",
        "          property_arrays[row_index[0]] = property_array\n",
        "      else:\n",
        "          print(f\"Filename '{filename_without_extension}' not found in the DataFrame.\")\n",
        "\n",
        "  dataframe['SOAP'] = property_arrays\n",
        "  print(\"Finished appending SOAP!\")\n",
        "\n",
        "append_soap(DatasetAuNC, \"/content/drive/MyDrive/AuNC database/Optimized_geometries\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unxheweybkcO",
        "outputId": "bd22f611-e31f-43e4-b77e-c9d55c70a00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done:  Au2_PET_PPh3_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au2_SbTol3_2_Cl2_c0.xyz shape:  (44296,)\n",
            "Done:  Au3_iylidene_3_c1.xyz shape:  (44296,)\n",
            "Done:  Au3_pylidene_2_PPh3_c1.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh2an_4_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_Br2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_I2_c0.xyz shape:  (44296,)\n",
            "Done:  Au4_PPh3_4_I2_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_STol_2_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au4_dppm_3_I2_c0.xyz shape:  (44296,)\n",
            "Done:  Au5_P_PPh3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au5_dppm_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_C_PPh2pyr_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_C_bylidene_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PPh3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PXant_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_PhDP_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_S2Tol_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au6_Se2_dppNPh_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_Se2_dppNR4_3_c2.xyz shape:  (44296,)\n",
            "Done:  Au6_dppp_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au7_PPh3_7_c1.xyz shape:  (44296,)\n",
            "Done:  Au7_dppp_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au8_PPh3_7_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_Pmet3_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_S2_dppm_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppf_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCBu_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCCCPh_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_CCPh_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_SPyH_2_c4.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_SPy_2_c2.xyz shape:  (44296,)\n",
            "Done:  Au8_dppp_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au9_PNC_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_C4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_D2h_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_PPh3_8_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_Pan3_8_C4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_Pan3_8_D2h_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_R-BINAP_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_S-BINAP_4_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_dpph_4_db_c3.xyz shape:  (44296,)\n",
            "Done:  Au9_dpph_4_dc_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_PCy2Ph_6_Cl3_c1.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_5_C6F5_4_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_7_S2C2CN2_2_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_PPh3_8_NCO_Cl_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_R-BINAP_4_CCPhCF3_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR1_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR2_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNR3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S4_dppNTol_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_S-BINAP_4_CCPhCF3_c3.xyz shape:  (44296,)\n",
            "Done:  Au10_SC4_10_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_Se4_dppNR4_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au10_TBBT_10_c0.xyz shape:  (44296,)\n",
            "Done:  Au10_bylidene_4_Br2_c2.xyz shape:  (44296,)\n",
            "Done:  Au11_PDPE_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PMePh2_10_C3v_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_PMePh2_10_D4d_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_PNC_6_PPh3_2_c5.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Br3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_CNPr_2_I_c2.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Cl3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_I3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_Spyr_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_7_p-MBA_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_8_CCPhCF3_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PPh3_8_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_PPhCF3_7_Cl3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PPhF3_7_I3_c0.xyz shape:  (44296,)\n",
            "Done:  Au11_PXant_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_bylidene_5_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dppe_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_Br2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_I2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppf_4_SCN_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au11_dppp_5_c3.xyz shape:  (44296,)\n",
            "Done:  Au11_dpppen_4_SePh_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au12_SC4_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au13_PMe2Ph_10_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_PPh3_8_p-MBA_3_c0.xyz shape:  (44296,)\n",
            "Done:  Au13_SAdm_8_dppb_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_SbPh3_8_Cl4_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_5_Br2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_5_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_8_CCPhF_4_c1.xyz shape:  (44296,)\n",
            "Done:  Au13_bylidene_9_Cl3_c2.xyz shape:  (44296,)\n",
            "Done:  Au13_dppe_5_CCPh_2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppe_5_Cl2_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppm_6_c3.xyz shape:  (44296,)\n",
            "Done:  Au13_dppm_6_c5.xyz shape:  (44296,)\n",
            "Done:  Au14_SCy_10_dppb_c0.xyz shape:  (44296,)\n",
            "Done:  Au16_SAdm_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_S2_STipb_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_S8_dppe_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au18_SCy_14_c0.xyz shape:  (44296,)\n",
            "Done:  Au18_dppm_6_Br4_c2.xyz shape:  (44296,)\n",
            "Done:  Au18_dppm_6_Cl4_c4.xyz shape:  (44296,)\n",
            "Done:  Au19_dppNH_3_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au20_PP3_4_c4.xyz shape:  (44296,)\n",
            "Done:  Au20_PPhpy2_10_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au20_PTBu3_8_c0.xyz shape:  (44296,)\n",
            "Done:  Au20_TBBT_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au20_dppm_6_CN_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_SAdm_15_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_SCy_12_dppm_2_c1.xyz shape:  (44296,)\n",
            "Done:  Au21_STBu_15_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au21_S_SAdm_15_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_CCR_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_CCTBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_SAdm_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au22_dppo_6_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_CCTBu_15_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_CCTBu_15_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au23_PPh3_6_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au23_PPh3_10_dpa_2_Cl_c2.xyz shape:  (44296,)\n",
            "Done:  Au23_SCy_16_c-1.xyz shape:  (44296,)\n",
            "Done:  Au23_bylidene_6_CCPh_9_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_PPh3_4_CCPh_14_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_PPh3_10_PET_5_Br2_c1.xyz shape:  (44296,)\n",
            "Done:  Au24_R-dppb_6_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_S-dppb_6_Cl4_c2.xyz shape:  (44296,)\n",
            "Done:  Au24_SAdm_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SCH2Ph_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SPh_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_STBu_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_SePh_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au24_TBTT_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_CCAr_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-mN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-oN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET-pN3_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_16_SPhBr_2_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c1.xyz shape:  (44296,)\n",
            "Done:  Au25_PET_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SCy_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SEt_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_SNap_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SPhF_18_c-1.xyz shape:  (44296,)\n",
            "Done:  Au25_SPh_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_SPh_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SPr_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au25_SePh_5_PPh3_10_Cl2_c1.xyz shape:  (44296,)\n",
            "Done:  Au25_SePh_5_PPh3_10_Cl2_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_bylidene_10_Br7_c2.xyz shape:  (44296,)\n",
            "Done:  Au25_bylidene_10_Br8_c1.xyz shape:  (44296,)\n",
            "Done:  Au28_SCy_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au28_TBBT_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au29_SAdm_19_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_SAdm_18_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_SAdm_18_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_STBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au30_S_STBu_18_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PBu3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PEt3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au32_PPh3_8_dpa_6_c2.xyz shape:  (44296,)\n",
            "Done:  Au32_PPr3_12_Cl8_c0.xyz shape:  (44296,)\n",
            "Done:  Au34_DMBT_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au34_SCy_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_CCPh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_DMBT_24_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_DMBT_24_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SCyp_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SPh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_STol_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_SePh_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au36_TBBT_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_CCPh_20_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au38_DMBT_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_PET_24_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_PET_24_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_S2_SAdm_20_c0.xyz shape:  (44296,)\n",
            "Done:  Au38_STol_20_PPh3_4_c2.xyz shape:  (44296,)\n",
            "Done:  Au40_SAdm_22_c0.xyz shape:  (44296,)\n",
            "Done:  Au40_STol_24_c0.xyz shape:  (44296,)\n",
            "Done:  Au40_dppm_4_CCPh_20_c4.xyz shape:  (44296,)\n",
            "Done:  Au42_SCH2Ph_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_SCy_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_TBBT_26_iso1_c0.xyz shape:  (44296,)\n",
            "Done:  Au42_TBBT_26_iso2_c0.xyz shape:  (44296,)\n",
            "Done:  Au43_SCy_25_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_CCPh_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_DMBT_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_TBBT_26_c0.xyz shape:  (44296,)\n",
            "Done:  Au44_TBBT_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au48_TBBT_28_c0.xyz shape:  (44296,)\n",
            "Done:  Au49_DMBT_27_c0.xyz shape:  (44296,)\n",
            "Done:  Au52_PET_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au52_TBBT_32_c0.xyz shape:  (44296,)\n",
            "Done:  Au54_PEt3_18_Cl12_c0.xyz shape:  (44296,)\n",
            "Done:  Au56_TBBT_34_c0.xyz shape:  (44296,)\n",
            "Done:  Au60_S6_SCH2Ph_36_c0.xyz shape:  (44296,)\n",
            "Done:  Au60_S7_SCH2Ph_36_c0.xyz shape:  (44296,)\n",
            "Done:  Au67_CCR_32_Cl4_c-3.xyz shape:  (44296,)\n",
            "Done:  Au67_SCH2Ph_35_c0.xyz shape:  (44296,)\n",
            "Done:  Au70_S20_PPh3_12_c0.xyz shape:  (44296,)\n",
            "Done:  Au92_TBBT_44_c0.xyz shape:  (44296,)\n",
            "Done:  Au103_S2_SNap_41_c0.xyz shape:  (44296,)\n",
            "Done:  Au108_S24_PPh3_16_c0.xyz shape:  (44296,)\n",
            "Done:  Au110_CCPhCF3_48_c-2.xyz shape:  (44296,)\n",
            "Done:  Au133_TBBT_52_c0.xyz shape:  (44296,)\n",
            "Done:  Au144_PET_60_c0.xyz shape:  (44296,)\n",
            "Done:  Au_PPh3_CH3_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbPh3_4_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbPh3_Cl_c0.xyz shape:  (44296,)\n",
            "Done:  Au_SbTol3_3_SbCl2Tol2_c0.xyz shape:  (44296,)\n",
            "error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "rziN2BWXwncJ",
        "outputId": "e73606a3-8ee4-4f48-eab7-844893386e03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-740eec0d-951d-42cf-8050-a56b8961525c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>names</th>\n",
              "      <th>imag+zpve</th>\n",
              "      <th>u298</th>\n",
              "      <th>h298</th>\n",
              "      <th>g298</th>\n",
              "      <th>gap</th>\n",
              "      <th>dipX</th>\n",
              "      <th>dipY</th>\n",
              "      <th>dipZ</th>\n",
              "      <th>dipTotal</th>\n",
              "      <th>core</th>\n",
              "      <th>Tetrahedral_count</th>\n",
              "      <th>Unconnected_triangles_count</th>\n",
              "      <th>Triangles_with_1_shared_vertex_count</th>\n",
              "      <th>Triangles_with_2_shared_vertices_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AuTM-1.xyz.1315600.hpc-mn1.out</td>\n",
              "      <td>-4.54      9.35       0.361254792113</td>\n",
              "      <td>-126.516193</td>\n",
              "      <td>-126.114631</td>\n",
              "      <td>-126.226368</td>\n",
              "      <td>1.947794</td>\n",
              "      <td>-1.8152</td>\n",
              "      <td>2.4689</td>\n",
              "      <td>-3.5907</td>\n",
              "      <td>11.998</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AuTM-2.xyz.1315601.hpc-mn1.out</td>\n",
              "      <td>5.26       5.70       0.300816191820</td>\n",
              "      <td>-75.059447</td>\n",
              "      <td>-74.731351</td>\n",
              "      <td>-74.816078</td>\n",
              "      <td>0.536785</td>\n",
              "      <td>1.7070</td>\n",
              "      <td>-12.6958</td>\n",
              "      <td>-6.9454</td>\n",
              "      <td>37.038</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AuTM-3.xyz.1315602.hpc-mn1.out</td>\n",
              "      <td>5.72       7.27       0.414537168724</td>\n",
              "      <td>-220.342502</td>\n",
              "      <td>-219.860514</td>\n",
              "      <td>-220.026097</td>\n",
              "      <td>1.281813</td>\n",
              "      <td>1.0570</td>\n",
              "      <td>0.0231</td>\n",
              "      <td>-1.4388</td>\n",
              "      <td>4.538</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AuTM-4.xyz.1315603.hpc-mn1.out</td>\n",
              "      <td>11.56      27.95      0.330145801891</td>\n",
              "      <td>-78.203268</td>\n",
              "      <td>-77.844378</td>\n",
              "      <td>-77.931539</td>\n",
              "      <td>2.252962</td>\n",
              "      <td>6.8291</td>\n",
              "      <td>3.2187</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>19.189</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AuTM-5.xyz.1315604.hpc-mn1.out</td>\n",
              "      <td>1.77       38.84      0.244731836526</td>\n",
              "      <td>-32.384417</td>\n",
              "      <td>-32.124641</td>\n",
              "      <td>-32.182588</td>\n",
              "      <td>4.939549</td>\n",
              "      <td>1.6536</td>\n",
              "      <td>9.0627</td>\n",
              "      <td>20.8189</td>\n",
              "      <td>57.866</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3154</th>\n",
              "      <td>AuTM-3155.xyz.1323291.hpc-mn1.out</td>\n",
              "      <td>14.72      21.95      0.405623835398</td>\n",
              "      <td>-85.628117</td>\n",
              "      <td>-85.190559</td>\n",
              "      <td>-85.282873</td>\n",
              "      <td>2.599206</td>\n",
              "      <td>3.8788</td>\n",
              "      <td>3.9315</td>\n",
              "      <td>1.8642</td>\n",
              "      <td>14.816</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3155</th>\n",
              "      <td>AuTM-3156.xyz.1323292.hpc-mn1.out</td>\n",
              "      <td>7.50       11.77      0.300199763344</td>\n",
              "      <td>-56.400818</td>\n",
              "      <td>-56.077525</td>\n",
              "      <td>-56.153166</td>\n",
              "      <td>1.946276</td>\n",
              "      <td>-0.4391</td>\n",
              "      <td>2.4436</td>\n",
              "      <td>-1.3932</td>\n",
              "      <td>7.236</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3156</th>\n",
              "      <td>AuTM-3157.xyz.1323293.hpc-mn1.out</td>\n",
              "      <td>9.52       16.36      0.262733823975</td>\n",
              "      <td>-56.829549</td>\n",
              "      <td>-56.543338</td>\n",
              "      <td>-56.620601</td>\n",
              "      <td>3.519847</td>\n",
              "      <td>0.2095</td>\n",
              "      <td>3.4129</td>\n",
              "      <td>-1.9723</td>\n",
              "      <td>10.033</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3157</th>\n",
              "      <td>AuTM-3158.xyz.1323294.hpc-mn1.out</td>\n",
              "      <td>7.27       8.38       1.048572866833</td>\n",
              "      <td>-198.407509</td>\n",
              "      <td>-197.277608</td>\n",
              "      <td>-197.473715</td>\n",
              "      <td>1.877934</td>\n",
              "      <td>0.6674</td>\n",
              "      <td>-0.1856</td>\n",
              "      <td>-1.2711</td>\n",
              "      <td>3.680</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3158</th>\n",
              "      <td>AuTM-3159.xyz.1323295.hpc-mn1.out</td>\n",
              "      <td>4.90       5.90       1.261562859735</td>\n",
              "      <td>-226.946163</td>\n",
              "      <td>-225.583462</td>\n",
              "      <td>-225.823290</td>\n",
              "      <td>3.277171</td>\n",
              "      <td>-0.6295</td>\n",
              "      <td>-2.4385</td>\n",
              "      <td>1.0070</td>\n",
              "      <td>6.894</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3159 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740eec0d-951d-42cf-8050-a56b8961525c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-740eec0d-951d-42cf-8050-a56b8961525c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-740eec0d-951d-42cf-8050-a56b8961525c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   names  \\\n",
              "0        AuTM-1.xyz.1315600.hpc-mn1.out    \n",
              "1        AuTM-2.xyz.1315601.hpc-mn1.out    \n",
              "2        AuTM-3.xyz.1315602.hpc-mn1.out    \n",
              "3        AuTM-4.xyz.1315603.hpc-mn1.out    \n",
              "4        AuTM-5.xyz.1315604.hpc-mn1.out    \n",
              "...                                  ...   \n",
              "3154  AuTM-3155.xyz.1323291.hpc-mn1.out    \n",
              "3155  AuTM-3156.xyz.1323292.hpc-mn1.out    \n",
              "3156  AuTM-3157.xyz.1323293.hpc-mn1.out    \n",
              "3157  AuTM-3158.xyz.1323294.hpc-mn1.out    \n",
              "3158  AuTM-3159.xyz.1323295.hpc-mn1.out    \n",
              "\n",
              "                                   imag+zpve        u298        h298  \\\n",
              "0      -4.54      9.35       0.361254792113  -126.516193 -126.114631   \n",
              "1      5.26       5.70       0.300816191820   -75.059447  -74.731351   \n",
              "2      5.72       7.27       0.414537168724  -220.342502 -219.860514   \n",
              "3      11.56      27.95      0.330145801891   -78.203268  -77.844378   \n",
              "4      1.77       38.84      0.244731836526   -32.384417  -32.124641   \n",
              "...                                      ...         ...         ...   \n",
              "3154   14.72      21.95      0.405623835398   -85.628117  -85.190559   \n",
              "3155   7.50       11.77      0.300199763344   -56.400818  -56.077525   \n",
              "3156   9.52       16.36      0.262733823975   -56.829549  -56.543338   \n",
              "3157   7.27       8.38       1.048572866833  -198.407509 -197.277608   \n",
              "3158   4.90       5.90       1.261562859735  -226.946163 -225.583462   \n",
              "\n",
              "            g298       gap    dipX     dipY     dipZ  dipTotal  core  \\\n",
              "0    -126.226368  1.947794 -1.8152   2.4689  -3.5907    11.998     1   \n",
              "1     -74.816078  0.536785  1.7070 -12.6958  -6.9454    37.038     1   \n",
              "2    -220.026097  1.281813  1.0570   0.0231  -1.4388     4.538     1   \n",
              "3     -77.931539  2.252962  6.8291   3.2187   0.0003    19.189     1   \n",
              "4     -32.182588  4.939549  1.6536   9.0627  20.8189    57.866     1   \n",
              "...          ...       ...     ...      ...      ...       ...   ...   \n",
              "3154  -85.282873  2.599206  3.8788   3.9315   1.8642    14.816     1   \n",
              "3155  -56.153166  1.946276 -0.4391   2.4436  -1.3932     7.236     1   \n",
              "3156  -56.620601  3.519847  0.2095   3.4129  -1.9723    10.033     1   \n",
              "3157 -197.473715  1.877934  0.6674  -0.1856  -1.2711     3.680     1   \n",
              "3158 -225.823290  3.277171 -0.6295  -2.4385   1.0070     6.894     1   \n",
              "\n",
              "      Tetrahedral_count  Unconnected_triangles_count  \\\n",
              "0                     0                            0   \n",
              "1                     0                            0   \n",
              "2                     0                            0   \n",
              "3                     0                            0   \n",
              "4                     0                            0   \n",
              "...                 ...                          ...   \n",
              "3154                  0                            0   \n",
              "3155                  0                            0   \n",
              "3156                  0                            0   \n",
              "3157                  0                            0   \n",
              "3158                  0                            0   \n",
              "\n",
              "      Triangles_with_1_shared_vertex_count  \\\n",
              "0                                        0   \n",
              "1                                        0   \n",
              "2                                        0   \n",
              "3                                        0   \n",
              "4                                        0   \n",
              "...                                    ...   \n",
              "3154                                     0   \n",
              "3155                                     0   \n",
              "3156                                     0   \n",
              "3157                                     0   \n",
              "3158                                     0   \n",
              "\n",
              "      Triangles_with_2_shared_vertices_count  \n",
              "0                                          0  \n",
              "1                                          0  \n",
              "2                                          0  \n",
              "3                                          0  \n",
              "4                                          0  \n",
              "...                                      ...  \n",
              "3154                                       0  \n",
              "3155                                       0  \n",
              "3156                                       0  \n",
              "3157                                       0  \n",
              "3158                                       0  \n",
              "\n",
              "[3159 rows x 15 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "DatabaseTM16Jan includes the XTB calculated properties for the AuTM molecules, along with their simplex counts already calculated at 4.0 filter via the function above\n",
        "\"\"\"\n",
        "\n",
        "DatasetTM = pd.read_csv(\"DatabaseTM16Jan.csv\",\n",
        "                  sep=',',\n",
        "                  header = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R5PX6CcoOZp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Appending xyz texts of AuTMs to dataframe\n",
        "\"\"\"\n",
        "os.chdir(\"/content/drive/MyDrive/TM_dataset\")\n",
        "xyz_list = []\n",
        "files = []\n",
        "for i in range(len(DatasetTM)):\n",
        "  print(\"Datapoint: \",i)\n",
        "  filename = \"AuTM-\" + str(i+1) + \".xyz\"\n",
        "  with open(filename) as f:\n",
        "      lines = f.readlines()\n",
        "      files.append(filename)\n",
        "      xyz_list.append(lines)\n",
        "\n",
        "DatasetTM[\"xyz\"]=xyz_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS-kt-1FvwCD"
      },
      "outputs": [],
      "source": [
        "#Persistence Image Parameters\n",
        "resolution=100\n",
        "myspread=0.3\n",
        "min_bound=-0.3\n",
        "max_bound=7\n",
        "electroneg_addition=+0.4\n",
        "electroneg_division=10\n",
        "B1_buffer=0.5\n",
        "B2_buffer=0.05\n",
        "\n",
        "append_persistence_image(DatasetAuTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA8sidF0mGw7"
      },
      "source": [
        "# Read from pre-calculated dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database\")\n",
        "df.to_pickle('DatasetAuNC_290723_withCM_SOAP.pkl')"
      ],
      "metadata": {
        "id": "XuoheGg7a3f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgGD26OnmTv2"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database\")\n",
        "DatasetAuNC = pd.read_pickle('DatasetAuNC_290723_withCM_SOAP.pkl')\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/URECA_22\")\n",
        "DatasetTM = pd.read_pickle('DatasetTM100.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J82C1fy_PFD"
      },
      "source": [
        "## Result plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcVVD96c6KT6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def analyze_regression_performance(true_values, predicted_values, parameters, duration, text_position='top_left', save_image=False, image_name=\"regression_performance.png\"):\n",
        "    # Calculate evaluation metrics\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    # Create a blank figure for text-only display\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.axis('off')  # Turn off axis for text display only\n",
        "\n",
        "    # Determine text position\n",
        "    if text_position == 'top_left':\n",
        "        text_x, text_y = 0.02, 0.98\n",
        "        ha, va = 'left', 'top'\n",
        "    elif text_position == 'top_right':\n",
        "        text_x, text_y = 0.98, 0.98\n",
        "        ha, va = 'right', 'top'\n",
        "    else:\n",
        "        raise ValueError(\"Invalid text_position. Choose 'top_left' or 'top_right'.\")\n",
        "\n",
        "    # Add text with model parameters and evaluation metrics\n",
        "    parameters_text = '\\n'.join(f'{key}: {value}' for key, value in parameters.items())\n",
        "    metrics_text = f\"Mean Absolute Error (MAE): {mae:.4f}\\n\" \\\n",
        "                   f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\" \\\n",
        "                   f\"R-squared (R²): {r2:.4f}\\n\" \\\n",
        "                   f\"Training duration (s): {duration:.1f}\\n\"\n",
        "    text = f\"{parameters_text}\\n\\n{metrics_text}\"\n",
        "    ax.text(text_x, text_y, text, transform=ax.transAxes,\n",
        "            bbox=dict(facecolor='white', edgecolor='black', alpha=0.8),\n",
        "            horizontalalignment=ha, verticalalignment=va)\n",
        "\n",
        "    if save_image:\n",
        "        plt.savefig(image_name, bbox_inches='tight')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFoUzP3B6LnN"
      },
      "outputs": [],
      "source": [
        "def plot_ML_results(true_values, predicted_values, x_tick_sep=1.0, y_tick_sep=1.0, dpi=900, save_image=False, image_name=\"plot_ML_results.png\"):\n",
        "    \"\"\"\n",
        "    Plot the true values against predicted values for machine learning model evaluation.\n",
        "\n",
        "    Parameters:\n",
        "        true_values (array-like): True target values.\n",
        "        predicted_values (array-like): Predicted target values.\n",
        "        x_tick_sep (float): Separation between x-axis tick marks. Default is 1.0.\n",
        "        y_tick_sep (float): Separation between y-axis tick marks. Default is 1.0.\n",
        "        dpi (int): Dots per inch for the image resolution. Default is 900.\n",
        "        save_image (bool): If True, save the image. Default is False.\n",
        "        image_name (str): Name of the saved image file. Default is \"plot_ML_results.png\".\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(true_values, predicted_values, c='teal', s=40)\n",
        "\n",
        "    max_value = max(max(predicted_values), max(true_values))\n",
        "    min_value = min(min(predicted_values), min(true_values))\n",
        "\n",
        "    # Ensure the axes always start at 0.0\n",
        "    max_value = max(max_value, 0.0)\n",
        "    min_value = min(min_value, 0.0)\n",
        "\n",
        "    plt.plot([min_value, max_value], [min_value, max_value], 'black')\n",
        "\n",
        "    plt.xlabel('Calculated HOMO-LUMO gap (eV)', fontsize=25)\n",
        "    plt.ylabel('Predicted HOMO-LUMO gap (eV)', fontsize=25)\n",
        "\n",
        "    plt.tick_params(axis='both', labelsize=20, pad=8)\n",
        "    x_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, x_tick_sep)\n",
        "    y_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, y_tick_sep)\n",
        "\n",
        "    plt.xticks(x_ticks)\n",
        "    plt.yticks(y_ticks)\n",
        "\n",
        "    plt.axis('equal')\n",
        "\n",
        "    if save_image:\n",
        "        plt.savefig(image_name, dpi=dpi)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnucUzOaGOMG"
      },
      "source": [
        "## Pre-training, Convolutional Neural Network\n",
        "\n",
        "Pre-training is done using the AuTM dataset, where the structures are obtained from the tmQM dataset.\n",
        "\n",
        "Pre training using the same PI model structure as non-pretrained CNN models. Afterwards, CNN models will replace the PI processing block with the pre-trained model, with its layer frozen except for the final 2 layers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_cnn_model(dataset, target_variable, learning_rate, patience, batch_size, model_save_name):\n",
        "    x, y = [], []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        x.append(np.asarray(dataset[\"PersImg\"][i]).reshape(100, 100, 1))\n",
        "        y.append(float(dataset[target_variable][i]))\n",
        "\n",
        "    y = np.array(y)\n",
        "    x = np.array(x)\n",
        "\n",
        "    predicted_arr = []\n",
        "    true_arr = []\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    x_train_full = []\n",
        "    y_train_full = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    print(\"Running model...\")\n",
        "    input2 = Input(shape=(100, 100, 1))\n",
        "    conv_1 = Conv2D(16, (3, 3), activation='relu')(input2)\n",
        "    maxpool_1 = MaxPooling2D((2, 2))(conv_1)\n",
        "    conv_2 = Conv2D(16, (3, 3), activation='relu')(maxpool_1)\n",
        "    maxpool_2 = MaxPooling2D((2, 2))(conv_2)\n",
        "    flatten = Flatten()(maxpool_2)\n",
        "    dense_2 = Dense(32, activation='relu', name=\"dense_a\")(flatten)\n",
        "    dense_3 = Dense(16, activation='relu', name=\"dense_b\")(dense_2)\n",
        "    dense_4 = Dense(1, name=\"dense_c\")(dense_3)\n",
        "\n",
        "    model = Model(inputs=input2, outputs=dense_4)\n",
        "    model.summary()\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mean_absolute_error',\n",
        "        metrics=['mean_squared_error']\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    model_checkpoint = ModelCheckpoint(filepath=model_save_name,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       monitor='val_loss',\n",
        "                                       mode='min',\n",
        "                                       verbose=1)\n",
        "    start_time = time.time()  # Start the timer\n",
        "    model.fit([x_train],\n",
        "              y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=500,\n",
        "              verbose=1,\n",
        "              validation_data=([x_test], y_test),\n",
        "              callbacks=[early_stopping, model_checkpoint])\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(\"fitting...\")\n",
        "    model = load_model(model_save_name)\n",
        "\n",
        "    for i in range(len(x_test)):\n",
        "        y_pred = model.predict(np.asarray([x_test[i]]))\n",
        "        predicted_arr.append(y_pred)\n",
        "        true_arr.append(float(y_test[i]))\n",
        "\n",
        "    return predicted_arr, true_arr, duration\n"
      ],
      "metadata": {
        "id": "QsKhJH9OKKeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database/fig4_plots\")\n",
        "\n",
        "# Note: dipole moment variable for AuTM dataset is named \"dipTotal\" instead of \"mu\"\n",
        "target_variable = \"gap\"\n",
        "batch_size = 128\n",
        "learning_rate=0.0005\n",
        "patience = 30\n",
        "model_save_name = \"best_tf_gap.h5\"\n",
        "Filename = \"TF_CNN_pretraining\" + \"_\" + target_variable\n",
        "seed = 42\n",
        "\n",
        "parameters = {'target_variable': target_variable, 'batch_size': batch_size, \"lr\": learning_rate, \"seed\": seed}\n",
        "\n",
        "\n",
        "if target_variable == \"u298\": tick_sep=150\n",
        "elif target_variable == \"gap\": tick_sep=0.5\n",
        "else: tick_sep=50\n",
        "model_name = \"transferTM_\" + target_variable\n",
        "\n",
        "predicted_values, true_values, duration = pretrain_cnn_model(DatasetTM, target_variable=target_variable,\n",
        "                                          learning_rate=learning_rate, patience=patience,\n",
        "                                          batch_size=batch_size, model_save_name=model_save_name)\n",
        "\n",
        "predicted_values = [prediction[0][0] for prediction in predicted_values]\n",
        "\n",
        "analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=False, image_name=Filename+\"_data.png\")\n",
        "plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=False, image_name=Filename+\".pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9y3vZxiTRRYK",
        "outputId": "3ac83f85-0759-4258-ba13-f66732c33cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN model training"
      ],
      "metadata": {
        "id": "lxPJr3a-aaJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_model(use_simplex=True, use_charge=True, pre_trained_model=None):\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Define the input layers\n",
        "    input_1 = Input(shape=(5,))\n",
        "    input_2 = Input(shape=(100, 100, 1))\n",
        "    input_3 = Input(shape=(1,))\n",
        "\n",
        "    # Model 1 - Simplexes\n",
        "    if use_simplex:\n",
        "        dense_1 = Dense(5, activation='relu')(input_1)\n",
        "        dense_1_extra = Dense(3, activation='relu')(dense_1)\n",
        "        input_concat = [dense_1_extra]\n",
        "    else:\n",
        "        input_concat = []\n",
        "\n",
        "    # Model 2 - Convolutional Neural Network\n",
        "    base_model = load_model(pre_trained_model)\n",
        "    for layer in base_model.layers[:-2]:\n",
        "        layer.trainable = False\n",
        "    new_layer = base_model(input_2)\n",
        "    flatten = Flatten()(new_layer)\n",
        "    input_concat.append(flatten)\n",
        "\n",
        "    # Model 3 - Charge\n",
        "    if use_charge:\n",
        "        input_concat.append(input_3)\n",
        "\n",
        "    # Concatenate input layers based on whether simplex and/or charge are used\n",
        "    if len(input_concat) > 1:\n",
        "        x = Concatenate()(input_concat)\n",
        "    else:\n",
        "        x = input_concat[0]\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    output = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_cnn_leave_one_out(Dataset, sample_size, target_variable, batch_size, learning_rate, patience, model_save_name, pre_trained_model, use_simplex=True, use_charge=True, random_state=42):\n",
        "    random.seed(random_state)\n",
        "    np.random.seed(random_state)\n",
        "    tf.random.set_seed(random_state)\n",
        "\n",
        "    # Initialize the lists for input data and target variable\n",
        "    x1, x2, x3, y = [], [], [], []\n",
        "\n",
        "    # Append data to the lists\n",
        "    for i in range(len(Dataset)):\n",
        "        x1.append(np.asarray([Dataset[\"core\"][i],\n",
        "                              Dataset[\"Tetrahedral_count\"][i],\n",
        "                              Dataset[\"Unconnected_triangles_count\"][i],\n",
        "                              Dataset[\"Triangles_with_1_shared_vertex_count\"][i],\n",
        "                              Dataset[\"Triangles_with_2_shared_vertices_count\"][i]]))\n",
        "\n",
        "        x2.append(np.asarray(Dataset[\"PersImg\"][i]).reshape(100, 100, 1))\n",
        "        x3.append([Dataset[\"Charge\"][i]])\n",
        "        y.append(float(Dataset[\"gap\"][i]))\n",
        "\n",
        "    y = np.array(y)\n",
        "    x1 = np.array(x1)\n",
        "    x2 = np.array(x2)\n",
        "    x3 = np.array(x3)\n",
        "\n",
        "    predicted_arr = []\n",
        "    true_arr = []\n",
        "    MAE_arr = []\n",
        "    RMSE_arr = []\n",
        "    TotalAccuracy = 0\n",
        "    TotalError = 0\n",
        "    MSE = 0\n",
        "\n",
        "    x1_train_full = x1.tolist()\n",
        "    x2_train_full = x2.tolist()\n",
        "    x3_train_full = x3.tolist()\n",
        "    y_train_full = y.tolist()\n",
        "\n",
        "    for test_index in range(len(x1)):\n",
        "        print(\"Cycle: \", test_index)\n",
        "\n",
        "        x1_train = deepcopy(x1_train_full)\n",
        "        x2_train = deepcopy(x2_train_full)\n",
        "        x3_train = deepcopy(x3_train_full)\n",
        "        y_train = deepcopy(y_train_full)\n",
        "\n",
        "        x1_test = x1[test_index]\n",
        "        x2_test = x2[test_index]\n",
        "        x3_test = x3[test_index]\n",
        "        y_test = y[test_index]\n",
        "\n",
        "        x1_train.pop(test_index)\n",
        "        x2_train.pop(test_index)\n",
        "        x3_train.pop(test_index)\n",
        "        y_train.pop(test_index)\n",
        "\n",
        "        x1_train = np.asarray(x1_train)\n",
        "        x2_train = np.asarray(x2_train)\n",
        "        x3_train = np.asarray(x3_train)\n",
        "        y_train = np.asarray(y_train)\n",
        "\n",
        "\n",
        "        # Split training data into train and validation sets\n",
        "        x1_train, x1_val, x2_train, x2_val, x3_train, x3_val, y_train, y_val = train_test_split(\n",
        "            x1_train, x2_train, x3_train, y_train, test_size=10, random_state=random_state)\n",
        "\n",
        "        print(\"start compiling\")\n",
        "\n",
        "        model = create_tf_model(use_simplex=use_simplex, use_charge=use_charge, pre_trained_model=pre_trained_model)\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                      loss='mean_absolute_error',\n",
        "                      metrics=['mean_squared_error'])\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "        model_checkpoint = ModelCheckpoint(filepath=model_save_name,\n",
        "                                           save_best_only=True,\n",
        "                                           save_weights_only=False,\n",
        "                                           monitor='val_loss',\n",
        "                                           mode='min',\n",
        "                                           verbose=1)\n",
        "\n",
        "        start_time = time.time()  # Start the timer\n",
        "        print(\"start fitting\")\n",
        "        model.fit([x1_train, x2_train, x3_train],\n",
        "                  y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=500,\n",
        "                  verbose=1,\n",
        "                  validation_data=([x1_val, x2_val, x3_val], y_val),\n",
        "                  callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "        end_time = time.time()  # Stop the timer\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        model = load_model(model_save_name)\n",
        "\n",
        "        y_pred = model.predict([np.asarray([x1_test]), np.asarray([x2_test]), np.asarray([x3_test])])\n",
        "        predicted_arr.append(y_pred)\n",
        "        true_arr.append(float(y_test))\n",
        "\n",
        "    return predicted_arr, true_arr, duration\n"
      ],
      "metadata": {
        "id": "_3Uc96RgTtg3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/AuNC database/fig4_plots\")\n",
        "\n",
        "pre_trained_model = \"best_tf_gap.h5\" #set pre-trained model variable\n",
        "\n",
        "target_variable = \"gap\"\n",
        "batch_size = 24\n",
        "learning_rate=0.001\n",
        "patience = 40\n",
        "Filename = \"CNN\" + \"_\" + target_variable\n",
        "seed = 42\n",
        "model_save_name = \"best_cnn_gap_tf.h5\"\n",
        "if target_variable == \"u298\": tick_sep=150\n",
        "elif target_variable == \"gap\": tick_sep=0.5\n",
        "else: tick_sep=50\n",
        "\n",
        "parameters = {'target_variable': target_variable, 'batch_size': batch_size, \"lr\": learning_rate, \"seed\": seed}\n",
        "\n",
        "predicted_values, true_values, duration = run_cnn_leave_one_out(Dataset=DatasetAuNC,\n",
        "                                                        sample_size=len(DatasetAuNC),\n",
        "                                                        target_variable=target_variable,\n",
        "                                                        batch_size=batch_size, learning_rate=learning_rate, patience=patience,\n",
        "                                                        model_save_name=model_save_name, pre_trained_model = pre_trained_model,\n",
        "                                                        random_state=seed,\n",
        "                                                        use_simplex=True, use_charge=True,\n",
        "                                                        )\n",
        "analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=False, image_name=Filename+\"_data.png\")\n",
        "plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=False, image_name=Filename+\".pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-nZ26W_rW8wE",
        "outputId": "fea26167-a543-49aa-9260-6b37085f24cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cycle:  0\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5769 - mean_squared_error: 3.8868 \n",
            "Epoch 1: val_loss improved from inf to 1.50206, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 4s 88ms/step - loss: 1.5769 - mean_squared_error: 3.8868 - val_loss: 1.5021 - val_mean_squared_error: 3.1025\n",
            "Epoch 2/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.2065 - mean_squared_error: 2.3083\n",
            "Epoch 2: val_loss improved from 1.50206 to 1.15557, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 1.1905 - mean_squared_error: 2.2511 - val_loss: 1.1556 - val_mean_squared_error: 2.1040\n",
            "Epoch 3/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9727 - mean_squared_error: 1.5566\n",
            "Epoch 3: val_loss improved from 1.15557 to 0.82554, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.9727 - mean_squared_error: 1.5566 - val_loss: 0.8255 - val_mean_squared_error: 1.3671\n",
            "Epoch 4/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7769 - mean_squared_error: 1.1186\n",
            "Epoch 4: val_loss improved from 0.82554 to 0.79848, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.7769 - mean_squared_error: 1.1186 - val_loss: 0.7985 - val_mean_squared_error: 1.1323\n",
            "Epoch 5/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6648 - mean_squared_error: 0.8582\n",
            "Epoch 5: val_loss improved from 0.79848 to 0.71233, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6600 - mean_squared_error: 0.8418 - val_loss: 0.7123 - val_mean_squared_error: 0.9818\n",
            "Epoch 6/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5997 - mean_squared_error: 0.7228\n",
            "Epoch 6: val_loss improved from 0.71233 to 0.64649, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.5941 - mean_squared_error: 0.7111 - val_loss: 0.6465 - val_mean_squared_error: 0.8944\n",
            "Epoch 7/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5378 - mean_squared_error: 0.5758\n",
            "Epoch 7: val_loss improved from 0.64649 to 0.64253, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.5740 - mean_squared_error: 0.6713 - val_loss: 0.6425 - val_mean_squared_error: 0.8258\n",
            "Epoch 8/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5295 - mean_squared_error: 0.5818\n",
            "Epoch 8: val_loss improved from 0.64253 to 0.59479, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.5371 - mean_squared_error: 0.5846 - val_loss: 0.5948 - val_mean_squared_error: 0.7629\n",
            "Epoch 9/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5422 - mean_squared_error: 0.5853\n",
            "Epoch 9: val_loss did not improve from 0.59479\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.5381 - mean_squared_error: 0.5717 - val_loss: 0.7046 - val_mean_squared_error: 0.8050\n",
            "Epoch 10/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5180 - mean_squared_error: 0.5354\n",
            "Epoch 10: val_loss did not improve from 0.59479\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.5180 - mean_squared_error: 0.5354 - val_loss: 0.7380 - val_mean_squared_error: 0.8369\n",
            "Epoch 11/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5297 - mean_squared_error: 0.5716\n",
            "Epoch 11: val_loss improved from 0.59479 to 0.54385, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.5310 - mean_squared_error: 0.5624 - val_loss: 0.5439 - val_mean_squared_error: 0.6166\n",
            "Epoch 12/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5193 - mean_squared_error: 0.6569\n",
            "Epoch 12: val_loss did not improve from 0.54385\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.4734 - mean_squared_error: 0.4616 - val_loss: 0.5467 - val_mean_squared_error: 0.5720\n",
            "Epoch 13/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4471 - mean_squared_error: 0.3509\n",
            "Epoch 13: val_loss improved from 0.54385 to 0.51229, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4502 - mean_squared_error: 0.4176 - val_loss: 0.5123 - val_mean_squared_error: 0.5291\n",
            "Epoch 14/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4435 - mean_squared_error: 0.3198\n",
            "Epoch 14: val_loss did not improve from 0.51229\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4393 - mean_squared_error: 0.3964 - val_loss: 0.5582 - val_mean_squared_error: 0.5210\n",
            "Epoch 15/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3843 - mean_squared_error: 0.2400\n",
            "Epoch 15: val_loss improved from 0.51229 to 0.47959, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.4251 - mean_squared_error: 0.3668 - val_loss: 0.4796 - val_mean_squared_error: 0.4333\n",
            "Epoch 16/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5260 - mean_squared_error: 0.5007\n",
            "Epoch 16: val_loss improved from 0.47959 to 0.46565, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.4129 - mean_squared_error: 0.3504 - val_loss: 0.4657 - val_mean_squared_error: 0.4035\n",
            "Epoch 17/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2854 - mean_squared_error: 0.1649\n",
            "Epoch 17: val_loss improved from 0.46565 to 0.44038, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3929 - mean_squared_error: 0.3261 - val_loss: 0.4404 - val_mean_squared_error: 0.3735\n",
            "Epoch 18/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4846 - mean_squared_error: 0.4939\n",
            "Epoch 18: val_loss improved from 0.44038 to 0.42336, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3966 - mean_squared_error: 0.3159 - val_loss: 0.4234 - val_mean_squared_error: 0.3333\n",
            "Epoch 19/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3610 - mean_squared_error: 0.2384\n",
            "Epoch 19: val_loss improved from 0.42336 to 0.41521, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3637 - mean_squared_error: 0.2781 - val_loss: 0.4152 - val_mean_squared_error: 0.3138\n",
            "Epoch 20/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3519 - mean_squared_error: 0.2473\n",
            "Epoch 20: val_loss did not improve from 0.41521\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3578 - mean_squared_error: 0.2661 - val_loss: 0.5005 - val_mean_squared_error: 0.3619\n",
            "Epoch 21/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4065 - mean_squared_error: 0.3702\n",
            "Epoch 21: val_loss improved from 0.41521 to 0.37657, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3585 - mean_squared_error: 0.2600 - val_loss: 0.3766 - val_mean_squared_error: 0.2717\n",
            "Epoch 22/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4388 - mean_squared_error: 0.4273\n",
            "Epoch 22: val_loss did not improve from 0.37657\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3374 - mean_squared_error: 0.2417 - val_loss: 0.3944 - val_mean_squared_error: 0.2801\n",
            "Epoch 23/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3329 - mean_squared_error: 0.2747\n",
            "Epoch 23: val_loss did not improve from 0.37657\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3509 - mean_squared_error: 0.2465 - val_loss: 0.3784 - val_mean_squared_error: 0.2721\n",
            "Epoch 24/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2139 - mean_squared_error: 0.0828\n",
            "Epoch 24: val_loss did not improve from 0.37657\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.3252 - mean_squared_error: 0.2257 - val_loss: 0.3900 - val_mean_squared_error: 0.2707\n",
            "Epoch 25/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2668 - mean_squared_error: 0.1420\n",
            "Epoch 25: val_loss improved from 0.37657 to 0.34723, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3266 - mean_squared_error: 0.2204 - val_loss: 0.3472 - val_mean_squared_error: 0.2347\n",
            "Epoch 26/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3496 - mean_squared_error: 0.2228\n",
            "Epoch 26: val_loss did not improve from 0.34723\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3225 - mean_squared_error: 0.2136 - val_loss: 0.3803 - val_mean_squared_error: 0.2576\n",
            "Epoch 27/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3760 - mean_squared_error: 0.2235\n",
            "Epoch 27: val_loss improved from 0.34723 to 0.34062, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3152 - mean_squared_error: 0.2046 - val_loss: 0.3406 - val_mean_squared_error: 0.2205\n",
            "Epoch 28/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2871 - mean_squared_error: 0.1437\n",
            "Epoch 28: val_loss improved from 0.34062 to 0.33560, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3111 - mean_squared_error: 0.1959 - val_loss: 0.3356 - val_mean_squared_error: 0.2156\n",
            "Epoch 29/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3896 - mean_squared_error: 0.2919\n",
            "Epoch 29: val_loss did not improve from 0.33560\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3031 - mean_squared_error: 0.1884 - val_loss: 0.3445 - val_mean_squared_error: 0.2151\n",
            "Epoch 30/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2290 - mean_squared_error: 0.0913\n",
            "Epoch 30: val_loss did not improve from 0.33560\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3014 - mean_squared_error: 0.1844 - val_loss: 0.3846 - val_mean_squared_error: 0.2269\n",
            "Epoch 31/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3547 - mean_squared_error: 0.2701\n",
            "Epoch 31: val_loss did not improve from 0.33560\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3270 - mean_squared_error: 0.2052 - val_loss: 0.3402 - val_mean_squared_error: 0.1949\n",
            "Epoch 32/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2817 - mean_squared_error: 0.1822\n",
            "Epoch 32: val_loss improved from 0.33560 to 0.30873, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3151 - mean_squared_error: 0.1945 - val_loss: 0.3087 - val_mean_squared_error: 0.1898\n",
            "Epoch 33/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1881 - mean_squared_error: 0.0479\n",
            "Epoch 33: val_loss did not improve from 0.30873\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2987 - mean_squared_error: 0.1860 - val_loss: 0.3751 - val_mean_squared_error: 0.2528\n",
            "Epoch 34/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2908 - mean_squared_error: 0.1744\n",
            "Epoch 34: val_loss did not improve from 0.30873\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2997 - mean_squared_error: 0.1876 - val_loss: 0.3412 - val_mean_squared_error: 0.1917\n",
            "Epoch 35/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3929 - mean_squared_error: 0.3061\n",
            "Epoch 35: val_loss did not improve from 0.30873\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2988 - mean_squared_error: 0.1773 - val_loss: 0.3873 - val_mean_squared_error: 0.2181\n",
            "Epoch 36/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2902 - mean_squared_error: 0.1338\n",
            "Epoch 36: val_loss improved from 0.30873 to 0.30336, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3030 - mean_squared_error: 0.1806 - val_loss: 0.3034 - val_mean_squared_error: 0.1722\n",
            "Epoch 37/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3551 - mean_squared_error: 0.2863\n",
            "Epoch 37: val_loss did not improve from 0.30336\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2779 - mean_squared_error: 0.1567 - val_loss: 0.3119 - val_mean_squared_error: 0.1696\n",
            "Epoch 38/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2838 - mean_squared_error: 0.1563\n",
            "Epoch 38: val_loss did not improve from 0.30336\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2813 - mean_squared_error: 0.1556 - val_loss: 0.3465 - val_mean_squared_error: 0.2132\n",
            "Epoch 39/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3419 - mean_squared_error: 0.2222\n",
            "Epoch 39: val_loss did not improve from 0.30336\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2863 - mean_squared_error: 0.1630 - val_loss: 0.3035 - val_mean_squared_error: 0.1598\n",
            "Epoch 40/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2689 - mean_squared_error: 0.1198\n",
            "Epoch 40: val_loss improved from 0.30336 to 0.26942, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2711 - mean_squared_error: 0.1472 - val_loss: 0.2694 - val_mean_squared_error: 0.1384\n",
            "Epoch 41/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.1628\n",
            "Epoch 41: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2760 - mean_squared_error: 0.1485 - val_loss: 0.3056 - val_mean_squared_error: 0.1604\n",
            "Epoch 42/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3470 - mean_squared_error: 0.1734\n",
            "Epoch 42: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2987 - mean_squared_error: 0.1640 - val_loss: 0.4107 - val_mean_squared_error: 0.2759\n",
            "Epoch 43/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2528 - mean_squared_error: 0.0931\n",
            "Epoch 43: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3302 - mean_squared_error: 0.2009 - val_loss: 0.2719 - val_mean_squared_error: 0.1371\n",
            "Epoch 44/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2714 - mean_squared_error: 0.1343\n",
            "Epoch 44: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2801 - mean_squared_error: 0.1526 - val_loss: 0.3366 - val_mean_squared_error: 0.1670\n",
            "Epoch 45/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5198 - mean_squared_error: 0.4637\n",
            "Epoch 45: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2873 - mean_squared_error: 0.1592 - val_loss: 0.3098 - val_mean_squared_error: 0.1505\n",
            "Epoch 46/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2930 - mean_squared_error: 0.1366\n",
            "Epoch 46: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2952 - mean_squared_error: 0.1605 - val_loss: 0.3032 - val_mean_squared_error: 0.1563\n",
            "Epoch 47/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3100 - mean_squared_error: 0.1755\n",
            "Epoch 47: val_loss did not improve from 0.26942\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2778 - mean_squared_error: 0.1437 - val_loss: 0.2794 - val_mean_squared_error: 0.1333\n",
            "Epoch 48/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3005 - mean_squared_error: 0.1300\n",
            "Epoch 48: val_loss improved from 0.26942 to 0.26047, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2616 - mean_squared_error: 0.1310 - val_loss: 0.2605 - val_mean_squared_error: 0.1168\n",
            "Epoch 49/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2559 - mean_squared_error: 0.1148\n",
            "Epoch 49: val_loss did not improve from 0.26047\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2718 - mean_squared_error: 0.1376 - val_loss: 0.3094 - val_mean_squared_error: 0.1374\n",
            "Epoch 50/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3002 - mean_squared_error: 0.1346\n",
            "Epoch 50: val_loss improved from 0.26047 to 0.24748, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2862 - mean_squared_error: 0.1550 - val_loss: 0.2475 - val_mean_squared_error: 0.1071\n",
            "Epoch 51/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1836 - mean_squared_error: 0.0612\n",
            "Epoch 51: val_loss did not improve from 0.24748\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2571 - mean_squared_error: 0.1235 - val_loss: 0.2649 - val_mean_squared_error: 0.1190\n",
            "Epoch 52/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2890 - mean_squared_error: 0.1573\n",
            "Epoch 52: val_loss improved from 0.24748 to 0.24142, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2492 - mean_squared_error: 0.1194 - val_loss: 0.2414 - val_mean_squared_error: 0.1036\n",
            "Epoch 53/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1316 - mean_squared_error: 0.0347\n",
            "Epoch 53: val_loss improved from 0.24142 to 0.20838, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2473 - mean_squared_error: 0.1171 - val_loss: 0.2084 - val_mean_squared_error: 0.0856\n",
            "Epoch 54/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2878 - mean_squared_error: 0.1775\n",
            "Epoch 54: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2542 - mean_squared_error: 0.1197 - val_loss: 0.2446 - val_mean_squared_error: 0.1035\n",
            "Epoch 55/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2401 - mean_squared_error: 0.0916\n",
            "Epoch 55: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2459 - mean_squared_error: 0.1158 - val_loss: 0.2996 - val_mean_squared_error: 0.1283\n",
            "Epoch 56/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.1279\n",
            "Epoch 56: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2689 - mean_squared_error: 0.1267 - val_loss: 0.2146 - val_mean_squared_error: 0.0852\n",
            "Epoch 57/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2407 - mean_squared_error: 0.1012\n",
            "Epoch 57: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2615 - mean_squared_error: 0.1232 - val_loss: 0.2408 - val_mean_squared_error: 0.1004\n",
            "Epoch 58/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3194 - mean_squared_error: 0.1613\n",
            "Epoch 58: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2760 - mean_squared_error: 0.1465 - val_loss: 0.2651 - val_mean_squared_error: 0.1125\n",
            "Epoch 59/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2851 - mean_squared_error: 0.1130\n",
            "Epoch 59: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2588 - mean_squared_error: 0.1252 - val_loss: 0.2872 - val_mean_squared_error: 0.1395\n",
            "Epoch 60/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3006 - mean_squared_error: 0.1409\n",
            "Epoch 60: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2641 - mean_squared_error: 0.1256 - val_loss: 0.2221 - val_mean_squared_error: 0.0922\n",
            "Epoch 61/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3666 - mean_squared_error: 0.2339\n",
            "Epoch 61: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2374 - mean_squared_error: 0.1048 - val_loss: 0.2128 - val_mean_squared_error: 0.0903\n",
            "Epoch 62/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2214 - mean_squared_error: 0.0786\n",
            "Epoch 62: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2377 - mean_squared_error: 0.1067 - val_loss: 0.2251 - val_mean_squared_error: 0.0993\n",
            "Epoch 63/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2085 - mean_squared_error: 0.0831\n",
            "Epoch 63: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2486 - mean_squared_error: 0.1127 - val_loss: 0.2090 - val_mean_squared_error: 0.0871\n",
            "Epoch 64/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2055 - mean_squared_error: 0.0615\n",
            "Epoch 64: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2363 - mean_squared_error: 0.1016 - val_loss: 0.2569 - val_mean_squared_error: 0.1179\n",
            "Epoch 65/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3239 - mean_squared_error: 0.1546\n",
            "Epoch 65: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2740 - mean_squared_error: 0.1295 - val_loss: 0.2581 - val_mean_squared_error: 0.1123\n",
            "Epoch 66/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2581 - mean_squared_error: 0.1643\n",
            "Epoch 66: val_loss did not improve from 0.20838\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2672 - mean_squared_error: 0.1382 - val_loss: 0.2609 - val_mean_squared_error: 0.1084\n",
            "Epoch 67/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2400 - mean_squared_error: 0.1067\n",
            "Epoch 67: val_loss improved from 0.20838 to 0.20719, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2400 - mean_squared_error: 0.1067 - val_loss: 0.2072 - val_mean_squared_error: 0.0808\n",
            "Epoch 68/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1870 - mean_squared_error: 0.0663\n",
            "Epoch 68: val_loss improved from 0.20719 to 0.19954, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2339 - mean_squared_error: 0.1038 - val_loss: 0.1995 - val_mean_squared_error: 0.0797\n",
            "Epoch 69/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2311 - mean_squared_error: 0.0870\n",
            "Epoch 69: val_loss did not improve from 0.19954\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2350 - mean_squared_error: 0.0987 - val_loss: 0.2077 - val_mean_squared_error: 0.0845\n",
            "Epoch 70/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2835 - mean_squared_error: 0.1171\n",
            "Epoch 70: val_loss improved from 0.19954 to 0.19394, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2481 - mean_squared_error: 0.1132 - val_loss: 0.1939 - val_mean_squared_error: 0.0782\n",
            "Epoch 71/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1614 - mean_squared_error: 0.0426\n",
            "Epoch 71: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2331 - mean_squared_error: 0.1011 - val_loss: 0.2546 - val_mean_squared_error: 0.1035\n",
            "Epoch 72/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1922 - mean_squared_error: 0.0490\n",
            "Epoch 72: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2403 - mean_squared_error: 0.1028 - val_loss: 0.2132 - val_mean_squared_error: 0.0824\n",
            "Epoch 73/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2956 - mean_squared_error: 0.1395\n",
            "Epoch 73: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2237 - mean_squared_error: 0.0933 - val_loss: 0.2003 - val_mean_squared_error: 0.0766\n",
            "Epoch 74/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2285 - mean_squared_error: 0.1048\n",
            "Epoch 74: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2312 - mean_squared_error: 0.0999 - val_loss: 0.2075 - val_mean_squared_error: 0.0790\n",
            "Epoch 75/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2631 - mean_squared_error: 0.1248\n",
            "Epoch 75: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2317 - mean_squared_error: 0.0970 - val_loss: 0.2388 - val_mean_squared_error: 0.1278\n",
            "Epoch 76/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2306 - mean_squared_error: 0.0970\n",
            "Epoch 76: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2306 - mean_squared_error: 0.0970 - val_loss: 0.2323 - val_mean_squared_error: 0.1136\n",
            "Epoch 77/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.1051\n",
            "Epoch 77: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2989 - mean_squared_error: 0.1657 - val_loss: 0.3499 - val_mean_squared_error: 0.1755\n",
            "Epoch 78/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3249 - mean_squared_error: 0.1461\n",
            "Epoch 78: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2848 - mean_squared_error: 0.1423 - val_loss: 0.2377 - val_mean_squared_error: 0.1087\n",
            "Epoch 79/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.1051\n",
            "Epoch 79: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2575 - mean_squared_error: 0.1182 - val_loss: 0.2651 - val_mean_squared_error: 0.1634\n",
            "Epoch 80/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.1181\n",
            "Epoch 80: val_loss did not improve from 0.19394\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2564 - mean_squared_error: 0.1134 - val_loss: 0.2305 - val_mean_squared_error: 0.1054\n",
            "Epoch 81/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.0884\n",
            "Epoch 81: val_loss improved from 0.19394 to 0.18234, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2191 - mean_squared_error: 0.0884 - val_loss: 0.1823 - val_mean_squared_error: 0.0672\n",
            "Epoch 82/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3069 - mean_squared_error: 0.1693\n",
            "Epoch 82: val_loss did not improve from 0.18234\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2309 - mean_squared_error: 0.0982 - val_loss: 0.1867 - val_mean_squared_error: 0.0713\n",
            "Epoch 83/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2006 - mean_squared_error: 0.0666\n",
            "Epoch 83: val_loss did not improve from 0.18234\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2346 - mean_squared_error: 0.0979 - val_loss: 0.2674 - val_mean_squared_error: 0.1125\n",
            "Epoch 84/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3306 - mean_squared_error: 0.2007\n",
            "Epoch 84: val_loss did not improve from 0.18234\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2361 - mean_squared_error: 0.1072 - val_loss: 0.1840 - val_mean_squared_error: 0.0690\n",
            "Epoch 85/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3096 - mean_squared_error: 0.1788\n",
            "Epoch 85: val_loss did not improve from 0.18234\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2191 - mean_squared_error: 0.0905 - val_loss: 0.1989 - val_mean_squared_error: 0.0731\n",
            "Epoch 86/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2694 - mean_squared_error: 0.1180\n",
            "Epoch 86: val_loss improved from 0.18234 to 0.17858, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2147 - mean_squared_error: 0.0853 - val_loss: 0.1786 - val_mean_squared_error: 0.0691\n",
            "Epoch 87/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1838 - mean_squared_error: 0.1103\n",
            "Epoch 87: val_loss improved from 0.17858 to 0.16484, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2135 - mean_squared_error: 0.0881 - val_loss: 0.1648 - val_mean_squared_error: 0.0639\n",
            "Epoch 88/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2541 - mean_squared_error: 0.1398\n",
            "Epoch 88: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2106 - mean_squared_error: 0.0833 - val_loss: 0.1667 - val_mean_squared_error: 0.0633\n",
            "Epoch 89/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1937 - mean_squared_error: 0.0611\n",
            "Epoch 89: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2107 - mean_squared_error: 0.0834 - val_loss: 0.2135 - val_mean_squared_error: 0.0786\n",
            "Epoch 90/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2461 - mean_squared_error: 0.1019\n",
            "Epoch 90: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2118 - mean_squared_error: 0.0852 - val_loss: 0.2467 - val_mean_squared_error: 0.0986\n",
            "Epoch 91/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2283 - mean_squared_error: 0.1285\n",
            "Epoch 91: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2437 - mean_squared_error: 0.1061 - val_loss: 0.1745 - val_mean_squared_error: 0.0632\n",
            "Epoch 92/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1990 - mean_squared_error: 0.0641\n",
            "Epoch 92: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2306 - mean_squared_error: 0.0998 - val_loss: 0.1901 - val_mean_squared_error: 0.0694\n",
            "Epoch 93/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2343 - mean_squared_error: 0.0990\n",
            "Epoch 93: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2271 - mean_squared_error: 0.0935 - val_loss: 0.2119 - val_mean_squared_error: 0.0827\n",
            "Epoch 94/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2311 - mean_squared_error: 0.1122\n",
            "Epoch 94: val_loss did not improve from 0.16484\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2064 - mean_squared_error: 0.0802 - val_loss: 0.1840 - val_mean_squared_error: 0.0628\n",
            "Epoch 95/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2597 - mean_squared_error: 0.1384\n",
            "Epoch 95: val_loss improved from 0.16484 to 0.15770, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2060 - mean_squared_error: 0.0799 - val_loss: 0.1577 - val_mean_squared_error: 0.0591\n",
            "Epoch 96/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1624 - mean_squared_error: 0.0509\n",
            "Epoch 96: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2142 - mean_squared_error: 0.0890 - val_loss: 0.1595 - val_mean_squared_error: 0.0573\n",
            "Epoch 97/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1193 - mean_squared_error: 0.0268\n",
            "Epoch 97: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2021 - mean_squared_error: 0.0787 - val_loss: 0.1658 - val_mean_squared_error: 0.0593\n",
            "Epoch 98/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1957 - mean_squared_error: 0.0621\n",
            "Epoch 98: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1970 - mean_squared_error: 0.0762 - val_loss: 0.1952 - val_mean_squared_error: 0.0688\n",
            "Epoch 99/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1971 - mean_squared_error: 0.0672\n",
            "Epoch 99: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2035 - mean_squared_error: 0.0802 - val_loss: 0.1829 - val_mean_squared_error: 0.0628\n",
            "Epoch 100/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1954 - mean_squared_error: 0.0779\n",
            "Epoch 100: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1995 - mean_squared_error: 0.0784 - val_loss: 0.1834 - val_mean_squared_error: 0.0619\n",
            "Epoch 101/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1861 - mean_squared_error: 0.0654\n",
            "Epoch 101: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1950 - mean_squared_error: 0.0754 - val_loss: 0.2005 - val_mean_squared_error: 0.0655\n",
            "Epoch 102/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1958 - mean_squared_error: 0.0764\n",
            "Epoch 102: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1958 - mean_squared_error: 0.0764 - val_loss: 0.2335 - val_mean_squared_error: 0.0804\n",
            "Epoch 103/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2260 - mean_squared_error: 0.0892\n",
            "Epoch 103: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2215 - mean_squared_error: 0.0914 - val_loss: 0.1965 - val_mean_squared_error: 0.0671\n",
            "Epoch 104/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2115 - mean_squared_error: 0.0810\n",
            "Epoch 104: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2095 - mean_squared_error: 0.0800 - val_loss: 0.1925 - val_mean_squared_error: 0.0631\n",
            "Epoch 105/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2039 - mean_squared_error: 0.0797\n",
            "Epoch 105: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2011 - mean_squared_error: 0.0778 - val_loss: 0.1743 - val_mean_squared_error: 0.0569\n",
            "Epoch 106/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2073 - mean_squared_error: 0.0847\n",
            "Epoch 106: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2012 - mean_squared_error: 0.0779 - val_loss: 0.1757 - val_mean_squared_error: 0.0594\n",
            "Epoch 107/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0778\n",
            "Epoch 107: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2002 - mean_squared_error: 0.0766 - val_loss: 0.1836 - val_mean_squared_error: 0.0601\n",
            "Epoch 108/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2083 - mean_squared_error: 0.0848\n",
            "Epoch 108: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2052 - mean_squared_error: 0.0825 - val_loss: 0.2199 - val_mean_squared_error: 0.0737\n",
            "Epoch 109/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2080 - mean_squared_error: 0.0836\n",
            "Epoch 109: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2026 - mean_squared_error: 0.0777 - val_loss: 0.1751 - val_mean_squared_error: 0.0568\n",
            "Epoch 110/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1997 - mean_squared_error: 0.0751\n",
            "Epoch 110: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2097 - mean_squared_error: 0.0827 - val_loss: 0.1847 - val_mean_squared_error: 0.0625\n",
            "Epoch 111/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2041 - mean_squared_error: 0.0822\n",
            "Epoch 111: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2104 - mean_squared_error: 0.0863 - val_loss: 0.2602 - val_mean_squared_error: 0.0937\n",
            "Epoch 112/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2087 - mean_squared_error: 0.0817\n",
            "Epoch 112: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2086 - mean_squared_error: 0.0819 - val_loss: 0.1762 - val_mean_squared_error: 0.0584\n",
            "Epoch 113/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1919 - mean_squared_error: 0.0688\n",
            "Epoch 113: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1941 - mean_squared_error: 0.0732 - val_loss: 0.2178 - val_mean_squared_error: 0.0710\n",
            "Epoch 114/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1925 - mean_squared_error: 0.0773\n",
            "Epoch 114: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1999 - mean_squared_error: 0.0791 - val_loss: 0.2321 - val_mean_squared_error: 0.0784\n",
            "Epoch 115/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2040 - mean_squared_error: 0.0797\n",
            "Epoch 115: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2038 - mean_squared_error: 0.0788 - val_loss: 0.2483 - val_mean_squared_error: 0.0862\n",
            "Epoch 116/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1960 - mean_squared_error: 0.0696\n",
            "Epoch 116: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2046 - mean_squared_error: 0.0834 - val_loss: 0.2215 - val_mean_squared_error: 0.0781\n",
            "Epoch 117/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1962 - mean_squared_error: 0.0732\n",
            "Epoch 117: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1975 - mean_squared_error: 0.0747 - val_loss: 0.1616 - val_mean_squared_error: 0.0553\n",
            "Epoch 118/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1946 - mean_squared_error: 0.0730\n",
            "Epoch 118: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1951 - mean_squared_error: 0.0721 - val_loss: 0.1990 - val_mean_squared_error: 0.0711\n",
            "Epoch 119/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2096 - mean_squared_error: 0.0872\n",
            "Epoch 119: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2083 - mean_squared_error: 0.0870 - val_loss: 0.2150 - val_mean_squared_error: 0.0855\n",
            "Epoch 120/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2111 - mean_squared_error: 0.0879\n",
            "Epoch 120: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2024 - mean_squared_error: 0.0800 - val_loss: 0.2200 - val_mean_squared_error: 0.0882\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2125 - mean_squared_error: 0.0945\n",
            "Epoch 121: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2125 - mean_squared_error: 0.0945 - val_loss: 0.2574 - val_mean_squared_error: 0.0908\n",
            "Epoch 122/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2275 - mean_squared_error: 0.1046\n",
            "Epoch 122: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2297 - mean_squared_error: 0.1037 - val_loss: 0.2105 - val_mean_squared_error: 0.0753\n",
            "Epoch 123/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.0868\n",
            "Epoch 123: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2083 - mean_squared_error: 0.0796 - val_loss: 0.1882 - val_mean_squared_error: 0.0619\n",
            "Epoch 124/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1884 - mean_squared_error: 0.0699\n",
            "Epoch 124: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1950 - mean_squared_error: 0.0744 - val_loss: 0.1853 - val_mean_squared_error: 0.0676\n",
            "Epoch 125/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2015 - mean_squared_error: 0.0808\n",
            "Epoch 125: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1993 - mean_squared_error: 0.0786 - val_loss: 0.1599 - val_mean_squared_error: 0.0531\n",
            "Epoch 126/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1963 - mean_squared_error: 0.0818\n",
            "Epoch 126: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1952 - mean_squared_error: 0.0806 - val_loss: 0.2016 - val_mean_squared_error: 0.0650\n",
            "Epoch 127/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2105 - mean_squared_error: 0.0899\n",
            "Epoch 127: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1997 - mean_squared_error: 0.0810 - val_loss: 0.1659 - val_mean_squared_error: 0.0578\n",
            "Epoch 128/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2049 - mean_squared_error: 0.0822\n",
            "Epoch 128: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2003 - mean_squared_error: 0.0768 - val_loss: 0.1773 - val_mean_squared_error: 0.0556\n",
            "Epoch 129/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1874 - mean_squared_error: 0.0692\n",
            "Epoch 129: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1908 - mean_squared_error: 0.0733 - val_loss: 0.1784 - val_mean_squared_error: 0.0557\n",
            "Epoch 130/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1895 - mean_squared_error: 0.0717\n",
            "Epoch 130: val_loss did not improve from 0.15770\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1884 - mean_squared_error: 0.0706 - val_loss: 0.1951 - val_mean_squared_error: 0.0781\n",
            "Epoch 131/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2016 - mean_squared_error: 0.0783\n",
            "Epoch 131: val_loss improved from 0.15770 to 0.15694, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.1908 - mean_squared_error: 0.0728 - val_loss: 0.1569 - val_mean_squared_error: 0.0497\n",
            "Epoch 132/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1853 - mean_squared_error: 0.0699\n",
            "Epoch 132: val_loss did not improve from 0.15694\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1877 - mean_squared_error: 0.0708 - val_loss: 0.1627 - val_mean_squared_error: 0.0523\n",
            "Epoch 133/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2008 - mean_squared_error: 0.0810\n",
            "Epoch 133: val_loss improved from 0.15694 to 0.15437, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.1884 - mean_squared_error: 0.0728 - val_loss: 0.1544 - val_mean_squared_error: 0.0492\n",
            "Epoch 134/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1751 - mean_squared_error: 0.0549\n",
            "Epoch 134: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1873 - mean_squared_error: 0.0697 - val_loss: 0.1616 - val_mean_squared_error: 0.0507\n",
            "Epoch 135/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1883 - mean_squared_error: 0.0727\n",
            "Epoch 135: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1865 - mean_squared_error: 0.0710 - val_loss: 0.1796 - val_mean_squared_error: 0.0559\n",
            "Epoch 136/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1829 - mean_squared_error: 0.0665\n",
            "Epoch 136: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1896 - mean_squared_error: 0.0731 - val_loss: 0.2587 - val_mean_squared_error: 0.0995\n",
            "Epoch 137/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2433 - mean_squared_error: 0.1292\n",
            "Epoch 137: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2212 - mean_squared_error: 0.0978 - val_loss: 0.1608 - val_mean_squared_error: 0.0518\n",
            "Epoch 138/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1920 - mean_squared_error: 0.0725\n",
            "Epoch 138: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1935 - mean_squared_error: 0.0739 - val_loss: 0.1563 - val_mean_squared_error: 0.0492\n",
            "Epoch 139/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1247 - mean_squared_error: 0.0295\n",
            "Epoch 139: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1830 - mean_squared_error: 0.0688 - val_loss: 0.1754 - val_mean_squared_error: 0.0603\n",
            "Epoch 140/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2434 - mean_squared_error: 0.1193\n",
            "Epoch 140: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1831 - mean_squared_error: 0.0686 - val_loss: 0.1578 - val_mean_squared_error: 0.0507\n",
            "Epoch 141/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1584 - mean_squared_error: 0.0482\n",
            "Epoch 141: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1856 - mean_squared_error: 0.0709 - val_loss: 0.1993 - val_mean_squared_error: 0.0868\n",
            "Epoch 142/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1889 - mean_squared_error: 0.0797\n",
            "Epoch 142: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2024 - mean_squared_error: 0.0835 - val_loss: 0.1692 - val_mean_squared_error: 0.0583\n",
            "Epoch 143/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1417 - mean_squared_error: 0.0376\n",
            "Epoch 143: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2035 - mean_squared_error: 0.0855 - val_loss: 0.1937 - val_mean_squared_error: 0.0629\n",
            "Epoch 144/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1783 - mean_squared_error: 0.0604\n",
            "Epoch 144: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1839 - mean_squared_error: 0.0719 - val_loss: 0.1569 - val_mean_squared_error: 0.0516\n",
            "Epoch 145/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1931 - mean_squared_error: 0.0639\n",
            "Epoch 145: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1799 - mean_squared_error: 0.0690 - val_loss: 0.1546 - val_mean_squared_error: 0.0504\n",
            "Epoch 146/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1435 - mean_squared_error: 0.0416\n",
            "Epoch 146: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1789 - mean_squared_error: 0.0683 - val_loss: 0.1705 - val_mean_squared_error: 0.0573\n",
            "Epoch 147/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1897 - mean_squared_error: 0.0743\n",
            "Epoch 147: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1897 - mean_squared_error: 0.0743 - val_loss: 0.1726 - val_mean_squared_error: 0.0609\n",
            "Epoch 148/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1798 - mean_squared_error: 0.0676\n",
            "Epoch 148: val_loss did not improve from 0.15437\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1798 - mean_squared_error: 0.0676 - val_loss: 0.1862 - val_mean_squared_error: 0.0605\n",
            "Epoch 149/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1622 - mean_squared_error: 0.0411\n",
            "Epoch 149: val_loss improved from 0.15437 to 0.13845, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1939 - mean_squared_error: 0.0736 - val_loss: 0.1385 - val_mean_squared_error: 0.0469\n",
            "Epoch 150/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1846 - mean_squared_error: 0.0532\n",
            "Epoch 150: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1996 - mean_squared_error: 0.0826 - val_loss: 0.1950 - val_mean_squared_error: 0.0805\n",
            "Epoch 151/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1530 - mean_squared_error: 0.0403\n",
            "Epoch 151: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1914 - mean_squared_error: 0.0723 - val_loss: 0.1895 - val_mean_squared_error: 0.0730\n",
            "Epoch 152/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2211 - mean_squared_error: 0.0997\n",
            "Epoch 152: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1833 - mean_squared_error: 0.0703 - val_loss: 0.1592 - val_mean_squared_error: 0.0538\n",
            "Epoch 153/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1868 - mean_squared_error: 0.0708\n",
            "Epoch 153: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1824 - mean_squared_error: 0.0700 - val_loss: 0.1421 - val_mean_squared_error: 0.0478\n",
            "Epoch 154/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1722 - mean_squared_error: 0.0543\n",
            "Epoch 154: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1758 - mean_squared_error: 0.0664 - val_loss: 0.1487 - val_mean_squared_error: 0.0482\n",
            "Epoch 155/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1010 - mean_squared_error: 0.0183\n",
            "Epoch 155: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1786 - mean_squared_error: 0.0659 - val_loss: 0.1539 - val_mean_squared_error: 0.0523\n",
            "Epoch 156/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1285 - mean_squared_error: 0.0378\n",
            "Epoch 156: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1752 - mean_squared_error: 0.0674 - val_loss: 0.1675 - val_mean_squared_error: 0.0562\n",
            "Epoch 157/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1773 - mean_squared_error: 0.0654\n",
            "Epoch 157: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1773 - mean_squared_error: 0.0654 - val_loss: 0.1864 - val_mean_squared_error: 0.0597\n",
            "Epoch 158/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1801 - mean_squared_error: 0.0538\n",
            "Epoch 158: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1954 - mean_squared_error: 0.0755 - val_loss: 0.1740 - val_mean_squared_error: 0.0596\n",
            "Epoch 159/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1911 - mean_squared_error: 0.0740\n",
            "Epoch 159: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1911 - mean_squared_error: 0.0740 - val_loss: 0.1642 - val_mean_squared_error: 0.0551\n",
            "Epoch 160/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1787 - mean_squared_error: 0.0655\n",
            "Epoch 160: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1787 - mean_squared_error: 0.0655 - val_loss: 0.1710 - val_mean_squared_error: 0.0547\n",
            "Epoch 161/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1612 - mean_squared_error: 0.0503\n",
            "Epoch 161: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1756 - mean_squared_error: 0.0649 - val_loss: 0.1636 - val_mean_squared_error: 0.0541\n",
            "Epoch 162/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1212 - mean_squared_error: 0.0226\n",
            "Epoch 162: val_loss did not improve from 0.13845\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1705 - mean_squared_error: 0.0635 - val_loss: 0.1475 - val_mean_squared_error: 0.0482\n",
            "Epoch 163/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1403 - mean_squared_error: 0.0406\n",
            "Epoch 163: val_loss improved from 0.13845 to 0.13701, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.1730 - mean_squared_error: 0.0649 - val_loss: 0.1370 - val_mean_squared_error: 0.0435\n",
            "Epoch 164/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1239 - mean_squared_error: 0.0297\n",
            "Epoch 164: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1868 - mean_squared_error: 0.0717 - val_loss: 0.1788 - val_mean_squared_error: 0.0545\n",
            "Epoch 165/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3089 - mean_squared_error: 0.1832\n",
            "Epoch 165: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1834 - mean_squared_error: 0.0705 - val_loss: 0.1778 - val_mean_squared_error: 0.0597\n",
            "Epoch 166/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2055 - mean_squared_error: 0.0820\n",
            "Epoch 166: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2050 - mean_squared_error: 0.0814 - val_loss: 0.1710 - val_mean_squared_error: 0.0574\n",
            "Epoch 167/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1536 - mean_squared_error: 0.0514\n",
            "Epoch 167: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1753 - mean_squared_error: 0.0631 - val_loss: 0.1692 - val_mean_squared_error: 0.0602\n",
            "Epoch 168/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2071 - mean_squared_error: 0.0675\n",
            "Epoch 168: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1815 - mean_squared_error: 0.0676 - val_loss: 0.1552 - val_mean_squared_error: 0.0504\n",
            "Epoch 169/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1261 - mean_squared_error: 0.0354\n",
            "Epoch 169: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1759 - mean_squared_error: 0.0664 - val_loss: 0.1731 - val_mean_squared_error: 0.0565\n",
            "Epoch 170/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1706 - mean_squared_error: 0.0446\n",
            "Epoch 170: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1770 - mean_squared_error: 0.0650 - val_loss: 0.2186 - val_mean_squared_error: 0.0746\n",
            "Epoch 171/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2115 - mean_squared_error: 0.0936\n",
            "Epoch 171: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2643 - mean_squared_error: 0.1433 - val_loss: 0.2392 - val_mean_squared_error: 0.1007\n",
            "Epoch 172/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.0772\n",
            "Epoch 172: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2087 - mean_squared_error: 0.0876 - val_loss: 0.1903 - val_mean_squared_error: 0.0609\n",
            "Epoch 173/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1526 - mean_squared_error: 0.0374\n",
            "Epoch 173: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1918 - mean_squared_error: 0.0743 - val_loss: 0.2347 - val_mean_squared_error: 0.0789\n",
            "Epoch 174/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1812 - mean_squared_error: 0.0612\n",
            "Epoch 174: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1948 - mean_squared_error: 0.0794 - val_loss: 0.1722 - val_mean_squared_error: 0.0711\n",
            "Epoch 175/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2131 - mean_squared_error: 0.0917\n",
            "Epoch 175: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1882 - mean_squared_error: 0.0709 - val_loss: 0.1490 - val_mean_squared_error: 0.0476\n",
            "Epoch 176/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2183 - mean_squared_error: 0.0748\n",
            "Epoch 176: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1817 - mean_squared_error: 0.0708 - val_loss: 0.1521 - val_mean_squared_error: 0.0495\n",
            "Epoch 177/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1719 - mean_squared_error: 0.0639\n",
            "Epoch 177: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1719 - mean_squared_error: 0.0639 - val_loss: 0.1380 - val_mean_squared_error: 0.0452\n",
            "Epoch 178/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1782 - mean_squared_error: 0.0627\n",
            "Epoch 178: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1687 - mean_squared_error: 0.0614 - val_loss: 0.1487 - val_mean_squared_error: 0.0492\n",
            "Epoch 179/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1671 - mean_squared_error: 0.0532\n",
            "Epoch 179: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1676 - mean_squared_error: 0.0613 - val_loss: 0.1458 - val_mean_squared_error: 0.0495\n",
            "Epoch 180/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1222 - mean_squared_error: 0.0289\n",
            "Epoch 180: val_loss did not improve from 0.13701\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1656 - mean_squared_error: 0.0602 - val_loss: 0.1472 - val_mean_squared_error: 0.0488\n",
            "Epoch 181/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2343 - mean_squared_error: 0.1306\n",
            "Epoch 181: val_loss improved from 0.13701 to 0.13434, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.1730 - mean_squared_error: 0.0622 - val_loss: 0.1343 - val_mean_squared_error: 0.0432\n",
            "Epoch 182/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1792 - mean_squared_error: 0.0664\n",
            "Epoch 182: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1792 - mean_squared_error: 0.0664 - val_loss: 0.2189 - val_mean_squared_error: 0.0711\n",
            "Epoch 183/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2113 - mean_squared_error: 0.1001\n",
            "Epoch 183: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1872 - mean_squared_error: 0.0730 - val_loss: 0.1438 - val_mean_squared_error: 0.0474\n",
            "Epoch 184/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1455 - mean_squared_error: 0.0399\n",
            "Epoch 184: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1877 - mean_squared_error: 0.0691 - val_loss: 0.1880 - val_mean_squared_error: 0.0807\n",
            "Epoch 185/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2568 - mean_squared_error: 0.1458\n",
            "Epoch 185: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2036 - mean_squared_error: 0.0778 - val_loss: 0.1985 - val_mean_squared_error: 0.1077\n",
            "Epoch 186/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0805\n",
            "Epoch 186: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2007 - mean_squared_error: 0.0805 - val_loss: 0.1434 - val_mean_squared_error: 0.0498\n",
            "Epoch 187/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1764 - mean_squared_error: 0.0623\n",
            "Epoch 187: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1764 - mean_squared_error: 0.0623 - val_loss: 0.1375 - val_mean_squared_error: 0.0447\n",
            "Epoch 188/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1305 - mean_squared_error: 0.0354\n",
            "Epoch 188: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1682 - mean_squared_error: 0.0604 - val_loss: 0.1530 - val_mean_squared_error: 0.0483\n",
            "Epoch 189/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1495 - mean_squared_error: 0.0466\n",
            "Epoch 189: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1703 - mean_squared_error: 0.0605 - val_loss: 0.1727 - val_mean_squared_error: 0.0824\n",
            "Epoch 190/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1843 - mean_squared_error: 0.0809\n",
            "Epoch 190: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1794 - mean_squared_error: 0.0654 - val_loss: 0.1370 - val_mean_squared_error: 0.0512\n",
            "Epoch 191/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1564 - mean_squared_error: 0.0427\n",
            "Epoch 191: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1694 - mean_squared_error: 0.0635 - val_loss: 0.1485 - val_mean_squared_error: 0.0470\n",
            "Epoch 192/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1068 - mean_squared_error: 0.0239\n",
            "Epoch 192: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1745 - mean_squared_error: 0.0667 - val_loss: 0.1746 - val_mean_squared_error: 0.0582\n",
            "Epoch 193/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1752 - mean_squared_error: 0.0472\n",
            "Epoch 193: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1720 - mean_squared_error: 0.0593 - val_loss: 0.1433 - val_mean_squared_error: 0.0492\n",
            "Epoch 194/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1206 - mean_squared_error: 0.0219\n",
            "Epoch 194: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1770 - mean_squared_error: 0.0655 - val_loss: 0.1550 - val_mean_squared_error: 0.0501\n",
            "Epoch 195/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1927 - mean_squared_error: 0.0746\n",
            "Epoch 195: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1927 - mean_squared_error: 0.0746 - val_loss: 0.2403 - val_mean_squared_error: 0.0843\n",
            "Epoch 196/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2334 - mean_squared_error: 0.1112\n",
            "Epoch 196: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2130 - mean_squared_error: 0.0788 - val_loss: 0.2062 - val_mean_squared_error: 0.1191\n",
            "Epoch 197/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2483 - mean_squared_error: 0.1277\n",
            "Epoch 197: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2050 - mean_squared_error: 0.0864 - val_loss: 0.1409 - val_mean_squared_error: 0.0458\n",
            "Epoch 198/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1905 - mean_squared_error: 0.0714\n",
            "Epoch 198: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1905 - mean_squared_error: 0.0714 - val_loss: 0.2137 - val_mean_squared_error: 0.0701\n",
            "Epoch 199/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1905 - mean_squared_error: 0.0555\n",
            "Epoch 199: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1840 - mean_squared_error: 0.0639 - val_loss: 0.1548 - val_mean_squared_error: 0.0577\n",
            "Epoch 200/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1721 - mean_squared_error: 0.0591\n",
            "Epoch 200: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1721 - mean_squared_error: 0.0591 - val_loss: 0.1391 - val_mean_squared_error: 0.0506\n",
            "Epoch 201/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1692 - mean_squared_error: 0.0595\n",
            "Epoch 201: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1692 - mean_squared_error: 0.0595 - val_loss: 0.1446 - val_mean_squared_error: 0.0448\n",
            "Epoch 202/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1259 - mean_squared_error: 0.0270\n",
            "Epoch 202: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1689 - mean_squared_error: 0.0589 - val_loss: 0.1487 - val_mean_squared_error: 0.0489\n",
            "Epoch 203/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1122 - mean_squared_error: 0.0256\n",
            "Epoch 203: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1625 - mean_squared_error: 0.0572 - val_loss: 0.1460 - val_mean_squared_error: 0.0503\n",
            "Epoch 204/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1694 - mean_squared_error: 0.0578\n",
            "Epoch 204: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1696 - mean_squared_error: 0.0574 - val_loss: 0.1603 - val_mean_squared_error: 0.0585\n",
            "Epoch 205/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1184 - mean_squared_error: 0.0386\n",
            "Epoch 205: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1710 - mean_squared_error: 0.0615 - val_loss: 0.1758 - val_mean_squared_error: 0.0555\n",
            "Epoch 206/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1929 - mean_squared_error: 0.0524\n",
            "Epoch 206: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1710 - mean_squared_error: 0.0613 - val_loss: 0.1556 - val_mean_squared_error: 0.0489\n",
            "Epoch 207/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1789 - mean_squared_error: 0.0593\n",
            "Epoch 207: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1746 - mean_squared_error: 0.0605 - val_loss: 0.1355 - val_mean_squared_error: 0.0459\n",
            "Epoch 208/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1725 - mean_squared_error: 0.0584\n",
            "Epoch 208: val_loss did not improve from 0.13434\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1725 - mean_squared_error: 0.0584 - val_loss: 0.1461 - val_mean_squared_error: 0.0515\n",
            "Epoch 209/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1437 - mean_squared_error: 0.0457\n",
            "Epoch 209: val_loss improved from 0.13434 to 0.12875, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1824 - mean_squared_error: 0.0630 - val_loss: 0.1287 - val_mean_squared_error: 0.0382\n",
            "Epoch 210/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1794 - mean_squared_error: 0.0636\n",
            "Epoch 210: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1794 - mean_squared_error: 0.0636 - val_loss: 0.1682 - val_mean_squared_error: 0.0523\n",
            "Epoch 211/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1878 - mean_squared_error: 0.0684\n",
            "Epoch 211: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1878 - mean_squared_error: 0.0684 - val_loss: 0.1544 - val_mean_squared_error: 0.0498\n",
            "Epoch 212/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1292 - mean_squared_error: 0.0334\n",
            "Epoch 212: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1718 - mean_squared_error: 0.0588 - val_loss: 0.1790 - val_mean_squared_error: 0.0708\n",
            "Epoch 213/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2057 - mean_squared_error: 0.0699\n",
            "Epoch 213: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1781 - mean_squared_error: 0.0633 - val_loss: 0.1635 - val_mean_squared_error: 0.0512\n",
            "Epoch 214/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1808 - mean_squared_error: 0.0764\n",
            "Epoch 214: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1618 - mean_squared_error: 0.0545 - val_loss: 0.1585 - val_mean_squared_error: 0.0476\n",
            "Epoch 215/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1487 - mean_squared_error: 0.0400\n",
            "Epoch 215: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1677 - mean_squared_error: 0.0592 - val_loss: 0.1598 - val_mean_squared_error: 0.0595\n",
            "Epoch 216/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1592 - mean_squared_error: 0.0547\n",
            "Epoch 216: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1659 - mean_squared_error: 0.0561 - val_loss: 0.1445 - val_mean_squared_error: 0.0441\n",
            "Epoch 217/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1709 - mean_squared_error: 0.0604\n",
            "Epoch 217: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1709 - mean_squared_error: 0.0604 - val_loss: 0.1623 - val_mean_squared_error: 0.0524\n",
            "Epoch 218/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1438 - mean_squared_error: 0.0384\n",
            "Epoch 218: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1715 - mean_squared_error: 0.0588 - val_loss: 0.1838 - val_mean_squared_error: 0.0577\n",
            "Epoch 219/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1773 - mean_squared_error: 0.0620\n",
            "Epoch 219: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1803 - mean_squared_error: 0.0623 - val_loss: 0.1319 - val_mean_squared_error: 0.0439\n",
            "Epoch 220/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1971 - mean_squared_error: 0.0838\n",
            "Epoch 220: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1608 - mean_squared_error: 0.0540 - val_loss: 0.1410 - val_mean_squared_error: 0.0484\n",
            "Epoch 221/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1285 - mean_squared_error: 0.0353\n",
            "Epoch 221: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1738 - mean_squared_error: 0.0585 - val_loss: 0.1561 - val_mean_squared_error: 0.0479\n",
            "Epoch 222/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1610 - mean_squared_error: 0.0550\n",
            "Epoch 222: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1610 - mean_squared_error: 0.0550 - val_loss: 0.1573 - val_mean_squared_error: 0.0461\n",
            "Epoch 223/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2243 - mean_squared_error: 0.1204\n",
            "Epoch 223: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1585 - mean_squared_error: 0.0541 - val_loss: 0.1504 - val_mean_squared_error: 0.0466\n",
            "Epoch 224/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1527 - mean_squared_error: 0.0527\n",
            "Epoch 224: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1527 - mean_squared_error: 0.0527 - val_loss: 0.1470 - val_mean_squared_error: 0.0459\n",
            "Epoch 225/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1611 - mean_squared_error: 0.0569\n",
            "Epoch 225: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1645 - mean_squared_error: 0.0575 - val_loss: 0.1739 - val_mean_squared_error: 0.0577\n",
            "Epoch 226/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1646 - mean_squared_error: 0.0538\n",
            "Epoch 226: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1640 - mean_squared_error: 0.0557 - val_loss: 0.1738 - val_mean_squared_error: 0.0509\n",
            "Epoch 227/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1745 - mean_squared_error: 0.0652\n",
            "Epoch 227: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1719 - mean_squared_error: 0.0626 - val_loss: 0.1636 - val_mean_squared_error: 0.0563\n",
            "Epoch 228/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1781 - mean_squared_error: 0.0625\n",
            "Epoch 228: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1759 - mean_squared_error: 0.0615 - val_loss: 0.1585 - val_mean_squared_error: 0.0516\n",
            "Epoch 229/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1788 - mean_squared_error: 0.0667\n",
            "Epoch 229: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1790 - mean_squared_error: 0.0656 - val_loss: 0.1524 - val_mean_squared_error: 0.0479\n",
            "Epoch 230/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1710 - mean_squared_error: 0.0595\n",
            "Epoch 230: val_loss did not improve from 0.12875\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1710 - mean_squared_error: 0.0595 - val_loss: 0.1686 - val_mean_squared_error: 0.0525\n",
            "Epoch 231/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1683 - mean_squared_error: 0.0565\n",
            "Epoch 231: val_loss improved from 0.12875 to 0.12100, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1650 - mean_squared_error: 0.0562 - val_loss: 0.1210 - val_mean_squared_error: 0.0403\n",
            "Epoch 232/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1476 - mean_squared_error: 0.0453\n",
            "Epoch 232: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1593 - mean_squared_error: 0.0532 - val_loss: 0.1820 - val_mean_squared_error: 0.0518\n",
            "Epoch 233/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1760 - mean_squared_error: 0.0646\n",
            "Epoch 233: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1643 - mean_squared_error: 0.0560 - val_loss: 0.1225 - val_mean_squared_error: 0.0427\n",
            "Epoch 234/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1737 - mean_squared_error: 0.0596\n",
            "Epoch 234: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1763 - mean_squared_error: 0.0599 - val_loss: 0.1232 - val_mean_squared_error: 0.0415\n",
            "Epoch 235/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1642 - mean_squared_error: 0.0570\n",
            "Epoch 235: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.1640 - mean_squared_error: 0.0565 - val_loss: 0.1373 - val_mean_squared_error: 0.0404\n",
            "Epoch 236/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1732 - mean_squared_error: 0.0589\n",
            "Epoch 236: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1715 - mean_squared_error: 0.0578 - val_loss: 0.1428 - val_mean_squared_error: 0.0474\n",
            "Epoch 237/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1589 - mean_squared_error: 0.0564\n",
            "Epoch 237: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1583 - mean_squared_error: 0.0551 - val_loss: 0.1742 - val_mean_squared_error: 0.0516\n",
            "Epoch 238/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1579 - mean_squared_error: 0.0533\n",
            "Epoch 238: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1618 - mean_squared_error: 0.0554 - val_loss: 0.1488 - val_mean_squared_error: 0.0465\n",
            "Epoch 239/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1863 - mean_squared_error: 0.0657\n",
            "Epoch 239: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1920 - mean_squared_error: 0.0721 - val_loss: 0.1715 - val_mean_squared_error: 0.0566\n",
            "Epoch 240/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1798 - mean_squared_error: 0.0636\n",
            "Epoch 240: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.1751 - mean_squared_error: 0.0591 - val_loss: 0.1442 - val_mean_squared_error: 0.0510\n",
            "Epoch 241/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1644 - mean_squared_error: 0.0548\n",
            "Epoch 241: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1626 - mean_squared_error: 0.0539 - val_loss: 0.1928 - val_mean_squared_error: 0.0567\n",
            "Epoch 242/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1555 - mean_squared_error: 0.0526\n",
            "Epoch 242: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1616 - mean_squared_error: 0.0556 - val_loss: 0.1390 - val_mean_squared_error: 0.0443\n",
            "Epoch 243/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1500 - mean_squared_error: 0.0471\n",
            "Epoch 243: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1564 - mean_squared_error: 0.0519 - val_loss: 0.1454 - val_mean_squared_error: 0.0467\n",
            "Epoch 244/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1544 - mean_squared_error: 0.0501\n",
            "Epoch 244: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1611 - mean_squared_error: 0.0529 - val_loss: 0.1704 - val_mean_squared_error: 0.0513\n",
            "Epoch 245/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1812 - mean_squared_error: 0.0618\n",
            "Epoch 245: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1797 - mean_squared_error: 0.0607 - val_loss: 0.1808 - val_mean_squared_error: 0.0575\n",
            "Epoch 246/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1898 - mean_squared_error: 0.0687\n",
            "Epoch 246: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1895 - mean_squared_error: 0.0672 - val_loss: 0.1769 - val_mean_squared_error: 0.0607\n",
            "Epoch 247/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1706 - mean_squared_error: 0.0582\n",
            "Epoch 247: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1628 - mean_squared_error: 0.0530 - val_loss: 0.1410 - val_mean_squared_error: 0.0438\n",
            "Epoch 248/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1617 - mean_squared_error: 0.0562\n",
            "Epoch 248: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1583 - mean_squared_error: 0.0530 - val_loss: 0.1594 - val_mean_squared_error: 0.0493\n",
            "Epoch 249/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1591 - mean_squared_error: 0.0485\n",
            "Epoch 249: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1587 - mean_squared_error: 0.0527 - val_loss: 0.1523 - val_mean_squared_error: 0.0470\n",
            "Epoch 250/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1519 - mean_squared_error: 0.0499\n",
            "Epoch 250: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1534 - mean_squared_error: 0.0511 - val_loss: 0.1516 - val_mean_squared_error: 0.0468\n",
            "Epoch 251/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1599 - mean_squared_error: 0.0562\n",
            "Epoch 251: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1529 - mean_squared_error: 0.0503 - val_loss: 0.1782 - val_mean_squared_error: 0.0563\n",
            "Epoch 252/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1541 - mean_squared_error: 0.0460\n",
            "Epoch 252: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1564 - mean_squared_error: 0.0511 - val_loss: 0.1392 - val_mean_squared_error: 0.0471\n",
            "Epoch 253/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1606 - mean_squared_error: 0.0563\n",
            "Epoch 253: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1597 - mean_squared_error: 0.0535 - val_loss: 0.1402 - val_mean_squared_error: 0.0433\n",
            "Epoch 254/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1652 - mean_squared_error: 0.0587\n",
            "Epoch 254: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1590 - mean_squared_error: 0.0542 - val_loss: 0.1686 - val_mean_squared_error: 0.0560\n",
            "Epoch 255/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1576 - mean_squared_error: 0.0559\n",
            "Epoch 255: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1630 - mean_squared_error: 0.0565 - val_loss: 0.1726 - val_mean_squared_error: 0.0531\n",
            "Epoch 256/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1460 - mean_squared_error: 0.0417\n",
            "Epoch 256: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1567 - mean_squared_error: 0.0518 - val_loss: 0.1607 - val_mean_squared_error: 0.0529\n",
            "Epoch 257/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1656 - mean_squared_error: 0.0555\n",
            "Epoch 257: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1627 - mean_squared_error: 0.0538 - val_loss: 0.1378 - val_mean_squared_error: 0.0441\n",
            "Epoch 258/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1623 - mean_squared_error: 0.0568\n",
            "Epoch 258: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1615 - mean_squared_error: 0.0546 - val_loss: 0.1413 - val_mean_squared_error: 0.0412\n",
            "Epoch 259/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1628 - mean_squared_error: 0.0563\n",
            "Epoch 259: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1550 - mean_squared_error: 0.0515 - val_loss: 0.1469 - val_mean_squared_error: 0.0439\n",
            "Epoch 260/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1610 - mean_squared_error: 0.0546\n",
            "Epoch 260: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1566 - mean_squared_error: 0.0526 - val_loss: 0.1376 - val_mean_squared_error: 0.0440\n",
            "Epoch 261/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.0543\n",
            "Epoch 261: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1591 - mean_squared_error: 0.0515 - val_loss: 0.1264 - val_mean_squared_error: 0.0403\n",
            "Epoch 262/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1582 - mean_squared_error: 0.0533\n",
            "Epoch 262: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1575 - mean_squared_error: 0.0526 - val_loss: 0.1292 - val_mean_squared_error: 0.0400\n",
            "Epoch 263/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1656 - mean_squared_error: 0.0582\n",
            "Epoch 263: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1644 - mean_squared_error: 0.0567 - val_loss: 0.1590 - val_mean_squared_error: 0.0488\n",
            "Epoch 264/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1495 - mean_squared_error: 0.0434\n",
            "Epoch 264: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1557 - mean_squared_error: 0.0519 - val_loss: 0.1614 - val_mean_squared_error: 0.0515\n",
            "Epoch 265/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1850 - mean_squared_error: 0.0893\n",
            "Epoch 265: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1490 - mean_squared_error: 0.0506 - val_loss: 0.1444 - val_mean_squared_error: 0.0461\n",
            "Epoch 266/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1191 - mean_squared_error: 0.0237\n",
            "Epoch 266: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1598 - mean_squared_error: 0.0522 - val_loss: 0.1365 - val_mean_squared_error: 0.0438\n",
            "Epoch 267/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1508 - mean_squared_error: 0.0482\n",
            "Epoch 267: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1508 - mean_squared_error: 0.0482 - val_loss: 0.1543 - val_mean_squared_error: 0.0487\n",
            "Epoch 268/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1171 - mean_squared_error: 0.0261\n",
            "Epoch 268: val_loss did not improve from 0.12100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1553 - mean_squared_error: 0.0511 - val_loss: 0.1467 - val_mean_squared_error: 0.0435\n",
            "Epoch 269/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1110 - mean_squared_error: 0.0271\n",
            "Epoch 269: val_loss improved from 0.12100 to 0.11328, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1586 - mean_squared_error: 0.0510 - val_loss: 0.1133 - val_mean_squared_error: 0.0381\n",
            "Epoch 270/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1596 - mean_squared_error: 0.0475\n",
            "Epoch 270: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1569 - mean_squared_error: 0.0504 - val_loss: 0.1254 - val_mean_squared_error: 0.0391\n",
            "Epoch 271/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1372 - mean_squared_error: 0.0377\n",
            "Epoch 271: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1525 - mean_squared_error: 0.0489 - val_loss: 0.1667 - val_mean_squared_error: 0.0528\n",
            "Epoch 272/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1656 - mean_squared_error: 0.0522\n",
            "Epoch 272: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1567 - mean_squared_error: 0.0528 - val_loss: 0.1200 - val_mean_squared_error: 0.0379\n",
            "Epoch 273/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1720 - mean_squared_error: 0.0500\n",
            "Epoch 273: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1535 - mean_squared_error: 0.0502 - val_loss: 0.1346 - val_mean_squared_error: 0.0416\n",
            "Epoch 274/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1298 - mean_squared_error: 0.0370\n",
            "Epoch 274: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1526 - mean_squared_error: 0.0501 - val_loss: 0.1448 - val_mean_squared_error: 0.0446\n",
            "Epoch 275/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2009 - mean_squared_error: 0.0884\n",
            "Epoch 275: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1558 - mean_squared_error: 0.0503 - val_loss: 0.1303 - val_mean_squared_error: 0.0386\n",
            "Epoch 276/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1429 - mean_squared_error: 0.0445\n",
            "Epoch 276: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1472 - mean_squared_error: 0.0495 - val_loss: 0.1529 - val_mean_squared_error: 0.0465\n",
            "Epoch 277/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1666 - mean_squared_error: 0.0572\n",
            "Epoch 277: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1666 - mean_squared_error: 0.0572 - val_loss: 0.1708 - val_mean_squared_error: 0.0507\n",
            "Epoch 278/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.0561\n",
            "Epoch 278: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1603 - mean_squared_error: 0.0543 - val_loss: 0.1855 - val_mean_squared_error: 0.0592\n",
            "Epoch 279/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1708 - mean_squared_error: 0.0525\n",
            "Epoch 279: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1599 - mean_squared_error: 0.0535 - val_loss: 0.1398 - val_mean_squared_error: 0.0442\n",
            "Epoch 280/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1599 - mean_squared_error: 0.0502\n",
            "Epoch 280: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1530 - mean_squared_error: 0.0493 - val_loss: 0.1676 - val_mean_squared_error: 0.0607\n",
            "Epoch 281/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1656 - mean_squared_error: 0.0528\n",
            "Epoch 281: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1705 - mean_squared_error: 0.0587 - val_loss: 0.1751 - val_mean_squared_error: 0.0533\n",
            "Epoch 282/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1517 - mean_squared_error: 0.0480\n",
            "Epoch 282: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1517 - mean_squared_error: 0.0480 - val_loss: 0.1861 - val_mean_squared_error: 0.0605\n",
            "Epoch 283/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1262 - mean_squared_error: 0.0273\n",
            "Epoch 283: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1682 - mean_squared_error: 0.0552 - val_loss: 0.1906 - val_mean_squared_error: 0.0600\n",
            "Epoch 284/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1706 - mean_squared_error: 0.0578\n",
            "Epoch 284: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1706 - mean_squared_error: 0.0578 - val_loss: 0.1759 - val_mean_squared_error: 0.0594\n",
            "Epoch 285/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1302 - mean_squared_error: 0.0460\n",
            "Epoch 285: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1693 - mean_squared_error: 0.0567 - val_loss: 0.1586 - val_mean_squared_error: 0.0528\n",
            "Epoch 286/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.0842 - mean_squared_error: 0.0158\n",
            "Epoch 286: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1674 - mean_squared_error: 0.0571 - val_loss: 0.1985 - val_mean_squared_error: 0.0650\n",
            "Epoch 287/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1301 - mean_squared_error: 0.0304\n",
            "Epoch 287: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1550 - mean_squared_error: 0.0507 - val_loss: 0.1479 - val_mean_squared_error: 0.0466\n",
            "Epoch 288/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1585 - mean_squared_error: 0.0510\n",
            "Epoch 288: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1585 - mean_squared_error: 0.0510 - val_loss: 0.1385 - val_mean_squared_error: 0.0443\n",
            "Epoch 289/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1396 - mean_squared_error: 0.0407\n",
            "Epoch 289: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1477 - mean_squared_error: 0.0483 - val_loss: 0.1638 - val_mean_squared_error: 0.0503\n",
            "Epoch 290/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1513 - mean_squared_error: 0.0506\n",
            "Epoch 290: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1477 - mean_squared_error: 0.0488 - val_loss: 0.1523 - val_mean_squared_error: 0.0461\n",
            "Epoch 291/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1245 - mean_squared_error: 0.0328\n",
            "Epoch 291: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1464 - mean_squared_error: 0.0477 - val_loss: 0.1470 - val_mean_squared_error: 0.0470\n",
            "Epoch 292/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.0968 - mean_squared_error: 0.0219\n",
            "Epoch 292: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1500 - mean_squared_error: 0.0475 - val_loss: 0.1420 - val_mean_squared_error: 0.0420\n",
            "Epoch 293/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1494 - mean_squared_error: 0.0472\n",
            "Epoch 293: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1496 - mean_squared_error: 0.0470 - val_loss: 0.1486 - val_mean_squared_error: 0.0446\n",
            "Epoch 294/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1256 - mean_squared_error: 0.0319\n",
            "Epoch 294: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1518 - mean_squared_error: 0.0498 - val_loss: 0.1814 - val_mean_squared_error: 0.0569\n",
            "Epoch 295/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1701 - mean_squared_error: 0.0714\n",
            "Epoch 295: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1507 - mean_squared_error: 0.0482 - val_loss: 0.1663 - val_mean_squared_error: 0.0505\n",
            "Epoch 296/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1765 - mean_squared_error: 0.0522\n",
            "Epoch 296: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1476 - mean_squared_error: 0.0460 - val_loss: 0.1540 - val_mean_squared_error: 0.0483\n",
            "Epoch 297/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1552 - mean_squared_error: 0.0509\n",
            "Epoch 297: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1566 - mean_squared_error: 0.0510 - val_loss: 0.1259 - val_mean_squared_error: 0.0400\n",
            "Epoch 298/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1295 - mean_squared_error: 0.0570\n",
            "Epoch 298: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1466 - mean_squared_error: 0.0475 - val_loss: 0.1570 - val_mean_squared_error: 0.0522\n",
            "Epoch 299/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1451 - mean_squared_error: 0.0713\n",
            "Epoch 299: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1746 - mean_squared_error: 0.0600 - val_loss: 0.2054 - val_mean_squared_error: 0.0651\n",
            "Epoch 300/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1377 - mean_squared_error: 0.0411\n",
            "Epoch 300: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1634 - mean_squared_error: 0.0551 - val_loss: 0.1573 - val_mean_squared_error: 0.0504\n",
            "Epoch 301/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2096 - mean_squared_error: 0.0800\n",
            "Epoch 301: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1465 - mean_squared_error: 0.0467 - val_loss: 0.1477 - val_mean_squared_error: 0.0471\n",
            "Epoch 302/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1323 - mean_squared_error: 0.0326\n",
            "Epoch 302: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1446 - mean_squared_error: 0.0460 - val_loss: 0.1565 - val_mean_squared_error: 0.0499\n",
            "Epoch 303/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1325 - mean_squared_error: 0.0369\n",
            "Epoch 303: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1446 - mean_squared_error: 0.0468 - val_loss: 0.1364 - val_mean_squared_error: 0.0469\n",
            "Epoch 304/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1813 - mean_squared_error: 0.0798\n",
            "Epoch 304: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1491 - mean_squared_error: 0.0465 - val_loss: 0.1445 - val_mean_squared_error: 0.0430\n",
            "Epoch 305/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1892 - mean_squared_error: 0.0930\n",
            "Epoch 305: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1567 - mean_squared_error: 0.0518 - val_loss: 0.1530 - val_mean_squared_error: 0.0474\n",
            "Epoch 306/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1318 - mean_squared_error: 0.0315\n",
            "Epoch 306: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1609 - mean_squared_error: 0.0541 - val_loss: 0.1867 - val_mean_squared_error: 0.0591\n",
            "Epoch 307/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1612 - mean_squared_error: 0.0541\n",
            "Epoch 307: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1654 - mean_squared_error: 0.0552 - val_loss: 0.1373 - val_mean_squared_error: 0.0430\n",
            "Epoch 308/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1772 - mean_squared_error: 0.0650\n",
            "Epoch 308: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1555 - mean_squared_error: 0.0511 - val_loss: 0.1479 - val_mean_squared_error: 0.0488\n",
            "Epoch 309/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1506 - mean_squared_error: 0.0469\n",
            "Epoch 309: val_loss did not improve from 0.11328\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1614 - mean_squared_error: 0.0526 - val_loss: 0.1671 - val_mean_squared_error: 0.0542\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "Cycle:  1\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "1/9 [==>...........................] - ETA: 17s - loss: 2.3216 - mean_squared_error: 5.9887\n",
            "Epoch 1: val_loss improved from inf to 1.81458, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 3s 83ms/step - loss: 2.0170 - mean_squared_error: 4.8515 - val_loss: 1.8146 - val_mean_squared_error: 4.0960\n",
            "Epoch 2/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.8077 - mean_squared_error: 3.7991\n",
            "Epoch 2: val_loss improved from 1.81458 to 1.50244, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 1.5817 - mean_squared_error: 3.1155 - val_loss: 1.5024 - val_mean_squared_error: 2.9515\n",
            "Epoch 3/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.4653 - mean_squared_error: 2.7035\n",
            "Epoch 3: val_loss improved from 1.50244 to 1.26788, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.2814 - mean_squared_error: 2.1337 - val_loss: 1.2679 - val_mean_squared_error: 2.2304\n",
            "Epoch 4/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.1204 - mean_squared_error: 1.8518\n",
            "Epoch 4: val_loss improved from 1.26788 to 1.00174, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.0340 - mean_squared_error: 1.5108 - val_loss: 1.0017 - val_mean_squared_error: 1.6132\n",
            "Epoch 5/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.9502 - mean_squared_error: 1.1534\n",
            "Epoch 5: val_loss improved from 1.00174 to 0.78081, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.8000 - mean_squared_error: 1.0303 - val_loss: 0.7808 - val_mean_squared_error: 1.1829\n",
            "Epoch 6/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6684 - mean_squared_error: 0.8139\n",
            "Epoch 6: val_loss did not improve from 0.78081\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.6684 - mean_squared_error: 0.8139 - val_loss: 0.7880 - val_mean_squared_error: 1.0286\n",
            "Epoch 7/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6344 - mean_squared_error: 0.7331\n",
            "Epoch 7: val_loss improved from 0.78081 to 0.75975, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.6344 - mean_squared_error: 0.7331 - val_loss: 0.7597 - val_mean_squared_error: 0.9449\n",
            "Epoch 8/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5171 - mean_squared_error: 0.4697\n",
            "Epoch 8: val_loss improved from 0.75975 to 0.72417, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.5819 - mean_squared_error: 0.6454 - val_loss: 0.7242 - val_mean_squared_error: 0.9036\n",
            "Epoch 9/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5964 - mean_squared_error: 0.7144\n",
            "Epoch 9: val_loss improved from 0.72417 to 0.70278, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.5544 - mean_squared_error: 0.5972 - val_loss: 0.7028 - val_mean_squared_error: 0.8191\n",
            "Epoch 10/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5311 - mean_squared_error: 0.5611\n",
            "Epoch 10: val_loss improved from 0.70278 to 0.68509, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.5311 - mean_squared_error: 0.5611 - val_loss: 0.6851 - val_mean_squared_error: 0.7554\n",
            "Epoch 11/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4546 - mean_squared_error: 0.4883\n",
            "Epoch 11: val_loss improved from 0.68509 to 0.66952, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.5141 - mean_squared_error: 0.5362 - val_loss: 0.6695 - val_mean_squared_error: 0.7292\n",
            "Epoch 12/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5764 - mean_squared_error: 0.7632\n",
            "Epoch 12: val_loss improved from 0.66952 to 0.65562, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.5051 - mean_squared_error: 0.5181 - val_loss: 0.6556 - val_mean_squared_error: 0.6973\n",
            "Epoch 13/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4607 - mean_squared_error: 0.3624\n",
            "Epoch 13: val_loss improved from 0.65562 to 0.64731, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.4938 - mean_squared_error: 0.5024 - val_loss: 0.6473 - val_mean_squared_error: 0.6748\n",
            "Epoch 14/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5313 - mean_squared_error: 0.4884\n",
            "Epoch 14: val_loss improved from 0.64731 to 0.64140, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.4847 - mean_squared_error: 0.4875 - val_loss: 0.6414 - val_mean_squared_error: 0.6765\n",
            "Epoch 15/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4848 - mean_squared_error: 0.4024\n",
            "Epoch 15: val_loss improved from 0.64140 to 0.62845, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.4782 - mean_squared_error: 0.4767 - val_loss: 0.6285 - val_mean_squared_error: 0.6469\n",
            "Epoch 16/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4693 - mean_squared_error: 0.4630\n",
            "Epoch 16: val_loss improved from 0.62845 to 0.62516, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.4693 - mean_squared_error: 0.4630 - val_loss: 0.6252 - val_mean_squared_error: 0.6385\n",
            "Epoch 17/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3949 - mean_squared_error: 0.2572\n",
            "Epoch 17: val_loss improved from 0.62516 to 0.61720, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4634 - mean_squared_error: 0.4530 - val_loss: 0.6172 - val_mean_squared_error: 0.6267\n",
            "Epoch 18/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.6716 - mean_squared_error: 0.7843\n",
            "Epoch 18: val_loss improved from 0.61720 to 0.60911, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4554 - mean_squared_error: 0.4393 - val_loss: 0.6091 - val_mean_squared_error: 0.6019\n",
            "Epoch 19/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3870 - mean_squared_error: 0.2658\n",
            "Epoch 19: val_loss improved from 0.60911 to 0.60028, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.4482 - mean_squared_error: 0.4302 - val_loss: 0.6003 - val_mean_squared_error: 0.5972\n",
            "Epoch 20/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3839 - mean_squared_error: 0.2296\n",
            "Epoch 20: val_loss improved from 0.60028 to 0.58846, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.4416 - mean_squared_error: 0.4194 - val_loss: 0.5885 - val_mean_squared_error: 0.5784\n",
            "Epoch 21/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4229 - mean_squared_error: 0.3744\n",
            "Epoch 21: val_loss improved from 0.58846 to 0.58464, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.4376 - mean_squared_error: 0.4107 - val_loss: 0.5846 - val_mean_squared_error: 0.5755\n",
            "Epoch 22/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.6119 - mean_squared_error: 0.9092\n",
            "Epoch 22: val_loss improved from 0.58464 to 0.56816, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4305 - mean_squared_error: 0.4015 - val_loss: 0.5682 - val_mean_squared_error: 0.5513\n",
            "Epoch 23/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4230 - mean_squared_error: 0.3914\n",
            "Epoch 23: val_loss improved from 0.56816 to 0.56293, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4230 - mean_squared_error: 0.3914 - val_loss: 0.5629 - val_mean_squared_error: 0.5427\n",
            "Epoch 24/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3058 - mean_squared_error: 0.1758\n",
            "Epoch 24: val_loss improved from 0.56293 to 0.55715, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4181 - mean_squared_error: 0.3835 - val_loss: 0.5572 - val_mean_squared_error: 0.5428\n",
            "Epoch 25/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4141 - mean_squared_error: 0.3772\n",
            "Epoch 25: val_loss improved from 0.55715 to 0.54826, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.4141 - mean_squared_error: 0.3772 - val_loss: 0.5483 - val_mean_squared_error: 0.5190\n",
            "Epoch 26/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4874 - mean_squared_error: 0.5126\n",
            "Epoch 26: val_loss improved from 0.54826 to 0.54678, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.4128 - mean_squared_error: 0.3714 - val_loss: 0.5468 - val_mean_squared_error: 0.5285\n",
            "Epoch 27/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4635 - mean_squared_error: 0.4377\n",
            "Epoch 27: val_loss improved from 0.54678 to 0.53698, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4038 - mean_squared_error: 0.3624 - val_loss: 0.5370 - val_mean_squared_error: 0.5025\n",
            "Epoch 28/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4229 - mean_squared_error: 0.4070\n",
            "Epoch 28: val_loss improved from 0.53698 to 0.52641, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.4048 - mean_squared_error: 0.3598 - val_loss: 0.5264 - val_mean_squared_error: 0.5024\n",
            "Epoch 29/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5315 - mean_squared_error: 0.4451\n",
            "Epoch 29: val_loss improved from 0.52641 to 0.52260, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3978 - mean_squared_error: 0.3538 - val_loss: 0.5226 - val_mean_squared_error: 0.4978\n",
            "Epoch 30/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2845 - mean_squared_error: 0.1220\n",
            "Epoch 30: val_loss did not improve from 0.52260\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3930 - mean_squared_error: 0.3466 - val_loss: 0.5246 - val_mean_squared_error: 0.4789\n",
            "Epoch 31/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3903 - mean_squared_error: 0.3397\n",
            "Epoch 31: val_loss improved from 0.52260 to 0.51918, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3903 - mean_squared_error: 0.3397 - val_loss: 0.5192 - val_mean_squared_error: 0.4852\n",
            "Epoch 32/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.4216 - mean_squared_error: 0.4005\n",
            "Epoch 32: val_loss improved from 0.51918 to 0.51501, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3900 - mean_squared_error: 0.3385 - val_loss: 0.5150 - val_mean_squared_error: 0.4723\n",
            "Epoch 33/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2175 - mean_squared_error: 0.0774\n",
            "Epoch 33: val_loss improved from 0.51501 to 0.50546, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3804 - mean_squared_error: 0.3286 - val_loss: 0.5055 - val_mean_squared_error: 0.4562\n",
            "Epoch 34/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3785 - mean_squared_error: 0.3238\n",
            "Epoch 34: val_loss improved from 0.50546 to 0.50146, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3785 - mean_squared_error: 0.3238 - val_loss: 0.5015 - val_mean_squared_error: 0.4487\n",
            "Epoch 35/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4532 - mean_squared_error: 0.4685\n",
            "Epoch 35: val_loss improved from 0.50146 to 0.49902, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3732 - mean_squared_error: 0.3196 - val_loss: 0.4990 - val_mean_squared_error: 0.4434\n",
            "Epoch 36/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.1644\n",
            "Epoch 36: val_loss did not improve from 0.49902\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3696 - mean_squared_error: 0.3149 - val_loss: 0.5020 - val_mean_squared_error: 0.4447\n",
            "Epoch 37/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4726 - mean_squared_error: 0.6067\n",
            "Epoch 37: val_loss improved from 0.49902 to 0.49785, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3694 - mean_squared_error: 0.3120 - val_loss: 0.4978 - val_mean_squared_error: 0.4371\n",
            "Epoch 38/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3686 - mean_squared_error: 0.3100\n",
            "Epoch 38: val_loss improved from 0.49785 to 0.49263, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3686 - mean_squared_error: 0.3100 - val_loss: 0.4926 - val_mean_squared_error: 0.4354\n",
            "Epoch 39/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2633 - mean_squared_error: 0.1232\n",
            "Epoch 39: val_loss did not improve from 0.49263\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3685 - mean_squared_error: 0.3067 - val_loss: 0.4960 - val_mean_squared_error: 0.4304\n",
            "Epoch 40/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3747 - mean_squared_error: 0.3088\n",
            "Epoch 40: val_loss did not improve from 0.49263\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3747 - mean_squared_error: 0.3088 - val_loss: 0.4980 - val_mean_squared_error: 0.4383\n",
            "Epoch 41/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3896 - mean_squared_error: 0.3191\n",
            "Epoch 41: val_loss improved from 0.49263 to 0.48513, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3896 - mean_squared_error: 0.3191 - val_loss: 0.4851 - val_mean_squared_error: 0.4207\n",
            "Epoch 42/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3695 - mean_squared_error: 0.3017\n",
            "Epoch 42: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3695 - mean_squared_error: 0.3017 - val_loss: 0.5135 - val_mean_squared_error: 0.4457\n",
            "Epoch 43/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3669 - mean_squared_error: 0.3049\n",
            "Epoch 43: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3631 - mean_squared_error: 0.2967 - val_loss: 0.4890 - val_mean_squared_error: 0.4249\n",
            "Epoch 44/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4129 - mean_squared_error: 0.3081\n",
            "Epoch 44: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3630 - mean_squared_error: 0.2963 - val_loss: 0.4998 - val_mean_squared_error: 0.4328\n",
            "Epoch 45/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3580 - mean_squared_error: 0.2930\n",
            "Epoch 45: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3580 - mean_squared_error: 0.2930 - val_loss: 0.4873 - val_mean_squared_error: 0.4183\n",
            "Epoch 46/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3664 - mean_squared_error: 0.3046\n",
            "Epoch 46: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3589 - mean_squared_error: 0.2943 - val_loss: 0.4884 - val_mean_squared_error: 0.4190\n",
            "Epoch 47/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3564 - mean_squared_error: 0.2898\n",
            "Epoch 47: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3564 - mean_squared_error: 0.2898 - val_loss: 0.4920 - val_mean_squared_error: 0.4253\n",
            "Epoch 48/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3578 - mean_squared_error: 0.2911\n",
            "Epoch 48: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3562 - mean_squared_error: 0.2892 - val_loss: 0.4900 - val_mean_squared_error: 0.4188\n",
            "Epoch 49/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3543 - mean_squared_error: 0.2899\n",
            "Epoch 49: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3558 - mean_squared_error: 0.2870 - val_loss: 0.4931 - val_mean_squared_error: 0.4259\n",
            "Epoch 50/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3357 - mean_squared_error: 0.2161\n",
            "Epoch 50: val_loss did not improve from 0.48513\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3537 - mean_squared_error: 0.2848 - val_loss: 0.4861 - val_mean_squared_error: 0.4138\n",
            "Epoch 51/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3437 - mean_squared_error: 0.2683\n",
            "Epoch 51: val_loss improved from 0.48513 to 0.48067, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3511 - mean_squared_error: 0.2829 - val_loss: 0.4807 - val_mean_squared_error: 0.4142\n",
            "Epoch 52/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3511 - mean_squared_error: 0.2817\n",
            "Epoch 52: val_loss did not improve from 0.48067\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3511 - mean_squared_error: 0.2817 - val_loss: 0.4854 - val_mean_squared_error: 0.4148\n",
            "Epoch 53/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3611 - mean_squared_error: 0.2908\n",
            "Epoch 53: val_loss did not improve from 0.48067\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3548 - mean_squared_error: 0.2812 - val_loss: 0.4850 - val_mean_squared_error: 0.4116\n",
            "Epoch 54/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3577 - mean_squared_error: 0.2977\n",
            "Epoch 54: val_loss improved from 0.48067 to 0.47748, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.3493 - mean_squared_error: 0.2796 - val_loss: 0.4775 - val_mean_squared_error: 0.4112\n",
            "Epoch 55/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3443 - mean_squared_error: 0.2735\n",
            "Epoch 55: val_loss did not improve from 0.47748\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3498 - mean_squared_error: 0.2765 - val_loss: 0.4914 - val_mean_squared_error: 0.4272\n",
            "Epoch 56/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3484 - mean_squared_error: 0.2754\n",
            "Epoch 56: val_loss did not improve from 0.47748\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.3484 - mean_squared_error: 0.2754 - val_loss: 0.4795 - val_mean_squared_error: 0.4155\n",
            "Epoch 57/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3487 - mean_squared_error: 0.2796\n",
            "Epoch 57: val_loss improved from 0.47748 to 0.47086, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3433 - mean_squared_error: 0.2712 - val_loss: 0.4709 - val_mean_squared_error: 0.4027\n",
            "Epoch 58/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3482 - mean_squared_error: 0.2775\n",
            "Epoch 58: val_loss did not improve from 0.47086\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3455 - mean_squared_error: 0.2747 - val_loss: 0.4710 - val_mean_squared_error: 0.4034\n",
            "Epoch 59/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3441 - mean_squared_error: 0.2746\n",
            "Epoch 59: val_loss did not improve from 0.47086\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3416 - mean_squared_error: 0.2685 - val_loss: 0.4749 - val_mean_squared_error: 0.4165\n",
            "Epoch 60/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3426 - mean_squared_error: 0.2682\n",
            "Epoch 60: val_loss improved from 0.47086 to 0.46588, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3426 - mean_squared_error: 0.2682 - val_loss: 0.4659 - val_mean_squared_error: 0.4017\n",
            "Epoch 61/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3335 - mean_squared_error: 0.2551\n",
            "Epoch 61: val_loss did not improve from 0.46588\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3398 - mean_squared_error: 0.2657 - val_loss: 0.4706 - val_mean_squared_error: 0.4031\n",
            "Epoch 62/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3349 - mean_squared_error: 0.2511\n",
            "Epoch 62: val_loss improved from 0.46588 to 0.46478, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3423 - mean_squared_error: 0.2664 - val_loss: 0.4648 - val_mean_squared_error: 0.4013\n",
            "Epoch 63/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3437 - mean_squared_error: 0.2769\n",
            "Epoch 63: val_loss did not improve from 0.46478\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3400 - mean_squared_error: 0.2640 - val_loss: 0.4651 - val_mean_squared_error: 0.3956\n",
            "Epoch 64/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3526 - mean_squared_error: 0.2908\n",
            "Epoch 64: val_loss did not improve from 0.46478\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3364 - mean_squared_error: 0.2602 - val_loss: 0.4724 - val_mean_squared_error: 0.3992\n",
            "Epoch 65/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3466 - mean_squared_error: 0.2788\n",
            "Epoch 65: val_loss improved from 0.46478 to 0.46460, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3338 - mean_squared_error: 0.2578 - val_loss: 0.4646 - val_mean_squared_error: 0.3900\n",
            "Epoch 66/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3234 - mean_squared_error: 0.2315\n",
            "Epoch 66: val_loss improved from 0.46460 to 0.46172, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3338 - mean_squared_error: 0.2564 - val_loss: 0.4617 - val_mean_squared_error: 0.3874\n",
            "Epoch 67/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3331 - mean_squared_error: 0.2551\n",
            "Epoch 67: val_loss did not improve from 0.46172\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3331 - mean_squared_error: 0.2551 - val_loss: 0.4621 - val_mean_squared_error: 0.3886\n",
            "Epoch 68/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3381 - mean_squared_error: 0.2699\n",
            "Epoch 68: val_loss improved from 0.46172 to 0.46008, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3292 - mean_squared_error: 0.2502 - val_loss: 0.4601 - val_mean_squared_error: 0.3877\n",
            "Epoch 69/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3087 - mean_squared_error: 0.2241\n",
            "Epoch 69: val_loss did not improve from 0.46008\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3300 - mean_squared_error: 0.2489 - val_loss: 0.4655 - val_mean_squared_error: 0.3848\n",
            "Epoch 70/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3687 - mean_squared_error: 0.2882\n",
            "Epoch 70: val_loss did not improve from 0.46008\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3335 - mean_squared_error: 0.2492 - val_loss: 0.4609 - val_mean_squared_error: 0.3815\n",
            "Epoch 71/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3277 - mean_squared_error: 0.2524\n",
            "Epoch 71: val_loss improved from 0.46008 to 0.44602, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3253 - mean_squared_error: 0.2455 - val_loss: 0.4460 - val_mean_squared_error: 0.3724\n",
            "Epoch 72/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3303 - mean_squared_error: 0.2545\n",
            "Epoch 72: val_loss did not improve from 0.44602\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3290 - mean_squared_error: 0.2455 - val_loss: 0.4627 - val_mean_squared_error: 0.3772\n",
            "Epoch 73/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3087 - mean_squared_error: 0.2101\n",
            "Epoch 73: val_loss improved from 0.44602 to 0.43997, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3230 - mean_squared_error: 0.2411 - val_loss: 0.4400 - val_mean_squared_error: 0.3548\n",
            "Epoch 74/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3404 - mean_squared_error: 0.2653\n",
            "Epoch 74: val_loss did not improve from 0.43997\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3201 - mean_squared_error: 0.2341 - val_loss: 0.4553 - val_mean_squared_error: 0.3588\n",
            "Epoch 75/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3163 - mean_squared_error: 0.2177\n",
            "Epoch 75: val_loss improved from 0.43997 to 0.42911, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.3202 - mean_squared_error: 0.2338 - val_loss: 0.4291 - val_mean_squared_error: 0.3423\n",
            "Epoch 76/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3189 - mean_squared_error: 0.2342\n",
            "Epoch 76: val_loss did not improve from 0.42911\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3174 - mean_squared_error: 0.2302 - val_loss: 0.4393 - val_mean_squared_error: 0.3434\n",
            "Epoch 77/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3334 - mean_squared_error: 0.2674\n",
            "Epoch 77: val_loss did not improve from 0.42911\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3158 - mean_squared_error: 0.2272 - val_loss: 0.4398 - val_mean_squared_error: 0.3356\n",
            "Epoch 78/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3099 - mean_squared_error: 0.2266\n",
            "Epoch 78: val_loss did not improve from 0.42911\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3113 - mean_squared_error: 0.2242 - val_loss: 0.4314 - val_mean_squared_error: 0.3271\n",
            "Epoch 79/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2907 - mean_squared_error: 0.1896\n",
            "Epoch 79: val_loss did not improve from 0.42911\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3070 - mean_squared_error: 0.2183 - val_loss: 0.4371 - val_mean_squared_error: 0.3339\n",
            "Epoch 80/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3083 - mean_squared_error: 0.2481\n",
            "Epoch 80: val_loss improved from 0.42911 to 0.41527, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.3069 - mean_squared_error: 0.2191 - val_loss: 0.4153 - val_mean_squared_error: 0.3319\n",
            "Epoch 81/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3150 - mean_squared_error: 0.2169\n",
            "Epoch 81: val_loss improved from 0.41527 to 0.41061, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3150 - mean_squared_error: 0.2169 - val_loss: 0.4106 - val_mean_squared_error: 0.3239\n",
            "Epoch 82/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3025 - mean_squared_error: 0.2120\n",
            "Epoch 82: val_loss did not improve from 0.41061\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.3025 - mean_squared_error: 0.2120 - val_loss: 0.4161 - val_mean_squared_error: 0.3217\n",
            "Epoch 83/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3007 - mean_squared_error: 0.2118\n",
            "Epoch 83: val_loss improved from 0.41061 to 0.40523, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3019 - mean_squared_error: 0.2100 - val_loss: 0.4052 - val_mean_squared_error: 0.3141\n",
            "Epoch 84/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1999 - mean_squared_error: 0.0621\n",
            "Epoch 84: val_loss improved from 0.40523 to 0.39776, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3008 - mean_squared_error: 0.2108 - val_loss: 0.3978 - val_mean_squared_error: 0.3089\n",
            "Epoch 85/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4307 - mean_squared_error: 0.4369\n",
            "Epoch 85: val_loss improved from 0.39776 to 0.39563, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3009 - mean_squared_error: 0.2055 - val_loss: 0.3956 - val_mean_squared_error: 0.3021\n",
            "Epoch 86/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2931 - mean_squared_error: 0.2016\n",
            "Epoch 86: val_loss did not improve from 0.39563\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2982 - mean_squared_error: 0.2055 - val_loss: 0.3957 - val_mean_squared_error: 0.3027\n",
            "Epoch 87/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2629 - mean_squared_error: 0.1461\n",
            "Epoch 87: val_loss improved from 0.39563 to 0.39242, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2976 - mean_squared_error: 0.2010 - val_loss: 0.3924 - val_mean_squared_error: 0.2992\n",
            "Epoch 88/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3008 - mean_squared_error: 0.2051\n",
            "Epoch 88: val_loss did not improve from 0.39242\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3008 - mean_squared_error: 0.2051 - val_loss: 0.3993 - val_mean_squared_error: 0.3000\n",
            "Epoch 89/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3038 - mean_squared_error: 0.2061\n",
            "Epoch 89: val_loss improved from 0.39242 to 0.38589, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2993 - mean_squared_error: 0.2000 - val_loss: 0.3859 - val_mean_squared_error: 0.2967\n",
            "Epoch 90/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2895 - mean_squared_error: 0.1944\n",
            "Epoch 90: val_loss did not improve from 0.38589\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2927 - mean_squared_error: 0.1970 - val_loss: 0.3885 - val_mean_squared_error: 0.2942\n",
            "Epoch 91/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2909 - mean_squared_error: 0.2133\n",
            "Epoch 91: val_loss improved from 0.38589 to 0.38084, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2914 - mean_squared_error: 0.1966 - val_loss: 0.3808 - val_mean_squared_error: 0.2902\n",
            "Epoch 92/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2893 - mean_squared_error: 0.1800\n",
            "Epoch 92: val_loss did not improve from 0.38084\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2965 - mean_squared_error: 0.1941 - val_loss: 0.3882 - val_mean_squared_error: 0.2967\n",
            "Epoch 93/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2842 - mean_squared_error: 0.1834\n",
            "Epoch 93: val_loss improved from 0.38084 to 0.38065, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2920 - mean_squared_error: 0.1950 - val_loss: 0.3806 - val_mean_squared_error: 0.2837\n",
            "Epoch 94/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3828 - mean_squared_error: 0.3026\n",
            "Epoch 94: val_loss improved from 0.38065 to 0.37680, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2898 - mean_squared_error: 0.1881 - val_loss: 0.3768 - val_mean_squared_error: 0.2804\n",
            "Epoch 95/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2917 - mean_squared_error: 0.1915\n",
            "Epoch 95: val_loss did not improve from 0.37680\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2889 - mean_squared_error: 0.1921 - val_loss: 0.3876 - val_mean_squared_error: 0.2867\n",
            "Epoch 96/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2953 - mean_squared_error: 0.1957\n",
            "Epoch 96: val_loss did not improve from 0.37680\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2889 - mean_squared_error: 0.1886 - val_loss: 0.3916 - val_mean_squared_error: 0.2861\n",
            "Epoch 97/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2506 - mean_squared_error: 0.1271\n",
            "Epoch 97: val_loss did not improve from 0.37680\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2879 - mean_squared_error: 0.1911 - val_loss: 0.3808 - val_mean_squared_error: 0.2789\n",
            "Epoch 98/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2885 - mean_squared_error: 0.1904\n",
            "Epoch 98: val_loss did not improve from 0.37680\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2885 - mean_squared_error: 0.1904 - val_loss: 0.3827 - val_mean_squared_error: 0.2863\n",
            "Epoch 99/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4050 - mean_squared_error: 0.3262\n",
            "Epoch 99: val_loss did not improve from 0.37680\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2859 - mean_squared_error: 0.1850 - val_loss: 0.3773 - val_mean_squared_error: 0.2749\n",
            "Epoch 100/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2881 - mean_squared_error: 0.1884\n",
            "Epoch 100: val_loss improved from 0.37680 to 0.36114, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2881 - mean_squared_error: 0.1884 - val_loss: 0.3611 - val_mean_squared_error: 0.2679\n",
            "Epoch 101/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2361 - mean_squared_error: 0.1280\n",
            "Epoch 101: val_loss did not improve from 0.36114\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2796 - mean_squared_error: 0.1815 - val_loss: 0.3695 - val_mean_squared_error: 0.2751\n",
            "Epoch 102/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3890 - mean_squared_error: 0.3488\n",
            "Epoch 102: val_loss did not improve from 0.36114\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2824 - mean_squared_error: 0.1830 - val_loss: 0.3939 - val_mean_squared_error: 0.2851\n",
            "Epoch 103/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4509 - mean_squared_error: 0.4658\n",
            "Epoch 103: val_loss did not improve from 0.36114\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2822 - mean_squared_error: 0.1847 - val_loss: 0.3637 - val_mean_squared_error: 0.2706\n",
            "Epoch 104/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2790 - mean_squared_error: 0.1801\n",
            "Epoch 104: val_loss did not improve from 0.36114\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2790 - mean_squared_error: 0.1801 - val_loss: 0.3816 - val_mean_squared_error: 0.2786\n",
            "Epoch 105/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2800 - mean_squared_error: 0.1789\n",
            "Epoch 105: val_loss improved from 0.36114 to 0.35658, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2800 - mean_squared_error: 0.1789 - val_loss: 0.3566 - val_mean_squared_error: 0.2548\n",
            "Epoch 106/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2825 - mean_squared_error: 0.1801\n",
            "Epoch 106: val_loss did not improve from 0.35658\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2825 - mean_squared_error: 0.1801 - val_loss: 0.3686 - val_mean_squared_error: 0.2598\n",
            "Epoch 107/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2735 - mean_squared_error: 0.1685\n",
            "Epoch 107: val_loss improved from 0.35658 to 0.35187, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2724 - mean_squared_error: 0.1730 - val_loss: 0.3519 - val_mean_squared_error: 0.2542\n",
            "Epoch 108/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2794 - mean_squared_error: 0.1799\n",
            "Epoch 108: val_loss did not improve from 0.35187\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2743 - mean_squared_error: 0.1740 - val_loss: 0.3658 - val_mean_squared_error: 0.2653\n",
            "Epoch 109/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2752 - mean_squared_error: 0.1739\n",
            "Epoch 109: val_loss did not improve from 0.35187\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2752 - mean_squared_error: 0.1739 - val_loss: 0.3688 - val_mean_squared_error: 0.2648\n",
            "Epoch 110/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2729 - mean_squared_error: 0.1742\n",
            "Epoch 110: val_loss improved from 0.35187 to 0.35123, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2729 - mean_squared_error: 0.1742 - val_loss: 0.3512 - val_mean_squared_error: 0.2540\n",
            "Epoch 111/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2723 - mean_squared_error: 0.1682\n",
            "Epoch 111: val_loss did not improve from 0.35123\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2723 - mean_squared_error: 0.1682 - val_loss: 0.3803 - val_mean_squared_error: 0.2623\n",
            "Epoch 112/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2606 - mean_squared_error: 0.1399\n",
            "Epoch 112: val_loss improved from 0.35123 to 0.33324, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2798 - mean_squared_error: 0.1777 - val_loss: 0.3332 - val_mean_squared_error: 0.2336\n",
            "Epoch 113/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2461 - mean_squared_error: 0.1145\n",
            "Epoch 113: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2966 - mean_squared_error: 0.1710 - val_loss: 0.3380 - val_mean_squared_error: 0.2313\n",
            "Epoch 114/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2779 - mean_squared_error: 0.1764\n",
            "Epoch 114: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2779 - mean_squared_error: 0.1764 - val_loss: 0.3676 - val_mean_squared_error: 0.2538\n",
            "Epoch 115/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2711 - mean_squared_error: 0.1692\n",
            "Epoch 115: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2689 - mean_squared_error: 0.1669 - val_loss: 0.3367 - val_mean_squared_error: 0.2364\n",
            "Epoch 116/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3045 - mean_squared_error: 0.1752\n",
            "Epoch 116: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2725 - mean_squared_error: 0.1637 - val_loss: 0.3600 - val_mean_squared_error: 0.2434\n",
            "Epoch 117/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3020 - mean_squared_error: 0.1402\n",
            "Epoch 117: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2738 - mean_squared_error: 0.1710 - val_loss: 0.3479 - val_mean_squared_error: 0.2444\n",
            "Epoch 118/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2728 - mean_squared_error: 0.1628\n",
            "Epoch 118: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2728 - mean_squared_error: 0.1628 - val_loss: 0.3545 - val_mean_squared_error: 0.2428\n",
            "Epoch 119/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2172 - mean_squared_error: 0.0876\n",
            "Epoch 119: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2648 - mean_squared_error: 0.1615 - val_loss: 0.3476 - val_mean_squared_error: 0.2344\n",
            "Epoch 120/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2962 - mean_squared_error: 0.1809\n",
            "Epoch 120: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2659 - mean_squared_error: 0.1594 - val_loss: 0.3432 - val_mean_squared_error: 0.2332\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2652 - mean_squared_error: 0.1589\n",
            "Epoch 121: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2652 - mean_squared_error: 0.1589 - val_loss: 0.3361 - val_mean_squared_error: 0.2275\n",
            "Epoch 122/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2634 - mean_squared_error: 0.1593\n",
            "Epoch 122: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2634 - mean_squared_error: 0.1593 - val_loss: 0.3374 - val_mean_squared_error: 0.2235\n",
            "Epoch 123/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2630 - mean_squared_error: 0.1570\n",
            "Epoch 123: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2630 - mean_squared_error: 0.1570 - val_loss: 0.3590 - val_mean_squared_error: 0.2365\n",
            "Epoch 124/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2638 - mean_squared_error: 0.1607\n",
            "Epoch 124: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2655 - mean_squared_error: 0.1624 - val_loss: 0.3391 - val_mean_squared_error: 0.2223\n",
            "Epoch 125/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2632 - mean_squared_error: 0.1544\n",
            "Epoch 125: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2632 - mean_squared_error: 0.1544 - val_loss: 0.3454 - val_mean_squared_error: 0.2273\n",
            "Epoch 126/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2606 - mean_squared_error: 0.1550\n",
            "Epoch 126: val_loss did not improve from 0.33324\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2606 - mean_squared_error: 0.1550 - val_loss: 0.3361 - val_mean_squared_error: 0.2230\n",
            "Epoch 127/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2469 - mean_squared_error: 0.1385\n",
            "Epoch 127: val_loss improved from 0.33324 to 0.32968, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2591 - mean_squared_error: 0.1530 - val_loss: 0.3297 - val_mean_squared_error: 0.2088\n",
            "Epoch 128/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2937 - mean_squared_error: 0.2475\n",
            "Epoch 128: val_loss did not improve from 0.32968\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2598 - mean_squared_error: 0.1501 - val_loss: 0.3330 - val_mean_squared_error: 0.2050\n",
            "Epoch 129/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2606 - mean_squared_error: 0.1539\n",
            "Epoch 129: val_loss did not improve from 0.32968\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2620 - mean_squared_error: 0.1538 - val_loss: 0.3413 - val_mean_squared_error: 0.2196\n",
            "Epoch 130/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2586 - mean_squared_error: 0.1489\n",
            "Epoch 130: val_loss did not improve from 0.32968\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2555 - mean_squared_error: 0.1457 - val_loss: 0.3364 - val_mean_squared_error: 0.2129\n",
            "Epoch 131/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3184 - mean_squared_error: 0.2395\n",
            "Epoch 131: val_loss did not improve from 0.32968\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2596 - mean_squared_error: 0.1494 - val_loss: 0.3389 - val_mean_squared_error: 0.2086\n",
            "Epoch 132/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2542 - mean_squared_error: 0.1467\n",
            "Epoch 132: val_loss did not improve from 0.32968\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2548 - mean_squared_error: 0.1437 - val_loss: 0.3667 - val_mean_squared_error: 0.2239\n",
            "Epoch 133/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2547 - mean_squared_error: 0.1479\n",
            "Epoch 133: val_loss improved from 0.32968 to 0.32915, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2547 - mean_squared_error: 0.1479 - val_loss: 0.3291 - val_mean_squared_error: 0.2031\n",
            "Epoch 134/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2355 - mean_squared_error: 0.1252\n",
            "Epoch 134: val_loss did not improve from 0.32915\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2542 - mean_squared_error: 0.1409 - val_loss: 0.3446 - val_mean_squared_error: 0.2074\n",
            "Epoch 135/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.1514\n",
            "Epoch 135: val_loss did not improve from 0.32915\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2568 - mean_squared_error: 0.1453 - val_loss: 0.3309 - val_mean_squared_error: 0.2119\n",
            "Epoch 136/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2524 - mean_squared_error: 0.1376\n",
            "Epoch 136: val_loss did not improve from 0.32915\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2524 - mean_squared_error: 0.1376 - val_loss: 0.3434 - val_mean_squared_error: 0.2096\n",
            "Epoch 137/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2550 - mean_squared_error: 0.1412\n",
            "Epoch 137: val_loss improved from 0.32915 to 0.32426, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2550 - mean_squared_error: 0.1412 - val_loss: 0.3243 - val_mean_squared_error: 0.1995\n",
            "Epoch 138/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2487 - mean_squared_error: 0.1338\n",
            "Epoch 138: val_loss did not improve from 0.32426\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2485 - mean_squared_error: 0.1351 - val_loss: 0.3258 - val_mean_squared_error: 0.1961\n",
            "Epoch 139/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2484 - mean_squared_error: 0.1374\n",
            "Epoch 139: val_loss did not improve from 0.32426\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2484 - mean_squared_error: 0.1374 - val_loss: 0.3558 - val_mean_squared_error: 0.2166\n",
            "Epoch 140/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2422 - mean_squared_error: 0.1343\n",
            "Epoch 140: val_loss did not improve from 0.32426\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2476 - mean_squared_error: 0.1368 - val_loss: 0.3352 - val_mean_squared_error: 0.2082\n",
            "Epoch 141/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2462 - mean_squared_error: 0.1335\n",
            "Epoch 141: val_loss improved from 0.32426 to 0.32410, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2462 - mean_squared_error: 0.1335 - val_loss: 0.3241 - val_mean_squared_error: 0.1935\n",
            "Epoch 142/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2707 - mean_squared_error: 0.2081\n",
            "Epoch 142: val_loss did not improve from 0.32410\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2437 - mean_squared_error: 0.1308 - val_loss: 0.3314 - val_mean_squared_error: 0.2007\n",
            "Epoch 143/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2441 - mean_squared_error: 0.1288\n",
            "Epoch 143: val_loss did not improve from 0.32410\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2441 - mean_squared_error: 0.1288 - val_loss: 0.3383 - val_mean_squared_error: 0.1988\n",
            "Epoch 144/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2445 - mean_squared_error: 0.1280\n",
            "Epoch 144: val_loss improved from 0.32410 to 0.30815, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2445 - mean_squared_error: 0.1280 - val_loss: 0.3082 - val_mean_squared_error: 0.1712\n",
            "Epoch 145/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2549 - mean_squared_error: 0.1032\n",
            "Epoch 145: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2431 - mean_squared_error: 0.1259 - val_loss: 0.3634 - val_mean_squared_error: 0.2081\n",
            "Epoch 146/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2460 - mean_squared_error: 0.1297\n",
            "Epoch 146: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2416 - mean_squared_error: 0.1262 - val_loss: 0.3362 - val_mean_squared_error: 0.1886\n",
            "Epoch 147/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2362 - mean_squared_error: 0.1226\n",
            "Epoch 147: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2394 - mean_squared_error: 0.1237 - val_loss: 0.3160 - val_mean_squared_error: 0.1692\n",
            "Epoch 148/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2360 - mean_squared_error: 0.1173\n",
            "Epoch 148: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2360 - mean_squared_error: 0.1173 - val_loss: 0.3440 - val_mean_squared_error: 0.1884\n",
            "Epoch 149/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2419 - mean_squared_error: 0.1261\n",
            "Epoch 149: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2392 - mean_squared_error: 0.1234 - val_loss: 0.3227 - val_mean_squared_error: 0.1807\n",
            "Epoch 150/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2784 - mean_squared_error: 0.1106\n",
            "Epoch 150: val_loss did not improve from 0.30815\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2389 - mean_squared_error: 0.1206 - val_loss: 0.3302 - val_mean_squared_error: 0.1862\n",
            "Epoch 151/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2291 - mean_squared_error: 0.1131\n",
            "Epoch 151: val_loss improved from 0.30815 to 0.29521, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2334 - mean_squared_error: 0.1155 - val_loss: 0.2952 - val_mean_squared_error: 0.1568\n",
            "Epoch 152/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2385 - mean_squared_error: 0.1171\n",
            "Epoch 152: val_loss did not improve from 0.29521\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2347 - mean_squared_error: 0.1135 - val_loss: 0.3347 - val_mean_squared_error: 0.1738\n",
            "Epoch 153/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2319 - mean_squared_error: 0.1151\n",
            "Epoch 153: val_loss did not improve from 0.29521\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2319 - mean_squared_error: 0.1155 - val_loss: 0.3151 - val_mean_squared_error: 0.1708\n",
            "Epoch 154/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2353 - mean_squared_error: 0.1167\n",
            "Epoch 154: val_loss did not improve from 0.29521\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2300 - mean_squared_error: 0.1125 - val_loss: 0.3147 - val_mean_squared_error: 0.1655\n",
            "Epoch 155/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2323 - mean_squared_error: 0.1173\n",
            "Epoch 155: val_loss did not improve from 0.29521\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2290 - mean_squared_error: 0.1107 - val_loss: 0.3304 - val_mean_squared_error: 0.1708\n",
            "Epoch 156/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2327 - mean_squared_error: 0.1129\n",
            "Epoch 156: val_loss did not improve from 0.29521\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2293 - mean_squared_error: 0.1100 - val_loss: 0.3184 - val_mean_squared_error: 0.1645\n",
            "Epoch 157/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2301 - mean_squared_error: 0.1109\n",
            "Epoch 157: val_loss improved from 0.29521 to 0.28924, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2281 - mean_squared_error: 0.1105 - val_loss: 0.2892 - val_mean_squared_error: 0.1560\n",
            "Epoch 158/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2280 - mean_squared_error: 0.1097\n",
            "Epoch 158: val_loss did not improve from 0.28924\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2260 - mean_squared_error: 0.1061 - val_loss: 0.3290 - val_mean_squared_error: 0.1737\n",
            "Epoch 159/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2202 - mean_squared_error: 0.1049\n",
            "Epoch 159: val_loss did not improve from 0.28924\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2253 - mean_squared_error: 0.1093 - val_loss: 0.3119 - val_mean_squared_error: 0.1614\n",
            "Epoch 160/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2280 - mean_squared_error: 0.1084\n",
            "Epoch 160: val_loss did not improve from 0.28924\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2281 - mean_squared_error: 0.1071 - val_loss: 0.3514 - val_mean_squared_error: 0.1754\n",
            "Epoch 161/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2374 - mean_squared_error: 0.1156\n",
            "Epoch 161: val_loss improved from 0.28924 to 0.27731, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2326 - mean_squared_error: 0.1105 - val_loss: 0.2773 - val_mean_squared_error: 0.1405\n",
            "Epoch 162/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2266 - mean_squared_error: 0.1062\n",
            "Epoch 162: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2259 - mean_squared_error: 0.1053 - val_loss: 0.3434 - val_mean_squared_error: 0.1826\n",
            "Epoch 163/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2375 - mean_squared_error: 0.1179\n",
            "Epoch 163: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2278 - mean_squared_error: 0.1093 - val_loss: 0.2877 - val_mean_squared_error: 0.1402\n",
            "Epoch 164/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2271 - mean_squared_error: 0.1052\n",
            "Epoch 164: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2265 - mean_squared_error: 0.1039 - val_loss: 0.3274 - val_mean_squared_error: 0.1595\n",
            "Epoch 165/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2341 - mean_squared_error: 0.1107\n",
            "Epoch 165: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2244 - mean_squared_error: 0.1026 - val_loss: 0.2917 - val_mean_squared_error: 0.1503\n",
            "Epoch 166/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2103 - mean_squared_error: 0.0966\n",
            "Epoch 166: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2233 - mean_squared_error: 0.1038 - val_loss: 0.3312 - val_mean_squared_error: 0.1705\n",
            "Epoch 167/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2186 - mean_squared_error: 0.1032\n",
            "Epoch 167: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2203 - mean_squared_error: 0.1033 - val_loss: 0.3045 - val_mean_squared_error: 0.1552\n",
            "Epoch 168/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2157 - mean_squared_error: 0.0949\n",
            "Epoch 168: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2205 - mean_squared_error: 0.1017 - val_loss: 0.3059 - val_mean_squared_error: 0.1532\n",
            "Epoch 169/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2137 - mean_squared_error: 0.0974\n",
            "Epoch 169: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2208 - mean_squared_error: 0.1014 - val_loss: 0.3311 - val_mean_squared_error: 0.1690\n",
            "Epoch 170/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2219 - mean_squared_error: 0.1035\n",
            "Epoch 170: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2215 - mean_squared_error: 0.1032 - val_loss: 0.2901 - val_mean_squared_error: 0.1447\n",
            "Epoch 171/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2205 - mean_squared_error: 0.1002\n",
            "Epoch 171: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2205 - mean_squared_error: 0.1002 - val_loss: 0.3174 - val_mean_squared_error: 0.1592\n",
            "Epoch 172/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2291 - mean_squared_error: 0.1016\n",
            "Epoch 172: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2231 - mean_squared_error: 0.1025 - val_loss: 0.3160 - val_mean_squared_error: 0.1520\n",
            "Epoch 173/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2294 - mean_squared_error: 0.1088\n",
            "Epoch 173: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2212 - mean_squared_error: 0.1019 - val_loss: 0.2797 - val_mean_squared_error: 0.1312\n",
            "Epoch 174/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2178 - mean_squared_error: 0.0959\n",
            "Epoch 174: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2202 - mean_squared_error: 0.0989 - val_loss: 0.3176 - val_mean_squared_error: 0.1569\n",
            "Epoch 175/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2177 - mean_squared_error: 0.1000\n",
            "Epoch 175: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2177 - mean_squared_error: 0.1000 - val_loss: 0.2987 - val_mean_squared_error: 0.1386\n",
            "Epoch 176/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2223 - mean_squared_error: 0.1053\n",
            "Epoch 176: val_loss did not improve from 0.27731\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2171 - mean_squared_error: 0.0988 - val_loss: 0.3175 - val_mean_squared_error: 0.1477\n",
            "Epoch 177/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.1004\n",
            "Epoch 177: val_loss improved from 0.27731 to 0.27064, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2185 - mean_squared_error: 0.0993 - val_loss: 0.2706 - val_mean_squared_error: 0.1335\n",
            "Epoch 178/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2331 - mean_squared_error: 0.1114\n",
            "Epoch 178: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2177 - mean_squared_error: 0.0991 - val_loss: 0.2864 - val_mean_squared_error: 0.1267\n",
            "Epoch 179/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.1003\n",
            "Epoch 179: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2156 - mean_squared_error: 0.0977 - val_loss: 0.2954 - val_mean_squared_error: 0.1422\n",
            "Epoch 180/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2211 - mean_squared_error: 0.1031\n",
            "Epoch 180: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2154 - mean_squared_error: 0.0969 - val_loss: 0.3059 - val_mean_squared_error: 0.1409\n",
            "Epoch 181/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2334 - mean_squared_error: 0.1118\n",
            "Epoch 181: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2166 - mean_squared_error: 0.0991 - val_loss: 0.2785 - val_mean_squared_error: 0.1300\n",
            "Epoch 182/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2170 - mean_squared_error: 0.1003\n",
            "Epoch 182: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2141 - mean_squared_error: 0.0963 - val_loss: 0.2925 - val_mean_squared_error: 0.1299\n",
            "Epoch 183/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2285 - mean_squared_error: 0.1049\n",
            "Epoch 183: val_loss did not improve from 0.27064\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2176 - mean_squared_error: 0.0968 - val_loss: 0.3190 - val_mean_squared_error: 0.1485\n",
            "Epoch 184/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2138 - mean_squared_error: 0.0973\n",
            "Epoch 184: val_loss improved from 0.27064 to 0.26879, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2129 - mean_squared_error: 0.0952 - val_loss: 0.2688 - val_mean_squared_error: 0.1198\n",
            "Epoch 185/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2085 - mean_squared_error: 0.0929\n",
            "Epoch 185: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2135 - mean_squared_error: 0.0957 - val_loss: 0.3004 - val_mean_squared_error: 0.1344\n",
            "Epoch 186/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2099 - mean_squared_error: 0.0950\n",
            "Epoch 186: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2113 - mean_squared_error: 0.0955 - val_loss: 0.2861 - val_mean_squared_error: 0.1268\n",
            "Epoch 187/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2120 - mean_squared_error: 0.0944\n",
            "Epoch 187: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2120 - mean_squared_error: 0.0944 - val_loss: 0.3139 - val_mean_squared_error: 0.1410\n",
            "Epoch 188/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2103 - mean_squared_error: 0.0964\n",
            "Epoch 188: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2139 - mean_squared_error: 0.0971 - val_loss: 0.2707 - val_mean_squared_error: 0.1193\n",
            "Epoch 189/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2271 - mean_squared_error: 0.1032\n",
            "Epoch 189: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2218 - mean_squared_error: 0.0995 - val_loss: 0.2987 - val_mean_squared_error: 0.1304\n",
            "Epoch 190/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2262 - mean_squared_error: 0.0995\n",
            "Epoch 190: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2208 - mean_squared_error: 0.0938 - val_loss: 0.3140 - val_mean_squared_error: 0.1429\n",
            "Epoch 191/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.0912\n",
            "Epoch 191: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2152 - mean_squared_error: 0.0945 - val_loss: 0.2711 - val_mean_squared_error: 0.1230\n",
            "Epoch 192/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2055 - mean_squared_error: 0.0872\n",
            "Epoch 192: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2126 - mean_squared_error: 0.0945 - val_loss: 0.3042 - val_mean_squared_error: 0.1287\n",
            "Epoch 193/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2181 - mean_squared_error: 0.0969\n",
            "Epoch 193: val_loss did not improve from 0.26879\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2143 - mean_squared_error: 0.0940 - val_loss: 0.3144 - val_mean_squared_error: 0.1384\n",
            "Epoch 194/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2091 - mean_squared_error: 0.0915\n",
            "Epoch 194: val_loss improved from 0.26879 to 0.26590, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2112 - mean_squared_error: 0.0948 - val_loss: 0.2659 - val_mean_squared_error: 0.1195\n",
            "Epoch 195/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2156 - mean_squared_error: 0.0939\n",
            "Epoch 195: val_loss did not improve from 0.26590\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2161 - mean_squared_error: 0.0933 - val_loss: 0.2966 - val_mean_squared_error: 0.1226\n",
            "Epoch 196/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2118 - mean_squared_error: 0.0914\n",
            "Epoch 196: val_loss improved from 0.26590 to 0.24692, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2136 - mean_squared_error: 0.0925 - val_loss: 0.2469 - val_mean_squared_error: 0.1055\n",
            "Epoch 197/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.0949\n",
            "Epoch 197: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2120 - mean_squared_error: 0.0921 - val_loss: 0.3206 - val_mean_squared_error: 0.1450\n",
            "Epoch 198/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2157 - mean_squared_error: 0.0950\n",
            "Epoch 198: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2157 - mean_squared_error: 0.0950 - val_loss: 0.2791 - val_mean_squared_error: 0.1217\n",
            "Epoch 199/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2220 - mean_squared_error: 0.1006\n",
            "Epoch 199: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2122 - mean_squared_error: 0.0928 - val_loss: 0.2634 - val_mean_squared_error: 0.1133\n",
            "Epoch 200/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2070 - mean_squared_error: 0.0796\n",
            "Epoch 200: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2160 - mean_squared_error: 0.0928 - val_loss: 0.3158 - val_mean_squared_error: 0.1397\n",
            "Epoch 201/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2575 - mean_squared_error: 0.1198\n",
            "Epoch 201: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2097 - mean_squared_error: 0.0921 - val_loss: 0.2563 - val_mean_squared_error: 0.1143\n",
            "Epoch 202/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2135 - mean_squared_error: 0.0883\n",
            "Epoch 202: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2121 - mean_squared_error: 0.0884 - val_loss: 0.3106 - val_mean_squared_error: 0.1363\n",
            "Epoch 203/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2101 - mean_squared_error: 0.0915\n",
            "Epoch 203: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2101 - mean_squared_error: 0.0915 - val_loss: 0.2679 - val_mean_squared_error: 0.1152\n",
            "Epoch 204/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2047 - mean_squared_error: 0.0886\n",
            "Epoch 204: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2047 - mean_squared_error: 0.0886 - val_loss: 0.2917 - val_mean_squared_error: 0.1244\n",
            "Epoch 205/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2030 - mean_squared_error: 0.0895\n",
            "Epoch 205: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2043 - mean_squared_error: 0.0888 - val_loss: 0.2765 - val_mean_squared_error: 0.1174\n",
            "Epoch 206/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2062 - mean_squared_error: 0.0892\n",
            "Epoch 206: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2020 - mean_squared_error: 0.0870 - val_loss: 0.2910 - val_mean_squared_error: 0.1218\n",
            "Epoch 207/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2026 - mean_squared_error: 0.0885\n",
            "Epoch 207: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2026 - mean_squared_error: 0.0885 - val_loss: 0.2788 - val_mean_squared_error: 0.1191\n",
            "Epoch 208/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2037 - mean_squared_error: 0.0861\n",
            "Epoch 208: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2039 - mean_squared_error: 0.0868 - val_loss: 0.2616 - val_mean_squared_error: 0.1132\n",
            "Epoch 209/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2012 - mean_squared_error: 0.0883\n",
            "Epoch 209: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2049 - mean_squared_error: 0.0881 - val_loss: 0.2753 - val_mean_squared_error: 0.1107\n",
            "Epoch 210/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2127 - mean_squared_error: 0.0909\n",
            "Epoch 210: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2088 - mean_squared_error: 0.0883 - val_loss: 0.3171 - val_mean_squared_error: 0.1382\n",
            "Epoch 211/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2096 - mean_squared_error: 0.0911\n",
            "Epoch 211: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2096 - mean_squared_error: 0.0911 - val_loss: 0.2597 - val_mean_squared_error: 0.1145\n",
            "Epoch 212/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2232 - mean_squared_error: 0.0999\n",
            "Epoch 212: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2080 - mean_squared_error: 0.0869 - val_loss: 0.2913 - val_mean_squared_error: 0.1200\n",
            "Epoch 213/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2068 - mean_squared_error: 0.0852\n",
            "Epoch 213: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2068 - mean_squared_error: 0.0852 - val_loss: 0.2910 - val_mean_squared_error: 0.1185\n",
            "Epoch 214/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2065 - mean_squared_error: 0.0888\n",
            "Epoch 214: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2083 - mean_squared_error: 0.0902 - val_loss: 0.2878 - val_mean_squared_error: 0.1232\n",
            "Epoch 215/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2034 - mean_squared_error: 0.0842\n",
            "Epoch 215: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2034 - mean_squared_error: 0.0842 - val_loss: 0.2944 - val_mean_squared_error: 0.1276\n",
            "Epoch 216/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1992 - mean_squared_error: 0.0855\n",
            "Epoch 216: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1998 - mean_squared_error: 0.0854 - val_loss: 0.2929 - val_mean_squared_error: 0.1241\n",
            "Epoch 217/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1963 - mean_squared_error: 0.0840\n",
            "Epoch 217: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1963 - mean_squared_error: 0.0840 - val_loss: 0.2809 - val_mean_squared_error: 0.1165\n",
            "Epoch 218/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1989 - mean_squared_error: 0.0826\n",
            "Epoch 218: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1989 - mean_squared_error: 0.0826 - val_loss: 0.3089 - val_mean_squared_error: 0.1329\n",
            "Epoch 219/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1969 - mean_squared_error: 0.0810\n",
            "Epoch 219: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2018 - mean_squared_error: 0.0852 - val_loss: 0.2687 - val_mean_squared_error: 0.1120\n",
            "Epoch 220/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1925 - mean_squared_error: 0.0788\n",
            "Epoch 220: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1952 - mean_squared_error: 0.0826 - val_loss: 0.2958 - val_mean_squared_error: 0.1222\n",
            "Epoch 221/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2306 - mean_squared_error: 0.0919\n",
            "Epoch 221: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1969 - mean_squared_error: 0.0831 - val_loss: 0.2649 - val_mean_squared_error: 0.1137\n",
            "Epoch 222/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1982 - mean_squared_error: 0.0817\n",
            "Epoch 222: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1982 - mean_squared_error: 0.0817 - val_loss: 0.2926 - val_mean_squared_error: 0.1279\n",
            "Epoch 223/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1948 - mean_squared_error: 0.0828\n",
            "Epoch 223: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1948 - mean_squared_error: 0.0828 - val_loss: 0.2749 - val_mean_squared_error: 0.1151\n",
            "Epoch 224/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1951 - mean_squared_error: 0.0823\n",
            "Epoch 224: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1946 - mean_squared_error: 0.0824 - val_loss: 0.3055 - val_mean_squared_error: 0.1344\n",
            "Epoch 225/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1980 - mean_squared_error: 0.0842\n",
            "Epoch 225: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1980 - mean_squared_error: 0.0842 - val_loss: 0.2842 - val_mean_squared_error: 0.1184\n",
            "Epoch 226/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1983 - mean_squared_error: 0.0846\n",
            "Epoch 226: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1946 - mean_squared_error: 0.0821 - val_loss: 0.2813 - val_mean_squared_error: 0.1192\n",
            "Epoch 227/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1947 - mean_squared_error: 0.0822\n",
            "Epoch 227: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1947 - mean_squared_error: 0.0822 - val_loss: 0.2728 - val_mean_squared_error: 0.1201\n",
            "Epoch 228/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1876 - mean_squared_error: 0.0759\n",
            "Epoch 228: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1944 - mean_squared_error: 0.0813 - val_loss: 0.2988 - val_mean_squared_error: 0.1303\n",
            "Epoch 229/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1969 - mean_squared_error: 0.0821\n",
            "Epoch 229: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1955 - mean_squared_error: 0.0807 - val_loss: 0.2594 - val_mean_squared_error: 0.1091\n",
            "Epoch 230/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1954 - mean_squared_error: 0.0827\n",
            "Epoch 230: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1954 - mean_squared_error: 0.0827 - val_loss: 0.3123 - val_mean_squared_error: 0.1337\n",
            "Epoch 231/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1990 - mean_squared_error: 0.0861\n",
            "Epoch 231: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1927 - mean_squared_error: 0.0817 - val_loss: 0.2840 - val_mean_squared_error: 0.1232\n",
            "Epoch 232/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1924 - mean_squared_error: 0.0803\n",
            "Epoch 232: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1924 - mean_squared_error: 0.0803 - val_loss: 0.3060 - val_mean_squared_error: 0.1297\n",
            "Epoch 233/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2015 - mean_squared_error: 0.0847\n",
            "Epoch 233: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1937 - mean_squared_error: 0.0802 - val_loss: 0.2696 - val_mean_squared_error: 0.1142\n",
            "Epoch 234/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1949 - mean_squared_error: 0.0803\n",
            "Epoch 234: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1949 - mean_squared_error: 0.0803 - val_loss: 0.2894 - val_mean_squared_error: 0.1280\n",
            "Epoch 235/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1926 - mean_squared_error: 0.0802\n",
            "Epoch 235: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1926 - mean_squared_error: 0.0802 - val_loss: 0.2949 - val_mean_squared_error: 0.1234\n",
            "Epoch 236/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1908 - mean_squared_error: 0.0793\n",
            "Epoch 236: val_loss did not improve from 0.24692\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1908 - mean_squared_error: 0.0793 - val_loss: 0.2944 - val_mean_squared_error: 0.1283\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "Cycle:  2\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.7913 - mean_squared_error: 0.9609 \n",
            "Epoch 1: val_loss improved from inf to 0.83082, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 3s 81ms/step - loss: 0.7794 - mean_squared_error: 0.9425 - val_loss: 0.8308 - val_mean_squared_error: 1.0177\n",
            "Epoch 2/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5585 - mean_squared_error: 0.5640\n",
            "Epoch 2: val_loss improved from 0.83082 to 0.67066, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.5755 - mean_squared_error: 0.5798 - val_loss: 0.6707 - val_mean_squared_error: 0.6760\n",
            "Epoch 3/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5449 - mean_squared_error: 0.5520\n",
            "Epoch 3: val_loss did not improve from 0.67066\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.5428 - mean_squared_error: 0.5485 - val_loss: 0.6763 - val_mean_squared_error: 0.6900\n",
            "Epoch 4/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4997 - mean_squared_error: 0.4763\n",
            "Epoch 4: val_loss did not improve from 0.67066\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.5049 - mean_squared_error: 0.4763 - val_loss: 0.6854 - val_mean_squared_error: 0.7142\n",
            "Epoch 5/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4864 - mean_squared_error: 0.4467\n",
            "Epoch 5: val_loss did not improve from 0.67066\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.4864 - mean_squared_error: 0.4467 - val_loss: 0.6900 - val_mean_squared_error: 0.7297\n",
            "Epoch 6/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5987 - mean_squared_error: 0.5760\n",
            "Epoch 6: val_loss improved from 0.67066 to 0.66800, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.4732 - mean_squared_error: 0.4274 - val_loss: 0.6680 - val_mean_squared_error: 0.6816\n",
            "Epoch 7/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4612 - mean_squared_error: 0.4095\n",
            "Epoch 7: val_loss improved from 0.66800 to 0.64578, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4612 - mean_squared_error: 0.4095 - val_loss: 0.6458 - val_mean_squared_error: 0.6448\n",
            "Epoch 8/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4534 - mean_squared_error: 0.3987\n",
            "Epoch 8: val_loss did not improve from 0.64578\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.4534 - mean_squared_error: 0.3987 - val_loss: 0.6480 - val_mean_squared_error: 0.6481\n",
            "Epoch 9/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4488 - mean_squared_error: 0.3932\n",
            "Epoch 9: val_loss improved from 0.64578 to 0.62888, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.4423 - mean_squared_error: 0.3827 - val_loss: 0.6289 - val_mean_squared_error: 0.6166\n",
            "Epoch 10/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.4535 - mean_squared_error: 0.4052\n",
            "Epoch 10: val_loss did not improve from 0.62888\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.4346 - mean_squared_error: 0.3729 - val_loss: 0.6319 - val_mean_squared_error: 0.6220\n",
            "Epoch 11/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4280 - mean_squared_error: 0.3615\n",
            "Epoch 11: val_loss improved from 0.62888 to 0.62179, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.4280 - mean_squared_error: 0.3615 - val_loss: 0.6218 - val_mean_squared_error: 0.6058\n",
            "Epoch 12/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4412 - mean_squared_error: 0.4391\n",
            "Epoch 12: val_loss improved from 0.62179 to 0.61737, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4249 - mean_squared_error: 0.3566 - val_loss: 0.6174 - val_mean_squared_error: 0.5977\n",
            "Epoch 13/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4208 - mean_squared_error: 0.3482\n",
            "Epoch 13: val_loss improved from 0.61737 to 0.60414, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4208 - mean_squared_error: 0.3482 - val_loss: 0.6041 - val_mean_squared_error: 0.5744\n",
            "Epoch 14/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4238 - mean_squared_error: 0.3532\n",
            "Epoch 14: val_loss did not improve from 0.60414\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.4152 - mean_squared_error: 0.3408 - val_loss: 0.6068 - val_mean_squared_error: 0.5814\n",
            "Epoch 15/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4116 - mean_squared_error: 0.3406\n",
            "Epoch 15: val_loss improved from 0.60414 to 0.59810, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.4111 - mean_squared_error: 0.3359 - val_loss: 0.5981 - val_mean_squared_error: 0.5660\n",
            "Epoch 16/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4092 - mean_squared_error: 0.3314\n",
            "Epoch 16: val_loss improved from 0.59810 to 0.59517, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.4092 - mean_squared_error: 0.3314 - val_loss: 0.5952 - val_mean_squared_error: 0.5598\n",
            "Epoch 17/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4099 - mean_squared_error: 0.3313\n",
            "Epoch 17: val_loss improved from 0.59517 to 0.58104, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.4047 - mean_squared_error: 0.3215 - val_loss: 0.5810 - val_mean_squared_error: 0.5395\n",
            "Epoch 18/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.4192 - mean_squared_error: 0.3490\n",
            "Epoch 18: val_loss improved from 0.58104 to 0.57894, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.4004 - mean_squared_error: 0.3174 - val_loss: 0.5789 - val_mean_squared_error: 0.5400\n",
            "Epoch 19/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3885 - mean_squared_error: 0.2985\n",
            "Epoch 19: val_loss improved from 0.57894 to 0.57583, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3977 - mean_squared_error: 0.3142 - val_loss: 0.5758 - val_mean_squared_error: 0.5339\n",
            "Epoch 20/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3958 - mean_squared_error: 0.3104\n",
            "Epoch 20: val_loss improved from 0.57583 to 0.56127, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3958 - mean_squared_error: 0.3104 - val_loss: 0.5613 - val_mean_squared_error: 0.5134\n",
            "Epoch 21/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3834 - mean_squared_error: 0.2961\n",
            "Epoch 21: val_loss did not improve from 0.56127\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.3912 - mean_squared_error: 0.3026 - val_loss: 0.5714 - val_mean_squared_error: 0.5328\n",
            "Epoch 22/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3900 - mean_squared_error: 0.3006\n",
            "Epoch 22: val_loss improved from 0.56127 to 0.55319, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3900 - mean_squared_error: 0.3006 - val_loss: 0.5532 - val_mean_squared_error: 0.4977\n",
            "Epoch 23/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3842 - mean_squared_error: 0.2916\n",
            "Epoch 23: val_loss improved from 0.55319 to 0.54336, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3842 - mean_squared_error: 0.2916 - val_loss: 0.5434 - val_mean_squared_error: 0.4830\n",
            "Epoch 24/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3832 - mean_squared_error: 0.2899\n",
            "Epoch 24: val_loss improved from 0.54336 to 0.53709, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3832 - mean_squared_error: 0.2899 - val_loss: 0.5371 - val_mean_squared_error: 0.4752\n",
            "Epoch 25/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3783 - mean_squared_error: 0.2841\n",
            "Epoch 25: val_loss improved from 0.53709 to 0.53554, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3783 - mean_squared_error: 0.2841 - val_loss: 0.5355 - val_mean_squared_error: 0.4676\n",
            "Epoch 26/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3802 - mean_squared_error: 0.2863\n",
            "Epoch 26: val_loss did not improve from 0.53554\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3802 - mean_squared_error: 0.2863 - val_loss: 0.5404 - val_mean_squared_error: 0.4929\n",
            "Epoch 27/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3761 - mean_squared_error: 0.2885\n",
            "Epoch 27: val_loss improved from 0.53554 to 0.51911, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3717 - mean_squared_error: 0.2805 - val_loss: 0.5191 - val_mean_squared_error: 0.4418\n",
            "Epoch 28/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3657 - mean_squared_error: 0.2663\n",
            "Epoch 28: val_loss improved from 0.51911 to 0.51433, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3724 - mean_squared_error: 0.2720 - val_loss: 0.5143 - val_mean_squared_error: 0.4380\n",
            "Epoch 29/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3875 - mean_squared_error: 0.2974\n",
            "Epoch 29: val_loss improved from 0.51433 to 0.51118, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3680 - mean_squared_error: 0.2696 - val_loss: 0.5112 - val_mean_squared_error: 0.4380\n",
            "Epoch 30/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3686 - mean_squared_error: 0.2676\n",
            "Epoch 30: val_loss improved from 0.51118 to 0.49930, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3686 - mean_squared_error: 0.2676 - val_loss: 0.4993 - val_mean_squared_error: 0.4154\n",
            "Epoch 31/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3657 - mean_squared_error: 0.2624\n",
            "Epoch 31: val_loss improved from 0.49930 to 0.49199, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3657 - mean_squared_error: 0.2624 - val_loss: 0.4920 - val_mean_squared_error: 0.4180\n",
            "Epoch 32/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2858 - mean_squared_error: 0.1372\n",
            "Epoch 32: val_loss did not improve from 0.49199\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3631 - mean_squared_error: 0.2621 - val_loss: 0.4944 - val_mean_squared_error: 0.4150\n",
            "Epoch 33/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2041 - mean_squared_error: 0.0630\n",
            "Epoch 33: val_loss did not improve from 0.49199\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3626 - mean_squared_error: 0.2624 - val_loss: 0.4947 - val_mean_squared_error: 0.4256\n",
            "Epoch 34/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3598 - mean_squared_error: 0.2581\n",
            "Epoch 34: val_loss improved from 0.49199 to 0.47305, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3570 - mean_squared_error: 0.2559 - val_loss: 0.4731 - val_mean_squared_error: 0.3831\n",
            "Epoch 35/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3497 - mean_squared_error: 0.2450\n",
            "Epoch 35: val_loss improved from 0.47305 to 0.46645, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3551 - mean_squared_error: 0.2507 - val_loss: 0.4665 - val_mean_squared_error: 0.3859\n",
            "Epoch 36/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3570 - mean_squared_error: 0.2565\n",
            "Epoch 36: val_loss did not improve from 0.46645\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.3522 - mean_squared_error: 0.2489 - val_loss: 0.4669 - val_mean_squared_error: 0.3864\n",
            "Epoch 37/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3491 - mean_squared_error: 0.2448\n",
            "Epoch 37: val_loss did not improve from 0.46645\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3491 - mean_squared_error: 0.2448 - val_loss: 0.4686 - val_mean_squared_error: 0.3876\n",
            "Epoch 38/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3363 - mean_squared_error: 0.2261\n",
            "Epoch 38: val_loss improved from 0.46645 to 0.46238, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3493 - mean_squared_error: 0.2447 - val_loss: 0.4624 - val_mean_squared_error: 0.3743\n",
            "Epoch 39/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3484 - mean_squared_error: 0.2370\n",
            "Epoch 39: val_loss improved from 0.46238 to 0.45562, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3484 - mean_squared_error: 0.2370 - val_loss: 0.4556 - val_mean_squared_error: 0.3667\n",
            "Epoch 40/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3443 - mean_squared_error: 0.2373\n",
            "Epoch 40: val_loss did not improve from 0.45562\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3443 - mean_squared_error: 0.2373 - val_loss: 0.4582 - val_mean_squared_error: 0.3726\n",
            "Epoch 41/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3515 - mean_squared_error: 0.2376\n",
            "Epoch 41: val_loss improved from 0.45562 to 0.44563, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3515 - mean_squared_error: 0.2376 - val_loss: 0.4456 - val_mean_squared_error: 0.3496\n",
            "Epoch 42/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3413 - mean_squared_error: 0.2315\n",
            "Epoch 42: val_loss did not improve from 0.44563\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3413 - mean_squared_error: 0.2315 - val_loss: 0.4591 - val_mean_squared_error: 0.3721\n",
            "Epoch 43/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3416 - mean_squared_error: 0.2284\n",
            "Epoch 43: val_loss improved from 0.44563 to 0.44071, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3416 - mean_squared_error: 0.2284 - val_loss: 0.4407 - val_mean_squared_error: 0.3435\n",
            "Epoch 44/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3337 - mean_squared_error: 0.2111\n",
            "Epoch 44: val_loss did not improve from 0.44071\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3357 - mean_squared_error: 0.2264 - val_loss: 0.4497 - val_mean_squared_error: 0.3626\n",
            "Epoch 45/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3306 - mean_squared_error: 0.2365\n",
            "Epoch 45: val_loss improved from 0.44071 to 0.43964, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.3361 - mean_squared_error: 0.2266 - val_loss: 0.4396 - val_mean_squared_error: 0.3416\n",
            "Epoch 46/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3342 - mean_squared_error: 0.2233\n",
            "Epoch 46: val_loss did not improve from 0.43964\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3342 - mean_squared_error: 0.2233 - val_loss: 0.4502 - val_mean_squared_error: 0.3583\n",
            "Epoch 47/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3253 - mean_squared_error: 0.2086\n",
            "Epoch 47: val_loss did not improve from 0.43964\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3321 - mean_squared_error: 0.2224 - val_loss: 0.4556 - val_mean_squared_error: 0.3669\n",
            "Epoch 48/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3346 - mean_squared_error: 0.2277\n",
            "Epoch 48: val_loss did not improve from 0.43964\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3313 - mean_squared_error: 0.2222 - val_loss: 0.4426 - val_mean_squared_error: 0.3464\n",
            "Epoch 49/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3324 - mean_squared_error: 0.2226\n",
            "Epoch 49: val_loss improved from 0.43964 to 0.43698, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.3286 - mean_squared_error: 0.2172 - val_loss: 0.4370 - val_mean_squared_error: 0.3362\n",
            "Epoch 50/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3004 - mean_squared_error: 0.1739\n",
            "Epoch 50: val_loss did not improve from 0.43698\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3268 - mean_squared_error: 0.2138 - val_loss: 0.4404 - val_mean_squared_error: 0.3387\n",
            "Epoch 51/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3240 - mean_squared_error: 0.2065\n",
            "Epoch 51: val_loss did not improve from 0.43698\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3275 - mean_squared_error: 0.2152 - val_loss: 0.4382 - val_mean_squared_error: 0.3387\n",
            "Epoch 52/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3088 - mean_squared_error: 0.1861\n",
            "Epoch 52: val_loss improved from 0.43698 to 0.42791, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.3272 - mean_squared_error: 0.2100 - val_loss: 0.4279 - val_mean_squared_error: 0.3108\n",
            "Epoch 53/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3274 - mean_squared_error: 0.2131\n",
            "Epoch 53: val_loss did not improve from 0.42791\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3239 - mean_squared_error: 0.2084 - val_loss: 0.4386 - val_mean_squared_error: 0.3490\n",
            "Epoch 54/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3357 - mean_squared_error: 0.2299\n",
            "Epoch 54: val_loss did not improve from 0.42791\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3277 - mean_squared_error: 0.2160 - val_loss: 0.4397 - val_mean_squared_error: 0.3353\n",
            "Epoch 55/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3215 - mean_squared_error: 0.2079\n",
            "Epoch 55: val_loss did not improve from 0.42791\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3215 - mean_squared_error: 0.2079 - val_loss: 0.4309 - val_mean_squared_error: 0.3171\n",
            "Epoch 56/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3047 - mean_squared_error: 0.1921\n",
            "Epoch 56: val_loss did not improve from 0.42791\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3230 - mean_squared_error: 0.2088 - val_loss: 0.4297 - val_mean_squared_error: 0.3275\n",
            "Epoch 57/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3117 - mean_squared_error: 0.1966\n",
            "Epoch 57: val_loss improved from 0.42791 to 0.42522, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 56ms/step - loss: 0.3170 - mean_squared_error: 0.2039 - val_loss: 0.4252 - val_mean_squared_error: 0.3088\n",
            "Epoch 58/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.3031 - mean_squared_error: 0.1826\n",
            "Epoch 58: val_loss improved from 0.42522 to 0.41153, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 0.3171 - mean_squared_error: 0.2021 - val_loss: 0.4115 - val_mean_squared_error: 0.3024\n",
            "Epoch 59/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2768 - mean_squared_error: 0.1474\n",
            "Epoch 59: val_loss did not improve from 0.41153\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.3240 - mean_squared_error: 0.2113 - val_loss: 0.4284 - val_mean_squared_error: 0.3301\n",
            "Epoch 60/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3187 - mean_squared_error: 0.2002\n",
            "Epoch 60: val_loss did not improve from 0.41153\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.3187 - mean_squared_error: 0.2002 - val_loss: 0.4160 - val_mean_squared_error: 0.2940\n",
            "Epoch 61/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3132 - mean_squared_error: 0.2053\n",
            "Epoch 61: val_loss did not improve from 0.41153\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.3134 - mean_squared_error: 0.1991 - val_loss: 0.4221 - val_mean_squared_error: 0.3242\n",
            "Epoch 62/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3195 - mean_squared_error: 0.2073\n",
            "Epoch 62: val_loss did not improve from 0.41153\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.3195 - mean_squared_error: 0.2073 - val_loss: 0.4230 - val_mean_squared_error: 0.3118\n",
            "Epoch 63/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3118 - mean_squared_error: 0.2038\n",
            "Epoch 63: val_loss did not improve from 0.41153\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3129 - mean_squared_error: 0.1979 - val_loss: 0.4221 - val_mean_squared_error: 0.3028\n",
            "Epoch 64/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3094 - mean_squared_error: 0.1954\n",
            "Epoch 64: val_loss improved from 0.41153 to 0.41025, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 1s 88ms/step - loss: 0.3094 - mean_squared_error: 0.1954 - val_loss: 0.4103 - val_mean_squared_error: 0.3001\n",
            "Epoch 65/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3136 - mean_squared_error: 0.2088\n",
            "Epoch 65: val_loss did not improve from 0.41025\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 0.3121 - mean_squared_error: 0.1966 - val_loss: 0.4142 - val_mean_squared_error: 0.2994\n",
            "Epoch 66/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3159 - mean_squared_error: 0.2021\n",
            "Epoch 66: val_loss did not improve from 0.41025\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.3089 - mean_squared_error: 0.1948 - val_loss: 0.4154 - val_mean_squared_error: 0.3092\n",
            "Epoch 67/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2822 - mean_squared_error: 0.1659\n",
            "Epoch 67: val_loss improved from 0.41025 to 0.40902, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 0.3082 - mean_squared_error: 0.1944 - val_loss: 0.4090 - val_mean_squared_error: 0.2994\n",
            "Epoch 68/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3111 - mean_squared_error: 0.2063\n",
            "Epoch 68: val_loss improved from 0.40902 to 0.40481, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.3063 - mean_squared_error: 0.1933 - val_loss: 0.4048 - val_mean_squared_error: 0.2925\n",
            "Epoch 69/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3069 - mean_squared_error: 0.1831\n",
            "Epoch 69: val_loss improved from 0.40481 to 0.39430, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.3105 - mean_squared_error: 0.1914 - val_loss: 0.3943 - val_mean_squared_error: 0.2756\n",
            "Epoch 70/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3199 - mean_squared_error: 0.2102\n",
            "Epoch 70: val_loss did not improve from 0.39430\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3032 - mean_squared_error: 0.1882 - val_loss: 0.4035 - val_mean_squared_error: 0.3014\n",
            "Epoch 71/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3067 - mean_squared_error: 0.1912\n",
            "Epoch 71: val_loss did not improve from 0.39430\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3101 - mean_squared_error: 0.1955 - val_loss: 0.4101 - val_mean_squared_error: 0.2934\n",
            "Epoch 72/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3052 - mean_squared_error: 0.1892\n",
            "Epoch 72: val_loss did not improve from 0.39430\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3052 - mean_squared_error: 0.1892 - val_loss: 0.4028 - val_mean_squared_error: 0.2824\n",
            "Epoch 73/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2929 - mean_squared_error: 0.1586\n",
            "Epoch 73: val_loss did not improve from 0.39430\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3028 - mean_squared_error: 0.1875 - val_loss: 0.3965 - val_mean_squared_error: 0.2936\n",
            "Epoch 74/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2903 - mean_squared_error: 0.1639\n",
            "Epoch 74: val_loss improved from 0.39430 to 0.39364, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3032 - mean_squared_error: 0.1891 - val_loss: 0.3936 - val_mean_squared_error: 0.2869\n",
            "Epoch 75/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2929 - mean_squared_error: 0.1796\n",
            "Epoch 75: val_loss did not improve from 0.39364\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3009 - mean_squared_error: 0.1875 - val_loss: 0.3951 - val_mean_squared_error: 0.2826\n",
            "Epoch 76/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3078 - mean_squared_error: 0.1882\n",
            "Epoch 76: val_loss improved from 0.39364 to 0.38737, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3050 - mean_squared_error: 0.1839 - val_loss: 0.3874 - val_mean_squared_error: 0.2703\n",
            "Epoch 77/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3004 - mean_squared_error: 0.1823\n",
            "Epoch 77: val_loss did not improve from 0.38737\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3004 - mean_squared_error: 0.1823 - val_loss: 0.3911 - val_mean_squared_error: 0.2858\n",
            "Epoch 78/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.3509 - mean_squared_error: 0.2729\n",
            "Epoch 78: val_loss did not improve from 0.38737\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2997 - mean_squared_error: 0.1845 - val_loss: 0.3999 - val_mean_squared_error: 0.2859\n",
            "Epoch 79/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2958 - mean_squared_error: 0.1759\n",
            "Epoch 79: val_loss improved from 0.38737 to 0.38686, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.2999 - mean_squared_error: 0.1825 - val_loss: 0.3869 - val_mean_squared_error: 0.2752\n",
            "Epoch 80/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2989 - mean_squared_error: 0.1827\n",
            "Epoch 80: val_loss improved from 0.38686 to 0.38303, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2975 - mean_squared_error: 0.1812 - val_loss: 0.3830 - val_mean_squared_error: 0.2651\n",
            "Epoch 81/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.3052 - mean_squared_error: 0.2013\n",
            "Epoch 81: val_loss did not improve from 0.38303\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3002 - mean_squared_error: 0.1780 - val_loss: 0.3852 - val_mean_squared_error: 0.2693\n",
            "Epoch 82/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2936 - mean_squared_error: 0.1671\n",
            "Epoch 82: val_loss improved from 0.38303 to 0.38163, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2995 - mean_squared_error: 0.1799 - val_loss: 0.3816 - val_mean_squared_error: 0.2687\n",
            "Epoch 83/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2957 - mean_squared_error: 0.1771\n",
            "Epoch 83: val_loss did not improve from 0.38163\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2957 - mean_squared_error: 0.1771 - val_loss: 0.3826 - val_mean_squared_error: 0.2695\n",
            "Epoch 84/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2806 - mean_squared_error: 0.1574\n",
            "Epoch 84: val_loss improved from 0.38163 to 0.37472, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2953 - mean_squared_error: 0.1775 - val_loss: 0.3747 - val_mean_squared_error: 0.2559\n",
            "Epoch 85/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3000 - mean_squared_error: 0.1787\n",
            "Epoch 85: val_loss did not improve from 0.37472\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2982 - mean_squared_error: 0.1749 - val_loss: 0.3810 - val_mean_squared_error: 0.2543\n",
            "Epoch 86/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.1606\n",
            "Epoch 86: val_loss did not improve from 0.37472\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3013 - mean_squared_error: 0.1768 - val_loss: 0.3752 - val_mean_squared_error: 0.2536\n",
            "Epoch 87/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2974 - mean_squared_error: 0.1714\n",
            "Epoch 87: val_loss did not improve from 0.37472\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2974 - mean_squared_error: 0.1714 - val_loss: 0.3810 - val_mean_squared_error: 0.2473\n",
            "Epoch 88/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2783 - mean_squared_error: 0.1556\n",
            "Epoch 88: val_loss did not improve from 0.37472\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2943 - mean_squared_error: 0.1722 - val_loss: 0.3778 - val_mean_squared_error: 0.2643\n",
            "Epoch 89/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2931 - mean_squared_error: 0.1906\n",
            "Epoch 89: val_loss improved from 0.37472 to 0.37065, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.2924 - mean_squared_error: 0.1712 - val_loss: 0.3707 - val_mean_squared_error: 0.2432\n",
            "Epoch 90/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2855 - mean_squared_error: 0.1548\n",
            "Epoch 90: val_loss improved from 0.37065 to 0.36809, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2903 - mean_squared_error: 0.1682 - val_loss: 0.3681 - val_mean_squared_error: 0.2535\n",
            "Epoch 91/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.2429 - mean_squared_error: 0.1237\n",
            "Epoch 91: val_loss improved from 0.36809 to 0.36325, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2913 - mean_squared_error: 0.1700 - val_loss: 0.3632 - val_mean_squared_error: 0.2412\n",
            "Epoch 92/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2846 - mean_squared_error: 0.1565\n",
            "Epoch 92: val_loss did not improve from 0.36325\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2921 - mean_squared_error: 0.1664 - val_loss: 0.3730 - val_mean_squared_error: 0.2379\n",
            "Epoch 93/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2878 - mean_squared_error: 0.1708\n",
            "Epoch 93: val_loss did not improve from 0.36325\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2910 - mean_squared_error: 0.1677 - val_loss: 0.3755 - val_mean_squared_error: 0.2425\n",
            "Epoch 94/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2982 - mean_squared_error: 0.1784\n",
            "Epoch 94: val_loss did not improve from 0.36325\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2880 - mean_squared_error: 0.1635 - val_loss: 0.3659 - val_mean_squared_error: 0.2321\n",
            "Epoch 95/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2857 - mean_squared_error: 0.1606\n",
            "Epoch 95: val_loss improved from 0.36325 to 0.35926, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2893 - mean_squared_error: 0.1654 - val_loss: 0.3593 - val_mean_squared_error: 0.2278\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2859 - mean_squared_error: 0.1618\n",
            "Epoch 96: val_loss improved from 0.35926 to 0.35460, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2859 - mean_squared_error: 0.1618 - val_loss: 0.3546 - val_mean_squared_error: 0.2187\n",
            "Epoch 97/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2842 - mean_squared_error: 0.1616\n",
            "Epoch 97: val_loss did not improve from 0.35460\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2860 - mean_squared_error: 0.1614 - val_loss: 0.3566 - val_mean_squared_error: 0.2276\n",
            "Epoch 98/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2863 - mean_squared_error: 0.1629\n",
            "Epoch 98: val_loss did not improve from 0.35460\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2879 - mean_squared_error: 0.1643 - val_loss: 0.3647 - val_mean_squared_error: 0.2393\n",
            "Epoch 99/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1320\n",
            "Epoch 99: val_loss did not improve from 0.35460\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2907 - mean_squared_error: 0.1643 - val_loss: 0.3627 - val_mean_squared_error: 0.2223\n",
            "Epoch 100/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2969 - mean_squared_error: 0.1724\n",
            "Epoch 100: val_loss improved from 0.35460 to 0.34594, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2959 - mean_squared_error: 0.1654 - val_loss: 0.3459 - val_mean_squared_error: 0.2155\n",
            "Epoch 101/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2751 - mean_squared_error: 0.1534\n",
            "Epoch 101: val_loss improved from 0.34594 to 0.34459, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2839 - mean_squared_error: 0.1574 - val_loss: 0.3446 - val_mean_squared_error: 0.2192\n",
            "Epoch 102/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2822 - mean_squared_error: 0.1559\n",
            "Epoch 102: val_loss did not improve from 0.34459\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2842 - mean_squared_error: 0.1569 - val_loss: 0.3485 - val_mean_squared_error: 0.2124\n",
            "Epoch 103/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2754 - mean_squared_error: 0.1534\n",
            "Epoch 103: val_loss did not improve from 0.34459\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2829 - mean_squared_error: 0.1558 - val_loss: 0.3492 - val_mean_squared_error: 0.2156\n",
            "Epoch 104/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.1507\n",
            "Epoch 104: val_loss did not improve from 0.34459\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2811 - mean_squared_error: 0.1553 - val_loss: 0.3528 - val_mean_squared_error: 0.2167\n",
            "Epoch 105/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2736 - mean_squared_error: 0.1448\n",
            "Epoch 105: val_loss did not improve from 0.34459\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2810 - mean_squared_error: 0.1547 - val_loss: 0.3474 - val_mean_squared_error: 0.2079\n",
            "Epoch 106/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2829 - mean_squared_error: 0.1564\n",
            "Epoch 106: val_loss improved from 0.34459 to 0.33635, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2813 - mean_squared_error: 0.1536 - val_loss: 0.3364 - val_mean_squared_error: 0.2036\n",
            "Epoch 107/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2781 - mean_squared_error: 0.1469\n",
            "Epoch 107: val_loss did not improve from 0.33635\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2834 - mean_squared_error: 0.1544 - val_loss: 0.3405 - val_mean_squared_error: 0.2051\n",
            "Epoch 108/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2842 - mean_squared_error: 0.1569\n",
            "Epoch 108: val_loss did not improve from 0.33635\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2798 - mean_squared_error: 0.1525 - val_loss: 0.3529 - val_mean_squared_error: 0.2143\n",
            "Epoch 109/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.3049 - mean_squared_error: 0.1806\n",
            "Epoch 109: val_loss did not improve from 0.33635\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2817 - mean_squared_error: 0.1530 - val_loss: 0.3438 - val_mean_squared_error: 0.2048\n",
            "Epoch 110/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2817 - mean_squared_error: 0.1528\n",
            "Epoch 110: val_loss improved from 0.33635 to 0.33585, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2817 - mean_squared_error: 0.1528 - val_loss: 0.3359 - val_mean_squared_error: 0.2016\n",
            "Epoch 111/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.2821 - mean_squared_error: 0.1450\n",
            "Epoch 111: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2782 - mean_squared_error: 0.1505 - val_loss: 0.3504 - val_mean_squared_error: 0.2112\n",
            "Epoch 112/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2812 - mean_squared_error: 0.1513\n",
            "Epoch 112: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2812 - mean_squared_error: 0.1513 - val_loss: 0.3375 - val_mean_squared_error: 0.1956\n",
            "Epoch 113/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2707 - mean_squared_error: 0.1396\n",
            "Epoch 113: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2826 - mean_squared_error: 0.1511 - val_loss: 0.3398 - val_mean_squared_error: 0.2002\n",
            "Epoch 114/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2782 - mean_squared_error: 0.1497\n",
            "Epoch 114: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2782 - mean_squared_error: 0.1497 - val_loss: 0.3445 - val_mean_squared_error: 0.2108\n",
            "Epoch 115/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2761 - mean_squared_error: 0.1487\n",
            "Epoch 115: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2763 - mean_squared_error: 0.1489 - val_loss: 0.3448 - val_mean_squared_error: 0.2067\n",
            "Epoch 116/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2812 - mean_squared_error: 0.1500\n",
            "Epoch 116: val_loss did not improve from 0.33585\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2793 - mean_squared_error: 0.1501 - val_loss: 0.3446 - val_mean_squared_error: 0.1978\n",
            "Epoch 117/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2757 - mean_squared_error: 0.1471\n",
            "Epoch 117: val_loss improved from 0.33585 to 0.33437, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2757 - mean_squared_error: 0.1471 - val_loss: 0.3344 - val_mean_squared_error: 0.1990\n",
            "Epoch 118/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2816 - mean_squared_error: 0.1498\n",
            "Epoch 118: val_loss improved from 0.33437 to 0.33214, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2771 - mean_squared_error: 0.1463 - val_loss: 0.3321 - val_mean_squared_error: 0.1953\n",
            "Epoch 119/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2766 - mean_squared_error: 0.1479\n",
            "Epoch 119: val_loss did not improve from 0.33214\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2769 - mean_squared_error: 0.1471 - val_loss: 0.3489 - val_mean_squared_error: 0.2080\n",
            "Epoch 120/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2901 - mean_squared_error: 0.1538\n",
            "Epoch 120: val_loss improved from 0.33214 to 0.33119, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.2780 - mean_squared_error: 0.1456 - val_loss: 0.3312 - val_mean_squared_error: 0.1877\n",
            "Epoch 121/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2628 - mean_squared_error: 0.1325\n",
            "Epoch 121: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2749 - mean_squared_error: 0.1439 - val_loss: 0.3374 - val_mean_squared_error: 0.2127\n",
            "Epoch 122/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.1493\n",
            "Epoch 122: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.2758 - mean_squared_error: 0.1465 - val_loss: 0.3423 - val_mean_squared_error: 0.2159\n",
            "Epoch 123/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2826 - mean_squared_error: 0.1542\n",
            "Epoch 123: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2726 - mean_squared_error: 0.1444 - val_loss: 0.3322 - val_mean_squared_error: 0.1892\n",
            "Epoch 124/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2739 - mean_squared_error: 0.1424\n",
            "Epoch 124: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2739 - mean_squared_error: 0.1424 - val_loss: 0.3399 - val_mean_squared_error: 0.1945\n",
            "Epoch 125/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2854 - mean_squared_error: 0.1567\n",
            "Epoch 125: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2757 - mean_squared_error: 0.1443 - val_loss: 0.3374 - val_mean_squared_error: 0.2035\n",
            "Epoch 126/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2745 - mean_squared_error: 0.1441\n",
            "Epoch 126: val_loss did not improve from 0.33119\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2745 - mean_squared_error: 0.1441 - val_loss: 0.3313 - val_mean_squared_error: 0.2011\n",
            "Epoch 127/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.1354\n",
            "Epoch 127: val_loss improved from 0.33119 to 0.32329, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.2719 - mean_squared_error: 0.1407 - val_loss: 0.3233 - val_mean_squared_error: 0.1820\n",
            "Epoch 128/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2849 - mean_squared_error: 0.1678\n",
            "Epoch 128: val_loss did not improve from 0.32329\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2719 - mean_squared_error: 0.1401 - val_loss: 0.3416 - val_mean_squared_error: 0.2001\n",
            "Epoch 129/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2743 - mean_squared_error: 0.1447\n",
            "Epoch 129: val_loss improved from 0.32329 to 0.31684, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 57ms/step - loss: 0.2743 - mean_squared_error: 0.1409 - val_loss: 0.3168 - val_mean_squared_error: 0.1824\n",
            "Epoch 130/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2766 - mean_squared_error: 0.1425\n",
            "Epoch 130: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 0.2711 - mean_squared_error: 0.1387 - val_loss: 0.3227 - val_mean_squared_error: 0.1879\n",
            "Epoch 131/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2717 - mean_squared_error: 0.1391\n",
            "Epoch 131: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 0.2717 - mean_squared_error: 0.1391 - val_loss: 0.3334 - val_mean_squared_error: 0.1852\n",
            "Epoch 132/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2727 - mean_squared_error: 0.1390\n",
            "Epoch 132: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2727 - mean_squared_error: 0.1390 - val_loss: 0.3325 - val_mean_squared_error: 0.1950\n",
            "Epoch 133/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2689 - mean_squared_error: 0.1360\n",
            "Epoch 133: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2689 - mean_squared_error: 0.1360 - val_loss: 0.3198 - val_mean_squared_error: 0.1795\n",
            "Epoch 134/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2714 - mean_squared_error: 0.1381\n",
            "Epoch 134: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2694 - mean_squared_error: 0.1359 - val_loss: 0.3314 - val_mean_squared_error: 0.1929\n",
            "Epoch 135/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2481 - mean_squared_error: 0.1235\n",
            "Epoch 135: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2671 - mean_squared_error: 0.1353 - val_loss: 0.3312 - val_mean_squared_error: 0.1879\n",
            "Epoch 136/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2712 - mean_squared_error: 0.1398\n",
            "Epoch 136: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2675 - mean_squared_error: 0.1359 - val_loss: 0.3288 - val_mean_squared_error: 0.1903\n",
            "Epoch 137/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2578 - mean_squared_error: 0.1327\n",
            "Epoch 137: val_loss did not improve from 0.31684\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2662 - mean_squared_error: 0.1347 - val_loss: 0.3349 - val_mean_squared_error: 0.1918\n",
            "Epoch 138/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2714 - mean_squared_error: 0.1365\n",
            "Epoch 138: val_loss improved from 0.31684 to 0.31588, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2664 - mean_squared_error: 0.1334 - val_loss: 0.3159 - val_mean_squared_error: 0.1715\n",
            "Epoch 139/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2664 - mean_squared_error: 0.1339\n",
            "Epoch 139: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2664 - mean_squared_error: 0.1339 - val_loss: 0.3276 - val_mean_squared_error: 0.1977\n",
            "Epoch 140/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2588 - mean_squared_error: 0.1275\n",
            "Epoch 140: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2643 - mean_squared_error: 0.1323 - val_loss: 0.3248 - val_mean_squared_error: 0.1849\n",
            "Epoch 141/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.1284\n",
            "Epoch 141: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2632 - mean_squared_error: 0.1313 - val_loss: 0.3281 - val_mean_squared_error: 0.1815\n",
            "Epoch 142/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2620 - mean_squared_error: 0.1304\n",
            "Epoch 142: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2620 - mean_squared_error: 0.1304 - val_loss: 0.3256 - val_mean_squared_error: 0.1842\n",
            "Epoch 143/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.1306\n",
            "Epoch 143: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2626 - mean_squared_error: 0.1306 - val_loss: 0.3231 - val_mean_squared_error: 0.1809\n",
            "Epoch 144/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.1303\n",
            "Epoch 144: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2635 - mean_squared_error: 0.1303 - val_loss: 0.3248 - val_mean_squared_error: 0.1840\n",
            "Epoch 145/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2620 - mean_squared_error: 0.1296\n",
            "Epoch 145: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2620 - mean_squared_error: 0.1296 - val_loss: 0.3323 - val_mean_squared_error: 0.1946\n",
            "Epoch 146/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2618 - mean_squared_error: 0.1317\n",
            "Epoch 146: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2616 - mean_squared_error: 0.1293 - val_loss: 0.3242 - val_mean_squared_error: 0.1795\n",
            "Epoch 147/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2656 - mean_squared_error: 0.1322\n",
            "Epoch 147: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2656 - mean_squared_error: 0.1322 - val_loss: 0.3244 - val_mean_squared_error: 0.1863\n",
            "Epoch 148/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2559 - mean_squared_error: 0.1225\n",
            "Epoch 148: val_loss did not improve from 0.31588\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2619 - mean_squared_error: 0.1271 - val_loss: 0.3248 - val_mean_squared_error: 0.1828\n",
            "Epoch 149/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2637 - mean_squared_error: 0.1308\n",
            "Epoch 149: val_loss improved from 0.31588 to 0.31162, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.2629 - mean_squared_error: 0.1283 - val_loss: 0.3116 - val_mean_squared_error: 0.1633\n",
            "Epoch 150/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2526 - mean_squared_error: 0.1170\n",
            "Epoch 150: val_loss did not improve from 0.31162\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2608 - mean_squared_error: 0.1281 - val_loss: 0.3212 - val_mean_squared_error: 0.1908\n",
            "Epoch 151/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2508 - mean_squared_error: 0.1247\n",
            "Epoch 151: val_loss did not improve from 0.31162\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2593 - mean_squared_error: 0.1264 - val_loss: 0.3305 - val_mean_squared_error: 0.1953\n",
            "Epoch 152/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2462 - mean_squared_error: 0.1158\n",
            "Epoch 152: val_loss did not improve from 0.31162\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2616 - mean_squared_error: 0.1261 - val_loss: 0.3353 - val_mean_squared_error: 0.1977\n",
            "Epoch 153/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2571 - mean_squared_error: 0.1256\n",
            "Epoch 153: val_loss improved from 0.31162 to 0.30520, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 0.2571 - mean_squared_error: 0.1256 - val_loss: 0.3052 - val_mean_squared_error: 0.1595\n",
            "Epoch 154/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2687 - mean_squared_error: 0.1319\n",
            "Epoch 154: val_loss did not improve from 0.30520\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2605 - mean_squared_error: 0.1252 - val_loss: 0.3293 - val_mean_squared_error: 0.1944\n",
            "Epoch 155/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1255\n",
            "Epoch 155: val_loss did not improve from 0.30520\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2611 - mean_squared_error: 0.1255 - val_loss: 0.3201 - val_mean_squared_error: 0.1753\n",
            "Epoch 156/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2537 - mean_squared_error: 0.1234\n",
            "Epoch 156: val_loss did not improve from 0.30520\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2556 - mean_squared_error: 0.1229 - val_loss: 0.3067 - val_mean_squared_error: 0.1761\n",
            "Epoch 157/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2570 - mean_squared_error: 0.1239\n",
            "Epoch 157: val_loss improved from 0.30520 to 0.29872, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2550 - mean_squared_error: 0.1223 - val_loss: 0.2987 - val_mean_squared_error: 0.1618\n",
            "Epoch 158/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2477 - mean_squared_error: 0.1127\n",
            "Epoch 158: val_loss did not improve from 0.29872\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2556 - mean_squared_error: 0.1214 - val_loss: 0.3067 - val_mean_squared_error: 0.1652\n",
            "Epoch 159/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2502 - mean_squared_error: 0.1198\n",
            "Epoch 159: val_loss did not improve from 0.29872\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2543 - mean_squared_error: 0.1223 - val_loss: 0.3007 - val_mean_squared_error: 0.1703\n",
            "Epoch 160/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2551 - mean_squared_error: 0.1206\n",
            "Epoch 160: val_loss did not improve from 0.29872\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2551 - mean_squared_error: 0.1206 - val_loss: 0.3021 - val_mean_squared_error: 0.1679\n",
            "Epoch 161/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2572 - mean_squared_error: 0.1232\n",
            "Epoch 161: val_loss improved from 0.29872 to 0.28479, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2560 - mean_squared_error: 0.1206 - val_loss: 0.2848 - val_mean_squared_error: 0.1524\n",
            "Epoch 162/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2567 - mean_squared_error: 0.1219\n",
            "Epoch 162: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2527 - mean_squared_error: 0.1199 - val_loss: 0.3210 - val_mean_squared_error: 0.1799\n",
            "Epoch 163/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2525 - mean_squared_error: 0.1259\n",
            "Epoch 163: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2500 - mean_squared_error: 0.1186 - val_loss: 0.2962 - val_mean_squared_error: 0.1603\n",
            "Epoch 164/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2476 - mean_squared_error: 0.1165\n",
            "Epoch 164: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2485 - mean_squared_error: 0.1157 - val_loss: 0.3029 - val_mean_squared_error: 0.1624\n",
            "Epoch 165/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2522 - mean_squared_error: 0.1168\n",
            "Epoch 165: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2522 - mean_squared_error: 0.1168 - val_loss: 0.2859 - val_mean_squared_error: 0.1478\n",
            "Epoch 166/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2471 - mean_squared_error: 0.1154\n",
            "Epoch 166: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2501 - mean_squared_error: 0.1171 - val_loss: 0.2986 - val_mean_squared_error: 0.1575\n",
            "Epoch 167/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2486 - mean_squared_error: 0.1156\n",
            "Epoch 167: val_loss did not improve from 0.28479\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2486 - mean_squared_error: 0.1156 - val_loss: 0.3047 - val_mean_squared_error: 0.1665\n",
            "Epoch 168/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2497 - mean_squared_error: 0.1179\n",
            "Epoch 168: val_loss improved from 0.28479 to 0.28216, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2449 - mean_squared_error: 0.1136 - val_loss: 0.2822 - val_mean_squared_error: 0.1511\n",
            "Epoch 169/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2442 - mean_squared_error: 0.1122\n",
            "Epoch 169: val_loss did not improve from 0.28216\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2443 - mean_squared_error: 0.1130 - val_loss: 0.3510 - val_mean_squared_error: 0.1880\n",
            "Epoch 170/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2602 - mean_squared_error: 0.1210\n",
            "Epoch 170: val_loss improved from 0.28216 to 0.27606, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2571 - mean_squared_error: 0.1192 - val_loss: 0.2761 - val_mean_squared_error: 0.1361\n",
            "Epoch 171/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2574 - mean_squared_error: 0.1231\n",
            "Epoch 171: val_loss did not improve from 0.27606\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2524 - mean_squared_error: 0.1158 - val_loss: 0.3146 - val_mean_squared_error: 0.1731\n",
            "Epoch 172/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2504 - mean_squared_error: 0.1192\n",
            "Epoch 172: val_loss did not improve from 0.27606\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2467 - mean_squared_error: 0.1148 - val_loss: 0.3299 - val_mean_squared_error: 0.1749\n",
            "Epoch 173/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2563 - mean_squared_error: 0.1192\n",
            "Epoch 173: val_loss did not improve from 0.27606\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2448 - mean_squared_error: 0.1131 - val_loss: 0.2932 - val_mean_squared_error: 0.1508\n",
            "Epoch 174/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2475 - mean_squared_error: 0.1121\n",
            "Epoch 174: val_loss did not improve from 0.27606\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2475 - mean_squared_error: 0.1121 - val_loss: 0.3010 - val_mean_squared_error: 0.1545\n",
            "Epoch 175/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2458 - mean_squared_error: 0.1139\n",
            "Epoch 175: val_loss improved from 0.27606 to 0.27500, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2457 - mean_squared_error: 0.1129 - val_loss: 0.2750 - val_mean_squared_error: 0.1463\n",
            "Epoch 176/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2444 - mean_squared_error: 0.1098\n",
            "Epoch 176: val_loss did not improve from 0.27500\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2444 - mean_squared_error: 0.1098 - val_loss: 0.3167 - val_mean_squared_error: 0.1710\n",
            "Epoch 177/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2428 - mean_squared_error: 0.1084\n",
            "Epoch 177: val_loss improved from 0.27500 to 0.27365, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2433 - mean_squared_error: 0.1096 - val_loss: 0.2736 - val_mean_squared_error: 0.1424\n",
            "Epoch 178/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2407 - mean_squared_error: 0.1105\n",
            "Epoch 178: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2407 - mean_squared_error: 0.1105 - val_loss: 0.2826 - val_mean_squared_error: 0.1505\n",
            "Epoch 179/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2445 - mean_squared_error: 0.1127\n",
            "Epoch 179: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2398 - mean_squared_error: 0.1094 - val_loss: 0.2737 - val_mean_squared_error: 0.1474\n",
            "Epoch 180/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2458 - mean_squared_error: 0.1134\n",
            "Epoch 180: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2390 - mean_squared_error: 0.1081 - val_loss: 0.2934 - val_mean_squared_error: 0.1500\n",
            "Epoch 181/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2562 - mean_squared_error: 0.1235\n",
            "Epoch 181: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2420 - mean_squared_error: 0.1113 - val_loss: 0.3073 - val_mean_squared_error: 0.1613\n",
            "Epoch 182/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.1136\n",
            "Epoch 182: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2483 - mean_squared_error: 0.1116 - val_loss: 0.2748 - val_mean_squared_error: 0.1462\n",
            "Epoch 183/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2417 - mean_squared_error: 0.1088\n",
            "Epoch 183: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2417 - mean_squared_error: 0.1088 - val_loss: 0.3066 - val_mean_squared_error: 0.1642\n",
            "Epoch 184/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2437 - mean_squared_error: 0.1098\n",
            "Epoch 184: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2432 - mean_squared_error: 0.1090 - val_loss: 0.2810 - val_mean_squared_error: 0.1453\n",
            "Epoch 185/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2385 - mean_squared_error: 0.1084\n",
            "Epoch 185: val_loss did not improve from 0.27365\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2367 - mean_squared_error: 0.1054 - val_loss: 0.3201 - val_mean_squared_error: 0.1661\n",
            "Epoch 186/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2418 - mean_squared_error: 0.1098\n",
            "Epoch 186: val_loss improved from 0.27365 to 0.25421, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.2418 - mean_squared_error: 0.1098 - val_loss: 0.2542 - val_mean_squared_error: 0.1296\n",
            "Epoch 187/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2358 - mean_squared_error: 0.1067\n",
            "Epoch 187: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2355 - mean_squared_error: 0.1055 - val_loss: 0.3148 - val_mean_squared_error: 0.1684\n",
            "Epoch 188/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2360 - mean_squared_error: 0.1069\n",
            "Epoch 188: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2392 - mean_squared_error: 0.1079 - val_loss: 0.2667 - val_mean_squared_error: 0.1346\n",
            "Epoch 189/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2490 - mean_squared_error: 0.1118\n",
            "Epoch 189: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2457 - mean_squared_error: 0.1089 - val_loss: 0.3032 - val_mean_squared_error: 0.1596\n",
            "Epoch 190/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2332 - mean_squared_error: 0.1113\n",
            "Epoch 190: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2357 - mean_squared_error: 0.1037 - val_loss: 0.2904 - val_mean_squared_error: 0.1541\n",
            "Epoch 191/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2278 - mean_squared_error: 0.0910\n",
            "Epoch 191: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2374 - mean_squared_error: 0.1030 - val_loss: 0.2768 - val_mean_squared_error: 0.1434\n",
            "Epoch 192/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2376 - mean_squared_error: 0.1053\n",
            "Epoch 192: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2376 - mean_squared_error: 0.1053 - val_loss: 0.2798 - val_mean_squared_error: 0.1475\n",
            "Epoch 193/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2339 - mean_squared_error: 0.0995\n",
            "Epoch 193: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2349 - mean_squared_error: 0.1042 - val_loss: 0.2951 - val_mean_squared_error: 0.1541\n",
            "Epoch 194/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2357 - mean_squared_error: 0.1042\n",
            "Epoch 194: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2317 - mean_squared_error: 0.1014 - val_loss: 0.2729 - val_mean_squared_error: 0.1387\n",
            "Epoch 195/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2289 - mean_squared_error: 0.0972\n",
            "Epoch 195: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2331 - mean_squared_error: 0.1020 - val_loss: 0.3078 - val_mean_squared_error: 0.1569\n",
            "Epoch 196/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.0896\n",
            "Epoch 196: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2365 - mean_squared_error: 0.1033 - val_loss: 0.2552 - val_mean_squared_error: 0.1344\n",
            "Epoch 197/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2317 - mean_squared_error: 0.1016\n",
            "Epoch 197: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2317 - mean_squared_error: 0.1016 - val_loss: 0.2934 - val_mean_squared_error: 0.1542\n",
            "Epoch 198/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2281 - mean_squared_error: 0.1019\n",
            "Epoch 198: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2303 - mean_squared_error: 0.1017 - val_loss: 0.2906 - val_mean_squared_error: 0.1489\n",
            "Epoch 199/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2371 - mean_squared_error: 0.1041\n",
            "Epoch 199: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2325 - mean_squared_error: 0.1009 - val_loss: 0.2855 - val_mean_squared_error: 0.1448\n",
            "Epoch 200/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2189 - mean_squared_error: 0.0873\n",
            "Epoch 200: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2332 - mean_squared_error: 0.1019 - val_loss: 0.2932 - val_mean_squared_error: 0.1510\n",
            "Epoch 201/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2247 - mean_squared_error: 0.0968\n",
            "Epoch 201: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2281 - mean_squared_error: 0.0992 - val_loss: 0.2728 - val_mean_squared_error: 0.1413\n",
            "Epoch 202/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2356 - mean_squared_error: 0.1071\n",
            "Epoch 202: val_loss did not improve from 0.25421\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2290 - mean_squared_error: 0.0998 - val_loss: 0.3035 - val_mean_squared_error: 0.1628\n",
            "Epoch 203/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2337 - mean_squared_error: 0.1025\n",
            "Epoch 203: val_loss improved from 0.25421 to 0.24260, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2337 - mean_squared_error: 0.1025 - val_loss: 0.2426 - val_mean_squared_error: 0.1230\n",
            "Epoch 204/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2367 - mean_squared_error: 0.1004\n",
            "Epoch 204: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2327 - mean_squared_error: 0.1007 - val_loss: 0.3151 - val_mean_squared_error: 0.1688\n",
            "Epoch 205/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2381 - mean_squared_error: 0.1051\n",
            "Epoch 205: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2345 - mean_squared_error: 0.0983 - val_loss: 0.2768 - val_mean_squared_error: 0.1379\n",
            "Epoch 206/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2348 - mean_squared_error: 0.1040\n",
            "Epoch 206: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2312 - mean_squared_error: 0.1013 - val_loss: 0.3134 - val_mean_squared_error: 0.1687\n",
            "Epoch 207/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2292 - mean_squared_error: 0.1018\n",
            "Epoch 207: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2284 - mean_squared_error: 0.1004 - val_loss: 0.2639 - val_mean_squared_error: 0.1365\n",
            "Epoch 208/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2299 - mean_squared_error: 0.0984\n",
            "Epoch 208: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.2299 - mean_squared_error: 0.0984 - val_loss: 0.2671 - val_mean_squared_error: 0.1365\n",
            "Epoch 209/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2249 - mean_squared_error: 0.0978\n",
            "Epoch 209: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.2318 - mean_squared_error: 0.1019 - val_loss: 0.3018 - val_mean_squared_error: 0.1576\n",
            "Epoch 210/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.1101\n",
            "Epoch 210: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 0.2417 - mean_squared_error: 0.1068 - val_loss: 0.2932 - val_mean_squared_error: 0.1543\n",
            "Epoch 211/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2358 - mean_squared_error: 0.1074\n",
            "Epoch 211: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2393 - mean_squared_error: 0.1031 - val_loss: 0.2776 - val_mean_squared_error: 0.1426\n",
            "Epoch 212/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2277 - mean_squared_error: 0.0977\n",
            "Epoch 212: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2277 - mean_squared_error: 0.0977 - val_loss: 0.3119 - val_mean_squared_error: 0.1577\n",
            "Epoch 213/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2321 - mean_squared_error: 0.1001\n",
            "Epoch 213: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2275 - mean_squared_error: 0.0971 - val_loss: 0.2849 - val_mean_squared_error: 0.1425\n",
            "Epoch 214/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2274 - mean_squared_error: 0.0943\n",
            "Epoch 214: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2262 - mean_squared_error: 0.0962 - val_loss: 0.2675 - val_mean_squared_error: 0.1354\n",
            "Epoch 215/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2332 - mean_squared_error: 0.0937\n",
            "Epoch 215: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2290 - mean_squared_error: 0.0975 - val_loss: 0.2828 - val_mean_squared_error: 0.1485\n",
            "Epoch 216/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2224 - mean_squared_error: 0.0949\n",
            "Epoch 216: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2252 - mean_squared_error: 0.0958 - val_loss: 0.2695 - val_mean_squared_error: 0.1392\n",
            "Epoch 217/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2202 - mean_squared_error: 0.0983\n",
            "Epoch 217: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2251 - mean_squared_error: 0.0958 - val_loss: 0.2686 - val_mean_squared_error: 0.1348\n",
            "Epoch 218/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2359 - mean_squared_error: 0.1012\n",
            "Epoch 218: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2299 - mean_squared_error: 0.0974 - val_loss: 0.3295 - val_mean_squared_error: 0.1737\n",
            "Epoch 219/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2338 - mean_squared_error: 0.0985\n",
            "Epoch 219: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2338 - mean_squared_error: 0.0985 - val_loss: 0.2458 - val_mean_squared_error: 0.1229\n",
            "Epoch 220/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2095 - mean_squared_error: 0.0786\n",
            "Epoch 220: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2235 - mean_squared_error: 0.0955 - val_loss: 0.2991 - val_mean_squared_error: 0.1516\n",
            "Epoch 221/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2328 - mean_squared_error: 0.1007\n",
            "Epoch 221: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2244 - mean_squared_error: 0.0944 - val_loss: 0.2663 - val_mean_squared_error: 0.1321\n",
            "Epoch 222/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2216 - mean_squared_error: 0.0890\n",
            "Epoch 222: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2239 - mean_squared_error: 0.0940 - val_loss: 0.2776 - val_mean_squared_error: 0.1453\n",
            "Epoch 223/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2434 - mean_squared_error: 0.1052\n",
            "Epoch 223: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2224 - mean_squared_error: 0.0938 - val_loss: 0.2730 - val_mean_squared_error: 0.1354\n",
            "Epoch 224/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2011 - mean_squared_error: 0.0746\n",
            "Epoch 224: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2209 - mean_squared_error: 0.0928 - val_loss: 0.2807 - val_mean_squared_error: 0.1438\n",
            "Epoch 225/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2072 - mean_squared_error: 0.0878\n",
            "Epoch 225: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2242 - mean_squared_error: 0.0947 - val_loss: 0.2705 - val_mean_squared_error: 0.1387\n",
            "Epoch 226/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2206 - mean_squared_error: 0.0919\n",
            "Epoch 226: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2200 - mean_squared_error: 0.0926 - val_loss: 0.2866 - val_mean_squared_error: 0.1445\n",
            "Epoch 227/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2076 - mean_squared_error: 0.0799\n",
            "Epoch 227: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2245 - mean_squared_error: 0.0939 - val_loss: 0.2506 - val_mean_squared_error: 0.1164\n",
            "Epoch 228/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2326 - mean_squared_error: 0.0988\n",
            "Epoch 228: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2287 - mean_squared_error: 0.0960 - val_loss: 0.2789 - val_mean_squared_error: 0.1434\n",
            "Epoch 229/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2327 - mean_squared_error: 0.0986\n",
            "Epoch 229: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2220 - mean_squared_error: 0.0926 - val_loss: 0.2442 - val_mean_squared_error: 0.1193\n",
            "Epoch 230/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.0933\n",
            "Epoch 230: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2248 - mean_squared_error: 0.0933 - val_loss: 0.3033 - val_mean_squared_error: 0.1548\n",
            "Epoch 231/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2260 - mean_squared_error: 0.0905\n",
            "Epoch 231: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2206 - mean_squared_error: 0.0909 - val_loss: 0.2525 - val_mean_squared_error: 0.1283\n",
            "Epoch 232/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2155 - mean_squared_error: 0.0891\n",
            "Epoch 232: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2188 - mean_squared_error: 0.0915 - val_loss: 0.2829 - val_mean_squared_error: 0.1478\n",
            "Epoch 233/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2181 - mean_squared_error: 0.0908\n",
            "Epoch 233: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2181 - mean_squared_error: 0.0908 - val_loss: 0.2508 - val_mean_squared_error: 0.1291\n",
            "Epoch 234/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2161 - mean_squared_error: 0.0876\n",
            "Epoch 234: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2184 - mean_squared_error: 0.0921 - val_loss: 0.2690 - val_mean_squared_error: 0.1392\n",
            "Epoch 235/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2221 - mean_squared_error: 0.0924\n",
            "Epoch 235: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2221 - mean_squared_error: 0.0924 - val_loss: 0.2765 - val_mean_squared_error: 0.1416\n",
            "Epoch 236/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2193 - mean_squared_error: 0.0915\n",
            "Epoch 236: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2193 - mean_squared_error: 0.0915 - val_loss: 0.2542 - val_mean_squared_error: 0.1260\n",
            "Epoch 237/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2286 - mean_squared_error: 0.0988\n",
            "Epoch 237: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2192 - mean_squared_error: 0.0925 - val_loss: 0.2761 - val_mean_squared_error: 0.1430\n",
            "Epoch 238/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2183 - mean_squared_error: 0.0899\n",
            "Epoch 238: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2195 - mean_squared_error: 0.0906 - val_loss: 0.2430 - val_mean_squared_error: 0.1176\n",
            "Epoch 239/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2057 - mean_squared_error: 0.0810\n",
            "Epoch 239: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2175 - mean_squared_error: 0.0896 - val_loss: 0.2731 - val_mean_squared_error: 0.1381\n",
            "Epoch 240/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2235 - mean_squared_error: 0.0937\n",
            "Epoch 240: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2165 - mean_squared_error: 0.0894 - val_loss: 0.2589 - val_mean_squared_error: 0.1294\n",
            "Epoch 241/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2236 - mean_squared_error: 0.0945\n",
            "Epoch 241: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2209 - mean_squared_error: 0.0909 - val_loss: 0.2771 - val_mean_squared_error: 0.1409\n",
            "Epoch 242/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2255 - mean_squared_error: 0.0951\n",
            "Epoch 242: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2195 - mean_squared_error: 0.0894 - val_loss: 0.2763 - val_mean_squared_error: 0.1387\n",
            "Epoch 243/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2004 - mean_squared_error: 0.0727\n",
            "Epoch 243: val_loss did not improve from 0.24260\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2203 - mean_squared_error: 0.0897 - val_loss: 0.2723 - val_mean_squared_error: 0.1338\n",
            "1/1 [==============================] - 0s 195ms/step\n",
            "Cycle:  3\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 2.1672 - mean_squared_error: 11.9345\n",
            "Epoch 1: val_loss improved from inf to 1.89907, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 5s 84ms/step - loss: 1.9699 - mean_squared_error: 9.4201 - val_loss: 1.8991 - val_mean_squared_error: 5.6873\n",
            "Epoch 2/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 1.3959 - mean_squared_error: 2.8251\n",
            "Epoch 2: val_loss improved from 1.89907 to 1.52796, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 1.3926 - mean_squared_error: 2.8038 - val_loss: 1.5280 - val_mean_squared_error: 2.8541\n",
            "Epoch 3/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.2247 - mean_squared_error: 2.0548\n",
            "Epoch 3: val_loss improved from 1.52796 to 1.38335, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 1.2145 - mean_squared_error: 2.0949 - val_loss: 1.3833 - val_mean_squared_error: 2.6809\n",
            "Epoch 4/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0372 - mean_squared_error: 1.8002\n",
            "Epoch 4: val_loss improved from 1.38335 to 1.24673, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 1.0372 - mean_squared_error: 1.8002 - val_loss: 1.2467 - val_mean_squared_error: 2.3122\n",
            "Epoch 5/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9095 - mean_squared_error: 1.3795\n",
            "Epoch 5: val_loss improved from 1.24673 to 1.08133, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.9095 - mean_squared_error: 1.3795 - val_loss: 1.0813 - val_mean_squared_error: 1.5765\n",
            "Epoch 6/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.8265 - mean_squared_error: 1.0313\n",
            "Epoch 6: val_loss improved from 1.08133 to 0.94231, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.8082 - mean_squared_error: 1.0064 - val_loss: 0.9423 - val_mean_squared_error: 1.2207\n",
            "Epoch 7/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.7273 - mean_squared_error: 0.8592\n",
            "Epoch 7: val_loss improved from 0.94231 to 0.82408, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.7265 - mean_squared_error: 0.8551 - val_loss: 0.8241 - val_mean_squared_error: 1.0351\n",
            "Epoch 8/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.6447 - mean_squared_error: 0.6987\n",
            "Epoch 8: val_loss improved from 0.82408 to 0.71582, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.6504 - mean_squared_error: 0.7056 - val_loss: 0.7158 - val_mean_squared_error: 0.7686\n",
            "Epoch 9/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5922 - mean_squared_error: 0.6242\n",
            "Epoch 9: val_loss improved from 0.71582 to 0.64396, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.5922 - mean_squared_error: 0.6242 - val_loss: 0.6440 - val_mean_squared_error: 0.6079\n",
            "Epoch 10/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.5403 - mean_squared_error: 0.5437\n",
            "Epoch 10: val_loss improved from 0.64396 to 0.59722, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.5353 - mean_squared_error: 0.5299 - val_loss: 0.5972 - val_mean_squared_error: 0.5154\n",
            "Epoch 11/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.4649 - mean_squared_error: 0.3600\n",
            "Epoch 11: val_loss improved from 0.59722 to 0.53222, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.4861 - mean_squared_error: 0.4209 - val_loss: 0.5322 - val_mean_squared_error: 0.4270\n",
            "Epoch 12/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4563 - mean_squared_error: 0.3650\n",
            "Epoch 12: val_loss improved from 0.53222 to 0.49001, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.4559 - mean_squared_error: 0.3632 - val_loss: 0.4900 - val_mean_squared_error: 0.3713\n",
            "Epoch 13/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.4415 - mean_squared_error: 0.3642\n",
            "Epoch 13: val_loss improved from 0.49001 to 0.46856, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.4370 - mean_squared_error: 0.3560 - val_loss: 0.4686 - val_mean_squared_error: 0.3369\n",
            "Epoch 14/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4153 - mean_squared_error: 0.3229\n",
            "Epoch 14: val_loss improved from 0.46856 to 0.46427, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.4153 - mean_squared_error: 0.3229 - val_loss: 0.4643 - val_mean_squared_error: 0.3330\n",
            "Epoch 15/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3730 - mean_squared_error: 0.2828\n",
            "Epoch 15: val_loss improved from 0.46427 to 0.44154, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.3798 - mean_squared_error: 0.2795 - val_loss: 0.4415 - val_mean_squared_error: 0.2952\n",
            "Epoch 16/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4930 - mean_squared_error: 0.5280\n",
            "Epoch 16: val_loss did not improve from 0.44154\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.3651 - mean_squared_error: 0.2595 - val_loss: 0.4861 - val_mean_squared_error: 0.3472\n",
            "Epoch 17/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3865 - mean_squared_error: 0.2800\n",
            "Epoch 17: val_loss did not improve from 0.44154\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3853 - mean_squared_error: 0.2715 - val_loss: 0.4693 - val_mean_squared_error: 0.3169\n",
            "Epoch 18/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3610 - mean_squared_error: 0.2473\n",
            "Epoch 18: val_loss improved from 0.44154 to 0.42366, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3571 - mean_squared_error: 0.2418 - val_loss: 0.4237 - val_mean_squared_error: 0.2638\n",
            "Epoch 19/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3480 - mean_squared_error: 0.2245\n",
            "Epoch 19: val_loss improved from 0.42366 to 0.39665, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.3575 - mean_squared_error: 0.2392 - val_loss: 0.3966 - val_mean_squared_error: 0.2285\n",
            "Epoch 20/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3944 - mean_squared_error: 0.2826\n",
            "Epoch 20: val_loss did not improve from 0.39665\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.3944 - mean_squared_error: 0.2826 - val_loss: 0.3992 - val_mean_squared_error: 0.2320\n",
            "Epoch 21/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3913 - mean_squared_error: 0.2732\n",
            "Epoch 21: val_loss did not improve from 0.39665\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3913 - mean_squared_error: 0.2732 - val_loss: 0.4649 - val_mean_squared_error: 0.3241\n",
            "Epoch 22/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.4208 - mean_squared_error: 0.3129\n",
            "Epoch 22: val_loss did not improve from 0.39665\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3767 - mean_squared_error: 0.2519 - val_loss: 0.4239 - val_mean_squared_error: 0.2526\n",
            "Epoch 23/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3465 - mean_squared_error: 0.2138\n",
            "Epoch 23: val_loss did not improve from 0.39665\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.3465 - mean_squared_error: 0.2138 - val_loss: 0.4266 - val_mean_squared_error: 0.2647\n",
            "Epoch 24/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3412 - mean_squared_error: 0.2060\n",
            "Epoch 24: val_loss improved from 0.39665 to 0.38393, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 0.3412 - mean_squared_error: 0.2060 - val_loss: 0.3839 - val_mean_squared_error: 0.2140\n",
            "Epoch 25/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3463 - mean_squared_error: 0.2157\n",
            "Epoch 25: val_loss improved from 0.38393 to 0.37871, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 1s 62ms/step - loss: 0.3382 - mean_squared_error: 0.2085 - val_loss: 0.3787 - val_mean_squared_error: 0.2001\n",
            "Epoch 26/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.3262 - mean_squared_error: 0.1957\n",
            "Epoch 26: val_loss did not improve from 0.37871\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.3165 - mean_squared_error: 0.1859 - val_loss: 0.3988 - val_mean_squared_error: 0.2348\n",
            "Epoch 27/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3001 - mean_squared_error: 0.1703\n",
            "Epoch 27: val_loss did not improve from 0.37871\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.3113 - mean_squared_error: 0.1795 - val_loss: 0.3793 - val_mean_squared_error: 0.2004\n",
            "Epoch 28/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3186 - mean_squared_error: 0.1847\n",
            "Epoch 28: val_loss improved from 0.37871 to 0.35469, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3198 - mean_squared_error: 0.1831 - val_loss: 0.3547 - val_mean_squared_error: 0.1829\n",
            "Epoch 29/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3224 - mean_squared_error: 0.1954\n",
            "Epoch 29: val_loss improved from 0.35469 to 0.33499, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3088 - mean_squared_error: 0.1751 - val_loss: 0.3350 - val_mean_squared_error: 0.1657\n",
            "Epoch 30/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3158 - mean_squared_error: 0.1783\n",
            "Epoch 30: val_loss did not improve from 0.33499\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3132 - mean_squared_error: 0.1789 - val_loss: 0.3595 - val_mean_squared_error: 0.1801\n",
            "Epoch 31/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3382 - mean_squared_error: 0.1881\n",
            "Epoch 31: val_loss improved from 0.33499 to 0.32897, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.3429 - mean_squared_error: 0.1939 - val_loss: 0.3290 - val_mean_squared_error: 0.1626\n",
            "Epoch 32/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2980 - mean_squared_error: 0.1621\n",
            "Epoch 32: val_loss did not improve from 0.32897\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2980 - mean_squared_error: 0.1621 - val_loss: 0.3422 - val_mean_squared_error: 0.1659\n",
            "Epoch 33/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2987 - mean_squared_error: 0.1584\n",
            "Epoch 33: val_loss did not improve from 0.32897\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2987 - mean_squared_error: 0.1584 - val_loss: 0.3623 - val_mean_squared_error: 0.1800\n",
            "Epoch 34/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2965 - mean_squared_error: 0.1477\n",
            "Epoch 34: val_loss improved from 0.32897 to 0.31672, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2904 - mean_squared_error: 0.1519 - val_loss: 0.3167 - val_mean_squared_error: 0.1387\n",
            "Epoch 35/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.1467\n",
            "Epoch 35: val_loss improved from 0.31672 to 0.30684, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2873 - mean_squared_error: 0.1461 - val_loss: 0.3068 - val_mean_squared_error: 0.1334\n",
            "Epoch 36/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2814 - mean_squared_error: 0.1447\n",
            "Epoch 36: val_loss did not improve from 0.30684\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2798 - mean_squared_error: 0.1422 - val_loss: 0.3157 - val_mean_squared_error: 0.1405\n",
            "Epoch 37/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2962 - mean_squared_error: 0.1548\n",
            "Epoch 37: val_loss did not improve from 0.30684\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2819 - mean_squared_error: 0.1423 - val_loss: 0.3148 - val_mean_squared_error: 0.1395\n",
            "Epoch 38/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2896 - mean_squared_error: 0.1473\n",
            "Epoch 38: val_loss did not improve from 0.30684\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2856 - mean_squared_error: 0.1460 - val_loss: 0.3099 - val_mean_squared_error: 0.1310\n",
            "Epoch 39/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2698 - mean_squared_error: 0.1269\n",
            "Epoch 39: val_loss improved from 0.30684 to 0.29365, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2736 - mean_squared_error: 0.1346 - val_loss: 0.2936 - val_mean_squared_error: 0.1251\n",
            "Epoch 40/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2846 - mean_squared_error: 0.1403\n",
            "Epoch 40: val_loss did not improve from 0.29365\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2846 - mean_squared_error: 0.1403 - val_loss: 0.3087 - val_mean_squared_error: 0.1327\n",
            "Epoch 41/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2713 - mean_squared_error: 0.1333\n",
            "Epoch 41: val_loss did not improve from 0.29365\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2721 - mean_squared_error: 0.1320 - val_loss: 0.3082 - val_mean_squared_error: 0.1332\n",
            "Epoch 42/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2758 - mean_squared_error: 0.1292\n",
            "Epoch 42: val_loss did not improve from 0.29365\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2742 - mean_squared_error: 0.1322 - val_loss: 0.3195 - val_mean_squared_error: 0.1453\n",
            "Epoch 43/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2719 - mean_squared_error: 0.1323\n",
            "Epoch 43: val_loss did not improve from 0.29365\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2719 - mean_squared_error: 0.1323 - val_loss: 0.3431 - val_mean_squared_error: 0.1461\n",
            "Epoch 44/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2759 - mean_squared_error: 0.1377\n",
            "Epoch 44: val_loss improved from 0.29365 to 0.28485, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2801 - mean_squared_error: 0.1411 - val_loss: 0.2848 - val_mean_squared_error: 0.1217\n",
            "Epoch 45/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2589 - mean_squared_error: 0.1169\n",
            "Epoch 45: val_loss improved from 0.28485 to 0.26796, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2677 - mean_squared_error: 0.1274 - val_loss: 0.2680 - val_mean_squared_error: 0.1058\n",
            "Epoch 46/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2588 - mean_squared_error: 0.1232\n",
            "Epoch 46: val_loss did not improve from 0.26796\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2639 - mean_squared_error: 0.1260 - val_loss: 0.2861 - val_mean_squared_error: 0.1134\n",
            "Epoch 47/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2677 - mean_squared_error: 0.1249\n",
            "Epoch 47: val_loss did not improve from 0.26796\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2586 - mean_squared_error: 0.1171 - val_loss: 0.2906 - val_mean_squared_error: 0.1138\n",
            "Epoch 48/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2572 - mean_squared_error: 0.1123\n",
            "Epoch 48: val_loss did not improve from 0.26796\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2549 - mean_squared_error: 0.1169 - val_loss: 0.2714 - val_mean_squared_error: 0.0966\n",
            "Epoch 49/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2556 - mean_squared_error: 0.1172\n",
            "Epoch 49: val_loss improved from 0.26796 to 0.25558, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 0.2523 - mean_squared_error: 0.1147 - val_loss: 0.2556 - val_mean_squared_error: 0.0909\n",
            "Epoch 50/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.1178\n",
            "Epoch 50: val_loss improved from 0.25558 to 0.24449, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2598 - mean_squared_error: 0.1189 - val_loss: 0.2445 - val_mean_squared_error: 0.0816\n",
            "Epoch 51/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2561 - mean_squared_error: 0.1157\n",
            "Epoch 51: val_loss did not improve from 0.24449\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2561 - mean_squared_error: 0.1157 - val_loss: 0.2594 - val_mean_squared_error: 0.0882\n",
            "Epoch 52/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2522 - mean_squared_error: 0.1123\n",
            "Epoch 52: val_loss did not improve from 0.24449\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2488 - mean_squared_error: 0.1095 - val_loss: 0.2558 - val_mean_squared_error: 0.0882\n",
            "Epoch 53/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2476 - mean_squared_error: 0.1071\n",
            "Epoch 53: val_loss improved from 0.24449 to 0.22039, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.2577 - mean_squared_error: 0.1159 - val_loss: 0.2204 - val_mean_squared_error: 0.0752\n",
            "Epoch 54/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2552 - mean_squared_error: 0.1174\n",
            "Epoch 54: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2561 - mean_squared_error: 0.1171 - val_loss: 0.3227 - val_mean_squared_error: 0.1187\n",
            "Epoch 55/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2692 - mean_squared_error: 0.1246\n",
            "Epoch 55: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2750 - mean_squared_error: 0.1316 - val_loss: 0.2839 - val_mean_squared_error: 0.1312\n",
            "Epoch 56/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2434 - mean_squared_error: 0.1104\n",
            "Epoch 56: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2524 - mean_squared_error: 0.1124 - val_loss: 0.2376 - val_mean_squared_error: 0.0744\n",
            "Epoch 57/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2476 - mean_squared_error: 0.1072\n",
            "Epoch 57: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2431 - mean_squared_error: 0.1037 - val_loss: 0.2287 - val_mean_squared_error: 0.0791\n",
            "Epoch 58/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2479 - mean_squared_error: 0.1066\n",
            "Epoch 58: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2507 - mean_squared_error: 0.1096 - val_loss: 0.2731 - val_mean_squared_error: 0.0879\n",
            "Epoch 59/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2398 - mean_squared_error: 0.1040\n",
            "Epoch 59: val_loss did not improve from 0.22039\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2470 - mean_squared_error: 0.1100 - val_loss: 0.2242 - val_mean_squared_error: 0.0695\n",
            "Epoch 60/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2252 - mean_squared_error: 0.0950\n",
            "Epoch 60: val_loss improved from 0.22039 to 0.20509, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 0.2362 - mean_squared_error: 0.1010 - val_loss: 0.2051 - val_mean_squared_error: 0.0566\n",
            "Epoch 61/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2397 - mean_squared_error: 0.1033\n",
            "Epoch 61: val_loss did not improve from 0.20509\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2344 - mean_squared_error: 0.0997 - val_loss: 0.2069 - val_mean_squared_error: 0.0557\n",
            "Epoch 62/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2391 - mean_squared_error: 0.1008\n",
            "Epoch 62: val_loss did not improve from 0.20509\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2396 - mean_squared_error: 0.1000 - val_loss: 0.2411 - val_mean_squared_error: 0.0884\n",
            "Epoch 63/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2579 - mean_squared_error: 0.1162\n",
            "Epoch 63: val_loss did not improve from 0.20509\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2622 - mean_squared_error: 0.1151 - val_loss: 0.2785 - val_mean_squared_error: 0.0886\n",
            "Epoch 64/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2476 - mean_squared_error: 0.1108\n",
            "Epoch 64: val_loss improved from 0.20509 to 0.19183, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.2458 - mean_squared_error: 0.1093 - val_loss: 0.1918 - val_mean_squared_error: 0.0528\n",
            "Epoch 65/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2431 - mean_squared_error: 0.1102\n",
            "Epoch 65: val_loss did not improve from 0.19183\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2327 - mean_squared_error: 0.0966 - val_loss: 0.2112 - val_mean_squared_error: 0.0610\n",
            "Epoch 66/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2447 - mean_squared_error: 0.1042\n",
            "Epoch 66: val_loss did not improve from 0.19183\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2350 - mean_squared_error: 0.0984 - val_loss: 0.2030 - val_mean_squared_error: 0.0541\n",
            "Epoch 67/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2179 - mean_squared_error: 0.0885\n",
            "Epoch 67: val_loss improved from 0.19183 to 0.18783, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2282 - mean_squared_error: 0.0943 - val_loss: 0.1878 - val_mean_squared_error: 0.0502\n",
            "Epoch 68/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2266 - mean_squared_error: 0.0957\n",
            "Epoch 68: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2275 - mean_squared_error: 0.0967 - val_loss: 0.1923 - val_mean_squared_error: 0.0533\n",
            "Epoch 69/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2196 - mean_squared_error: 0.0884\n",
            "Epoch 69: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2284 - mean_squared_error: 0.0964 - val_loss: 0.1980 - val_mean_squared_error: 0.0539\n",
            "Epoch 70/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.0925\n",
            "Epoch 70: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2255 - mean_squared_error: 0.0912 - val_loss: 0.2038 - val_mean_squared_error: 0.0549\n",
            "Epoch 71/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.0855\n",
            "Epoch 71: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2238 - mean_squared_error: 0.0897 - val_loss: 0.2131 - val_mean_squared_error: 0.0653\n",
            "Epoch 72/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2411 - mean_squared_error: 0.1038\n",
            "Epoch 72: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2469 - mean_squared_error: 0.1040 - val_loss: 0.1995 - val_mean_squared_error: 0.0559\n",
            "Epoch 73/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2332 - mean_squared_error: 0.1001\n",
            "Epoch 73: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2313 - mean_squared_error: 0.0976 - val_loss: 0.2044 - val_mean_squared_error: 0.0568\n",
            "Epoch 74/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2432 - mean_squared_error: 0.1055\n",
            "Epoch 74: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2295 - mean_squared_error: 0.0947 - val_loss: 0.2020 - val_mean_squared_error: 0.0518\n",
            "Epoch 75/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2253 - mean_squared_error: 0.0902\n",
            "Epoch 75: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2238 - mean_squared_error: 0.0884 - val_loss: 0.2090 - val_mean_squared_error: 0.0542\n",
            "Epoch 76/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2166 - mean_squared_error: 0.0799\n",
            "Epoch 76: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2233 - mean_squared_error: 0.0878 - val_loss: 0.2052 - val_mean_squared_error: 0.0521\n",
            "Epoch 77/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2256 - mean_squared_error: 0.0934\n",
            "Epoch 77: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2227 - mean_squared_error: 0.0913 - val_loss: 0.2631 - val_mean_squared_error: 0.0782\n",
            "Epoch 78/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2597 - mean_squared_error: 0.1229\n",
            "Epoch 78: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2489 - mean_squared_error: 0.1128 - val_loss: 0.1922 - val_mean_squared_error: 0.0542\n",
            "Epoch 79/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2214 - mean_squared_error: 0.0898\n",
            "Epoch 79: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2174 - mean_squared_error: 0.0857 - val_loss: 0.2024 - val_mean_squared_error: 0.0512\n",
            "Epoch 80/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2163 - mean_squared_error: 0.0847\n",
            "Epoch 80: val_loss did not improve from 0.18783\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2165 - mean_squared_error: 0.0845 - val_loss: 0.2094 - val_mean_squared_error: 0.0537\n",
            "Epoch 81/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2278 - mean_squared_error: 0.0853\n",
            "Epoch 81: val_loss improved from 0.18783 to 0.17624, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2324 - mean_squared_error: 0.0945 - val_loss: 0.1762 - val_mean_squared_error: 0.0431\n",
            "Epoch 82/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2157 - mean_squared_error: 0.0856\n",
            "Epoch 82: val_loss did not improve from 0.17624\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2185 - mean_squared_error: 0.0866 - val_loss: 0.2184 - val_mean_squared_error: 0.0643\n",
            "Epoch 83/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2277 - mean_squared_error: 0.0927\n",
            "Epoch 83: val_loss did not improve from 0.17624\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2275 - mean_squared_error: 0.0915 - val_loss: 0.2564 - val_mean_squared_error: 0.0715\n",
            "Epoch 84/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2217 - mean_squared_error: 0.0894\n",
            "Epoch 84: val_loss did not improve from 0.17624\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2217 - mean_squared_error: 0.0894 - val_loss: 0.2002 - val_mean_squared_error: 0.0566\n",
            "Epoch 85/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2157 - mean_squared_error: 0.0883\n",
            "Epoch 85: val_loss improved from 0.17624 to 0.17444, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2157 - mean_squared_error: 0.0883 - val_loss: 0.1744 - val_mean_squared_error: 0.0465\n",
            "Epoch 86/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.0849\n",
            "Epoch 86: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2153 - mean_squared_error: 0.0849 - val_loss: 0.2171 - val_mean_squared_error: 0.0620\n",
            "Epoch 87/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2114 - mean_squared_error: 0.0856\n",
            "Epoch 87: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2128 - mean_squared_error: 0.0854 - val_loss: 0.2512 - val_mean_squared_error: 0.0682\n",
            "Epoch 88/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2260 - mean_squared_error: 0.0920\n",
            "Epoch 88: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2260 - mean_squared_error: 0.0920 - val_loss: 0.1947 - val_mean_squared_error: 0.0637\n",
            "Epoch 89/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2145 - mean_squared_error: 0.0835\n",
            "Epoch 89: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2145 - mean_squared_error: 0.0835 - val_loss: 0.2289 - val_mean_squared_error: 0.0602\n",
            "Epoch 90/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2188 - mean_squared_error: 0.0865\n",
            "Epoch 90: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2188 - mean_squared_error: 0.0865 - val_loss: 0.2075 - val_mean_squared_error: 0.0551\n",
            "Epoch 91/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2252 - mean_squared_error: 0.0915\n",
            "Epoch 91: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2252 - mean_squared_error: 0.0915 - val_loss: 0.2039 - val_mean_squared_error: 0.0609\n",
            "Epoch 92/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2085 - mean_squared_error: 0.0819\n",
            "Epoch 92: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2080 - mean_squared_error: 0.0811 - val_loss: 0.2419 - val_mean_squared_error: 0.0657\n",
            "Epoch 93/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2170 - mean_squared_error: 0.0835\n",
            "Epoch 93: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2145 - mean_squared_error: 0.0817 - val_loss: 0.1906 - val_mean_squared_error: 0.0600\n",
            "Epoch 94/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2074 - mean_squared_error: 0.0822\n",
            "Epoch 94: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2095 - mean_squared_error: 0.0834 - val_loss: 0.2386 - val_mean_squared_error: 0.0639\n",
            "Epoch 95/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2072 - mean_squared_error: 0.0801\n",
            "Epoch 95: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2068 - mean_squared_error: 0.0792 - val_loss: 0.1789 - val_mean_squared_error: 0.0535\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2075 - mean_squared_error: 0.0797\n",
            "Epoch 96: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2075 - mean_squared_error: 0.0797 - val_loss: 0.2105 - val_mean_squared_error: 0.0526\n",
            "Epoch 97/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2338 - mean_squared_error: 0.0964\n",
            "Epoch 97: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2338 - mean_squared_error: 0.0964 - val_loss: 0.2131 - val_mean_squared_error: 0.0517\n",
            "Epoch 98/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2151 - mean_squared_error: 0.0854\n",
            "Epoch 98: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2126 - mean_squared_error: 0.0834 - val_loss: 0.1976 - val_mean_squared_error: 0.0690\n",
            "Epoch 99/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2348 - mean_squared_error: 0.1019\n",
            "Epoch 99: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2348 - mean_squared_error: 0.1019 - val_loss: 0.3263 - val_mean_squared_error: 0.1197\n",
            "Epoch 100/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2942 - mean_squared_error: 0.1737\n",
            "Epoch 100: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2538 - mean_squared_error: 0.1166 - val_loss: 0.2183 - val_mean_squared_error: 0.0975\n",
            "Epoch 101/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2309 - mean_squared_error: 0.0932\n",
            "Epoch 101: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2387 - mean_squared_error: 0.1017 - val_loss: 0.2815 - val_mean_squared_error: 0.0867\n",
            "Epoch 102/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2317 - mean_squared_error: 0.0968\n",
            "Epoch 102: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2317 - mean_squared_error: 0.0968 - val_loss: 0.1882 - val_mean_squared_error: 0.0625\n",
            "Epoch 103/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2028 - mean_squared_error: 0.0779\n",
            "Epoch 103: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2028 - mean_squared_error: 0.0779 - val_loss: 0.2504 - val_mean_squared_error: 0.0688\n",
            "Epoch 104/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2179 - mean_squared_error: 0.0834\n",
            "Epoch 104: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2179 - mean_squared_error: 0.0834 - val_loss: 0.1995 - val_mean_squared_error: 0.0639\n",
            "Epoch 105/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2171 - mean_squared_error: 0.0863\n",
            "Epoch 105: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2175 - mean_squared_error: 0.0864 - val_loss: 0.2083 - val_mean_squared_error: 0.0561\n",
            "Epoch 106/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2086 - mean_squared_error: 0.0827\n",
            "Epoch 106: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2086 - mean_squared_error: 0.0827 - val_loss: 0.2038 - val_mean_squared_error: 0.0531\n",
            "Epoch 107/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2078 - mean_squared_error: 0.0796\n",
            "Epoch 107: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2078 - mean_squared_error: 0.0796 - val_loss: 0.1990 - val_mean_squared_error: 0.0562\n",
            "Epoch 108/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1987 - mean_squared_error: 0.0780\n",
            "Epoch 108: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1987 - mean_squared_error: 0.0780 - val_loss: 0.1927 - val_mean_squared_error: 0.0515\n",
            "Epoch 109/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0767\n",
            "Epoch 109: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2007 - mean_squared_error: 0.0767 - val_loss: 0.1935 - val_mean_squared_error: 0.0502\n",
            "Epoch 110/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2108 - mean_squared_error: 0.0838\n",
            "Epoch 110: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2160 - mean_squared_error: 0.0868 - val_loss: 0.2078 - val_mean_squared_error: 0.0535\n",
            "Epoch 111/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2087 - mean_squared_error: 0.0799\n",
            "Epoch 111: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2114 - mean_squared_error: 0.0819 - val_loss: 0.1831 - val_mean_squared_error: 0.0455\n",
            "Epoch 112/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2211 - mean_squared_error: 0.0855\n",
            "Epoch 112: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2211 - mean_squared_error: 0.0855 - val_loss: 0.2106 - val_mean_squared_error: 0.0542\n",
            "Epoch 113/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1945 - mean_squared_error: 0.0735\n",
            "Epoch 113: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1984 - mean_squared_error: 0.0766 - val_loss: 0.1839 - val_mean_squared_error: 0.0501\n",
            "Epoch 114/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1964 - mean_squared_error: 0.0743\n",
            "Epoch 114: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1941 - mean_squared_error: 0.0731 - val_loss: 0.1938 - val_mean_squared_error: 0.0480\n",
            "Epoch 115/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2069 - mean_squared_error: 0.1051\n",
            "Epoch 115: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1939 - mean_squared_error: 0.0725 - val_loss: 0.1927 - val_mean_squared_error: 0.0481\n",
            "Epoch 116/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1918 - mean_squared_error: 0.0685\n",
            "Epoch 116: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1957 - mean_squared_error: 0.0737 - val_loss: 0.1827 - val_mean_squared_error: 0.0460\n",
            "Epoch 117/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2030 - mean_squared_error: 0.0762\n",
            "Epoch 117: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1981 - mean_squared_error: 0.0734 - val_loss: 0.2048 - val_mean_squared_error: 0.0526\n",
            "Epoch 118/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1973 - mean_squared_error: 0.0744\n",
            "Epoch 118: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1936 - mean_squared_error: 0.0714 - val_loss: 0.1873 - val_mean_squared_error: 0.0457\n",
            "Epoch 119/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1945 - mean_squared_error: 0.0734\n",
            "Epoch 119: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1945 - mean_squared_error: 0.0734 - val_loss: 0.1777 - val_mean_squared_error: 0.0461\n",
            "Epoch 120/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1930 - mean_squared_error: 0.0722\n",
            "Epoch 120: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1930 - mean_squared_error: 0.0722 - val_loss: 0.1843 - val_mean_squared_error: 0.0462\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2005 - mean_squared_error: 0.0779\n",
            "Epoch 121: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2005 - mean_squared_error: 0.0779 - val_loss: 0.1942 - val_mean_squared_error: 0.0483\n",
            "Epoch 122/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1982 - mean_squared_error: 0.0743\n",
            "Epoch 122: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1982 - mean_squared_error: 0.0743 - val_loss: 0.1981 - val_mean_squared_error: 0.0644\n",
            "Epoch 123/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1954 - mean_squared_error: 0.0766\n",
            "Epoch 123: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1954 - mean_squared_error: 0.0766 - val_loss: 0.2004 - val_mean_squared_error: 0.0529\n",
            "Epoch 124/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1984 - mean_squared_error: 0.0760\n",
            "Epoch 124: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1984 - mean_squared_error: 0.0760 - val_loss: 0.1832 - val_mean_squared_error: 0.0462\n",
            "Epoch 125/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1939 - mean_squared_error: 0.0731\n",
            "Epoch 125: val_loss did not improve from 0.17444\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1939 - mean_squared_error: 0.0731 - val_loss: 0.2048 - val_mean_squared_error: 0.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 636 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cc26e19b910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 193ms/step\n",
            "Cycle:  4\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "1/9 [==>...........................] - ETA: 29s - loss: 4.0668 - mean_squared_error: 20.4878\n",
            "Epoch 1: val_loss improved from inf to 4.67577, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 4s 78ms/step - loss: 5.2869 - mean_squared_error: 45.1990 - val_loss: 4.6758 - val_mean_squared_error: 34.3325\n",
            "Epoch 2/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 3.5241 - mean_squared_error: 17.9805\n",
            "Epoch 2: val_loss improved from 4.67577 to 2.91038, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 3.5241 - mean_squared_error: 17.9805 - val_loss: 2.9104 - val_mean_squared_error: 10.8094\n",
            "Epoch 3/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 2.5041 - mean_squared_error: 6.8663\n",
            "Epoch 3: val_loss improved from 2.91038 to 1.29392, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 1.9854 - mean_squared_error: 4.8601 - val_loss: 1.2939 - val_mean_squared_error: 1.9041\n",
            "Epoch 4/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.0743 - mean_squared_error: 1.4123\n",
            "Epoch 4: val_loss did not improve from 1.29392\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8481 - mean_squared_error: 1.0951 - val_loss: 1.3036 - val_mean_squared_error: 3.0697\n",
            "Epoch 5/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.8244 - mean_squared_error: 1.3293\n",
            "Epoch 5: val_loss improved from 1.29392 to 1.08841, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.9227 - mean_squared_error: 1.6754 - val_loss: 1.0884 - val_mean_squared_error: 2.0276\n",
            "Epoch 6/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.7810 - mean_squared_error: 1.0422\n",
            "Epoch 6: val_loss improved from 1.08841 to 0.74147, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.6067 - mean_squared_error: 0.6484 - val_loss: 0.7415 - val_mean_squared_error: 0.7784\n",
            "Epoch 7/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5302 - mean_squared_error: 0.4121\n",
            "Epoch 7: val_loss improved from 0.74147 to 0.55342, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.5067 - mean_squared_error: 0.4271 - val_loss: 0.5534 - val_mean_squared_error: 0.4851\n",
            "Epoch 8/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4179 - mean_squared_error: 0.3444\n",
            "Epoch 8: val_loss improved from 0.55342 to 0.52321, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.4443 - mean_squared_error: 0.3448 - val_loss: 0.5232 - val_mean_squared_error: 0.4310\n",
            "Epoch 9/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4068 - mean_squared_error: 0.2743\n",
            "Epoch 9: val_loss improved from 0.52321 to 0.50727, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.4240 - mean_squared_error: 0.3255 - val_loss: 0.5073 - val_mean_squared_error: 0.4272\n",
            "Epoch 10/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3376 - mean_squared_error: 0.1591\n",
            "Epoch 10: val_loss did not improve from 0.50727\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.4219 - mean_squared_error: 0.3181 - val_loss: 0.5390 - val_mean_squared_error: 0.4565\n",
            "Epoch 11/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3900 - mean_squared_error: 0.2722\n",
            "Epoch 11: val_loss did not improve from 0.50727\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3989 - mean_squared_error: 0.2935 - val_loss: 0.5183 - val_mean_squared_error: 0.4308\n",
            "Epoch 12/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3809 - mean_squared_error: 0.3285\n",
            "Epoch 12: val_loss did not improve from 0.50727\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.3926 - mean_squared_error: 0.2852 - val_loss: 0.5157 - val_mean_squared_error: 0.4164\n",
            "Epoch 13/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4957 - mean_squared_error: 0.3499\n",
            "Epoch 13: val_loss improved from 0.50727 to 0.48449, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3841 - mean_squared_error: 0.2706 - val_loss: 0.4845 - val_mean_squared_error: 0.3772\n",
            "Epoch 14/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3627 - mean_squared_error: 0.2933\n",
            "Epoch 14: val_loss did not improve from 0.48449\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3846 - mean_squared_error: 0.2662 - val_loss: 0.5054 - val_mean_squared_error: 0.4055\n",
            "Epoch 15/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2520 - mean_squared_error: 0.1105\n",
            "Epoch 15: val_loss did not improve from 0.48449\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3688 - mean_squared_error: 0.2520 - val_loss: 0.5016 - val_mean_squared_error: 0.3875\n",
            "Epoch 16/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4862 - mean_squared_error: 0.4177\n",
            "Epoch 16: val_loss did not improve from 0.48449\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3607 - mean_squared_error: 0.2370 - val_loss: 0.5331 - val_mean_squared_error: 0.4445\n",
            "Epoch 17/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3364 - mean_squared_error: 0.1663\n",
            "Epoch 17: val_loss improved from 0.48449 to 0.48054, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3702 - mean_squared_error: 0.2474 - val_loss: 0.4805 - val_mean_squared_error: 0.3517\n",
            "Epoch 18/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3574 - mean_squared_error: 0.2345\n",
            "Epoch 18: val_loss improved from 0.48054 to 0.45872, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3557 - mean_squared_error: 0.2307 - val_loss: 0.4587 - val_mean_squared_error: 0.3233\n",
            "Epoch 19/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3376 - mean_squared_error: 0.1700\n",
            "Epoch 19: val_loss did not improve from 0.45872\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3383 - mean_squared_error: 0.2130 - val_loss: 0.4610 - val_mean_squared_error: 0.3194\n",
            "Epoch 20/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2773 - mean_squared_error: 0.1139\n",
            "Epoch 20: val_loss improved from 0.45872 to 0.45173, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3369 - mean_squared_error: 0.2071 - val_loss: 0.4517 - val_mean_squared_error: 0.3008\n",
            "Epoch 21/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3438 - mean_squared_error: 0.2438\n",
            "Epoch 21: val_loss did not improve from 0.45173\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.3346 - mean_squared_error: 0.2061 - val_loss: 0.4885 - val_mean_squared_error: 0.3584\n",
            "Epoch 22/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4079 - mean_squared_error: 0.3303\n",
            "Epoch 22: val_loss did not improve from 0.45173\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3451 - mean_squared_error: 0.2100 - val_loss: 0.4645 - val_mean_squared_error: 0.3051\n",
            "Epoch 23/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3198 - mean_squared_error: 0.2384\n",
            "Epoch 23: val_loss improved from 0.45173 to 0.44875, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3281 - mean_squared_error: 0.1927 - val_loss: 0.4487 - val_mean_squared_error: 0.3107\n",
            "Epoch 24/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3088 - mean_squared_error: 0.1677\n",
            "Epoch 24: val_loss improved from 0.44875 to 0.40537, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.3245 - mean_squared_error: 0.1893 - val_loss: 0.4054 - val_mean_squared_error: 0.2496\n",
            "Epoch 25/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3368 - mean_squared_error: 0.1947\n",
            "Epoch 25: val_loss did not improve from 0.40537\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3266 - mean_squared_error: 0.1854 - val_loss: 0.4517 - val_mean_squared_error: 0.2770\n",
            "Epoch 26/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3066 - mean_squared_error: 0.1631\n",
            "Epoch 26: val_loss did not improve from 0.40537\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.3131 - mean_squared_error: 0.1737 - val_loss: 0.4785 - val_mean_squared_error: 0.3443\n",
            "Epoch 27/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3157 - mean_squared_error: 0.1741\n",
            "Epoch 27: val_loss did not improve from 0.40537\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.3157 - mean_squared_error: 0.1741 - val_loss: 0.4201 - val_mean_squared_error: 0.2338\n",
            "Epoch 28/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3085 - mean_squared_error: 0.1685\n",
            "Epoch 28: val_loss improved from 0.40537 to 0.40487, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.3110 - mean_squared_error: 0.1685 - val_loss: 0.4049 - val_mean_squared_error: 0.2248\n",
            "Epoch 29/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2996 - mean_squared_error: 0.1636\n",
            "Epoch 29: val_loss improved from 0.40487 to 0.38174, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2965 - mean_squared_error: 0.1603 - val_loss: 0.3817 - val_mean_squared_error: 0.2064\n",
            "Epoch 30/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2936 - mean_squared_error: 0.1489\n",
            "Epoch 30: val_loss did not improve from 0.38174\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2947 - mean_squared_error: 0.1584 - val_loss: 0.4001 - val_mean_squared_error: 0.2113\n",
            "Epoch 31/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2990 - mean_squared_error: 0.1526\n",
            "Epoch 31: val_loss improved from 0.38174 to 0.38040, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3124 - mean_squared_error: 0.1618 - val_loss: 0.3804 - val_mean_squared_error: 0.2082\n",
            "Epoch 32/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2923 - mean_squared_error: 0.1559\n",
            "Epoch 32: val_loss did not improve from 0.38040\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2939 - mean_squared_error: 0.1558 - val_loss: 0.3931 - val_mean_squared_error: 0.2089\n",
            "Epoch 33/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2805 - mean_squared_error: 0.1431\n",
            "Epoch 33: val_loss did not improve from 0.38040\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2868 - mean_squared_error: 0.1479 - val_loss: 0.4099 - val_mean_squared_error: 0.2441\n",
            "Epoch 34/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2808 - mean_squared_error: 0.1373\n",
            "Epoch 34: val_loss improved from 0.38040 to 0.34902, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2854 - mean_squared_error: 0.1494 - val_loss: 0.3490 - val_mean_squared_error: 0.1592\n",
            "Epoch 35/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2878 - mean_squared_error: 0.1507\n",
            "Epoch 35: val_loss did not improve from 0.34902\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2927 - mean_squared_error: 0.1522 - val_loss: 0.3884 - val_mean_squared_error: 0.2085\n",
            "Epoch 36/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2802 - mean_squared_error: 0.1444\n",
            "Epoch 36: val_loss improved from 0.34902 to 0.34611, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2824 - mean_squared_error: 0.1428 - val_loss: 0.3461 - val_mean_squared_error: 0.1589\n",
            "Epoch 37/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2796 - mean_squared_error: 0.1462\n",
            "Epoch 37: val_loss did not improve from 0.34611\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2797 - mean_squared_error: 0.1447 - val_loss: 0.3483 - val_mean_squared_error: 0.1628\n",
            "Epoch 38/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2780 - mean_squared_error: 0.1457\n",
            "Epoch 38: val_loss improved from 0.34611 to 0.34369, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2833 - mean_squared_error: 0.1474 - val_loss: 0.3437 - val_mean_squared_error: 0.1487\n",
            "Epoch 39/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2661 - mean_squared_error: 0.1237\n",
            "Epoch 39: val_loss improved from 0.34369 to 0.34200, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2710 - mean_squared_error: 0.1361 - val_loss: 0.3420 - val_mean_squared_error: 0.1564\n",
            "Epoch 40/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2728 - mean_squared_error: 0.1383\n",
            "Epoch 40: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2691 - mean_squared_error: 0.1351 - val_loss: 0.3595 - val_mean_squared_error: 0.1729\n",
            "Epoch 41/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.1402\n",
            "Epoch 41: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2790 - mean_squared_error: 0.1409 - val_loss: 0.3420 - val_mean_squared_error: 0.1525\n",
            "Epoch 42/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2793 - mean_squared_error: 0.1298\n",
            "Epoch 42: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2825 - mean_squared_error: 0.1439 - val_loss: 0.3684 - val_mean_squared_error: 0.1903\n",
            "Epoch 43/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2927 - mean_squared_error: 0.1572\n",
            "Epoch 43: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2868 - mean_squared_error: 0.1521 - val_loss: 0.3854 - val_mean_squared_error: 0.1812\n",
            "Epoch 44/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.1461\n",
            "Epoch 44: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2849 - mean_squared_error: 0.1467 - val_loss: 0.4106 - val_mean_squared_error: 0.2424\n",
            "Epoch 45/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2724 - mean_squared_error: 0.1390\n",
            "Epoch 45: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2732 - mean_squared_error: 0.1384 - val_loss: 0.3493 - val_mean_squared_error: 0.1510\n",
            "Epoch 46/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.1387\n",
            "Epoch 46: val_loss did not improve from 0.34200\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2794 - mean_squared_error: 0.1405 - val_loss: 0.3696 - val_mean_squared_error: 0.1882\n",
            "Epoch 47/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2645 - mean_squared_error: 0.1276\n",
            "Epoch 47: val_loss improved from 0.34200 to 0.34073, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2610 - mean_squared_error: 0.1239 - val_loss: 0.3407 - val_mean_squared_error: 0.1624\n",
            "Epoch 48/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2580 - mean_squared_error: 0.1167\n",
            "Epoch 48: val_loss did not improve from 0.34073\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2606 - mean_squared_error: 0.1239 - val_loss: 0.3470 - val_mean_squared_error: 0.1429\n",
            "Epoch 49/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2633 - mean_squared_error: 0.1238\n",
            "Epoch 49: val_loss improved from 0.34073 to 0.33517, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2668 - mean_squared_error: 0.1266 - val_loss: 0.3352 - val_mean_squared_error: 0.1546\n",
            "Epoch 50/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2500 - mean_squared_error: 0.1231\n",
            "Epoch 50: val_loss did not improve from 0.33517\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2526 - mean_squared_error: 0.1228 - val_loss: 0.3426 - val_mean_squared_error: 0.1441\n",
            "Epoch 51/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2568 - mean_squared_error: 0.1182\n",
            "Epoch 51: val_loss did not improve from 0.33517\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2616 - mean_squared_error: 0.1238 - val_loss: 0.3370 - val_mean_squared_error: 0.1571\n",
            "Epoch 52/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2464 - mean_squared_error: 0.1060\n",
            "Epoch 52: val_loss improved from 0.33517 to 0.30453, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2502 - mean_squared_error: 0.1143 - val_loss: 0.3045 - val_mean_squared_error: 0.1159\n",
            "Epoch 53/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2460 - mean_squared_error: 0.1134\n",
            "Epoch 53: val_loss did not improve from 0.30453\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2474 - mean_squared_error: 0.1130 - val_loss: 0.3406 - val_mean_squared_error: 0.1609\n",
            "Epoch 54/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2665 - mean_squared_error: 0.1315\n",
            "Epoch 54: val_loss did not improve from 0.30453\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2614 - mean_squared_error: 0.1236 - val_loss: 0.3511 - val_mean_squared_error: 0.1510\n",
            "Epoch 55/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2574 - mean_squared_error: 0.1215\n",
            "Epoch 55: val_loss did not improve from 0.30453\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2574 - mean_squared_error: 0.1215 - val_loss: 0.3196 - val_mean_squared_error: 0.1407\n",
            "Epoch 56/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2449 - mean_squared_error: 0.1114\n",
            "Epoch 56: val_loss did not improve from 0.30453\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2423 - mean_squared_error: 0.1119 - val_loss: 0.3200 - val_mean_squared_error: 0.1331\n",
            "Epoch 57/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2459 - mean_squared_error: 0.1092\n",
            "Epoch 57: val_loss did not improve from 0.30453\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2459 - mean_squared_error: 0.1092 - val_loss: 0.3090 - val_mean_squared_error: 0.1327\n",
            "Epoch 58/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2841 - mean_squared_error: 0.1826\n",
            "Epoch 58: val_loss improved from 0.30453 to 0.29323, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2404 - mean_squared_error: 0.1083 - val_loss: 0.2932 - val_mean_squared_error: 0.1157\n",
            "Epoch 59/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1638 - mean_squared_error: 0.0454\n",
            "Epoch 59: val_loss did not improve from 0.29323\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2399 - mean_squared_error: 0.1082 - val_loss: 0.3185 - val_mean_squared_error: 0.1395\n",
            "Epoch 60/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1842 - mean_squared_error: 0.0595\n",
            "Epoch 60: val_loss improved from 0.29323 to 0.28008, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2362 - mean_squared_error: 0.1066 - val_loss: 0.2801 - val_mean_squared_error: 0.1002\n",
            "Epoch 61/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.1287\n",
            "Epoch 61: val_loss did not improve from 0.28008\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2449 - mean_squared_error: 0.1120 - val_loss: 0.3083 - val_mean_squared_error: 0.1251\n",
            "Epoch 62/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2381 - mean_squared_error: 0.1056\n",
            "Epoch 62: val_loss did not improve from 0.28008\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2446 - mean_squared_error: 0.1096 - val_loss: 0.3358 - val_mean_squared_error: 0.1524\n",
            "Epoch 63/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2136 - mean_squared_error: 0.1091\n",
            "Epoch 63: val_loss did not improve from 0.28008\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2540 - mean_squared_error: 0.1154 - val_loss: 0.3052 - val_mean_squared_error: 0.1114\n",
            "Epoch 64/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2395 - mean_squared_error: 0.1088\n",
            "Epoch 64: val_loss did not improve from 0.28008\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2395 - mean_squared_error: 0.1088 - val_loss: 0.2898 - val_mean_squared_error: 0.1106\n",
            "Epoch 65/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2547 - mean_squared_error: 0.1235\n",
            "Epoch 65: val_loss did not improve from 0.28008\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2338 - mean_squared_error: 0.1027 - val_loss: 0.2879 - val_mean_squared_error: 0.1030\n",
            "Epoch 66/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2336 - mean_squared_error: 0.0993\n",
            "Epoch 66: val_loss improved from 0.28008 to 0.27744, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2352 - mean_squared_error: 0.1037 - val_loss: 0.2774 - val_mean_squared_error: 0.0984\n",
            "Epoch 67/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2176 - mean_squared_error: 0.0786\n",
            "Epoch 67: val_loss did not improve from 0.27744\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2283 - mean_squared_error: 0.1005 - val_loss: 0.3039 - val_mean_squared_error: 0.1172\n",
            "Epoch 68/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2401 - mean_squared_error: 0.1134\n",
            "Epoch 68: val_loss did not improve from 0.27744\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2362 - mean_squared_error: 0.1048 - val_loss: 0.2955 - val_mean_squared_error: 0.1052\n",
            "Epoch 69/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2469 - mean_squared_error: 0.1118\n",
            "Epoch 69: val_loss improved from 0.27744 to 0.27657, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2294 - mean_squared_error: 0.0998 - val_loss: 0.2766 - val_mean_squared_error: 0.0909\n",
            "Epoch 70/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2445 - mean_squared_error: 0.1053\n",
            "Epoch 70: val_loss did not improve from 0.27657\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2445 - mean_squared_error: 0.1053 - val_loss: 0.3208 - val_mean_squared_error: 0.1431\n",
            "Epoch 71/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2758 - mean_squared_error: 0.1383\n",
            "Epoch 71: val_loss improved from 0.27657 to 0.27231, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2512 - mean_squared_error: 0.1127 - val_loss: 0.2723 - val_mean_squared_error: 0.0886\n",
            "Epoch 72/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1609 - mean_squared_error: 0.0406\n",
            "Epoch 72: val_loss improved from 0.27231 to 0.26130, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2260 - mean_squared_error: 0.0973 - val_loss: 0.2613 - val_mean_squared_error: 0.0876\n",
            "Epoch 73/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2221 - mean_squared_error: 0.0959\n",
            "Epoch 73: val_loss improved from 0.26130 to 0.25848, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2240 - mean_squared_error: 0.0958 - val_loss: 0.2585 - val_mean_squared_error: 0.0871\n",
            "Epoch 74/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2289 - mean_squared_error: 0.1033\n",
            "Epoch 74: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2211 - mean_squared_error: 0.0944 - val_loss: 0.2652 - val_mean_squared_error: 0.0816\n",
            "Epoch 75/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2894 - mean_squared_error: 0.1690\n",
            "Epoch 75: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2264 - mean_squared_error: 0.0962 - val_loss: 0.2719 - val_mean_squared_error: 0.0872\n",
            "Epoch 76/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2248 - mean_squared_error: 0.0965\n",
            "Epoch 76: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2214 - mean_squared_error: 0.0952 - val_loss: 0.2628 - val_mean_squared_error: 0.0898\n",
            "Epoch 77/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1696 - mean_squared_error: 0.0644\n",
            "Epoch 77: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2181 - mean_squared_error: 0.0918 - val_loss: 0.2705 - val_mean_squared_error: 0.0846\n",
            "Epoch 78/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2260 - mean_squared_error: 0.0968\n",
            "Epoch 78: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2260 - mean_squared_error: 0.0968 - val_loss: 0.2777 - val_mean_squared_error: 0.0910\n",
            "Epoch 79/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2126 - mean_squared_error: 0.0747\n",
            "Epoch 79: val_loss did not improve from 0.25848\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2229 - mean_squared_error: 0.0937 - val_loss: 0.2585 - val_mean_squared_error: 0.0774\n",
            "Epoch 80/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2271 - mean_squared_error: 0.1013\n",
            "Epoch 80: val_loss improved from 0.25848 to 0.24889, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2223 - mean_squared_error: 0.0921 - val_loss: 0.2489 - val_mean_squared_error: 0.0794\n",
            "Epoch 81/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.0878\n",
            "Epoch 81: val_loss did not improve from 0.24889\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2170 - mean_squared_error: 0.0902 - val_loss: 0.2564 - val_mean_squared_error: 0.0783\n",
            "Epoch 82/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3190 - mean_squared_error: 0.1914\n",
            "Epoch 82: val_loss did not improve from 0.24889\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2199 - mean_squared_error: 0.0929 - val_loss: 0.2581 - val_mean_squared_error: 0.0855\n",
            "Epoch 83/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2227 - mean_squared_error: 0.0941\n",
            "Epoch 83: val_loss improved from 0.24889 to 0.22558, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2205 - mean_squared_error: 0.0922 - val_loss: 0.2256 - val_mean_squared_error: 0.0670\n",
            "Epoch 84/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1755 - mean_squared_error: 0.0590\n",
            "Epoch 84: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2207 - mean_squared_error: 0.0934 - val_loss: 0.2747 - val_mean_squared_error: 0.0965\n",
            "Epoch 85/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2230 - mean_squared_error: 0.1003\n",
            "Epoch 85: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2164 - mean_squared_error: 0.0930 - val_loss: 0.2604 - val_mean_squared_error: 0.0773\n",
            "Epoch 86/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2232 - mean_squared_error: 0.0927\n",
            "Epoch 86: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2232 - mean_squared_error: 0.0927 - val_loss: 0.2564 - val_mean_squared_error: 0.0855\n",
            "Epoch 87/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1378 - mean_squared_error: 0.0393\n",
            "Epoch 87: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2121 - mean_squared_error: 0.0880 - val_loss: 0.2392 - val_mean_squared_error: 0.0714\n",
            "Epoch 88/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.0874\n",
            "Epoch 88: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2103 - mean_squared_error: 0.0856 - val_loss: 0.2490 - val_mean_squared_error: 0.0818\n",
            "Epoch 89/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2203 - mean_squared_error: 0.0938\n",
            "Epoch 89: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2145 - mean_squared_error: 0.0891 - val_loss: 0.2809 - val_mean_squared_error: 0.0850\n",
            "Epoch 90/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2423 - mean_squared_error: 0.0797\n",
            "Epoch 90: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2435 - mean_squared_error: 0.1090 - val_loss: 0.2454 - val_mean_squared_error: 0.0730\n",
            "Epoch 91/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2537 - mean_squared_error: 0.1118\n",
            "Epoch 91: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2289 - mean_squared_error: 0.0934 - val_loss: 0.2439 - val_mean_squared_error: 0.0796\n",
            "Epoch 92/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2179 - mean_squared_error: 0.0913\n",
            "Epoch 92: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2146 - mean_squared_error: 0.0888 - val_loss: 0.3013 - val_mean_squared_error: 0.0936\n",
            "Epoch 93/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2361 - mean_squared_error: 0.1000\n",
            "Epoch 93: val_loss did not improve from 0.22558\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.2361 - mean_squared_error: 0.1000 - val_loss: 0.2904 - val_mean_squared_error: 0.1328\n",
            "Epoch 94/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2304 - mean_squared_error: 0.1044\n",
            "Epoch 94: val_loss improved from 0.22558 to 0.22380, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2157 - mean_squared_error: 0.0883 - val_loss: 0.2238 - val_mean_squared_error: 0.0593\n",
            "Epoch 95/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2174 - mean_squared_error: 0.0831\n",
            "Epoch 95: val_loss did not improve from 0.22380\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2126 - mean_squared_error: 0.0868 - val_loss: 0.2302 - val_mean_squared_error: 0.0640\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.0876\n",
            "Epoch 96: val_loss did not improve from 0.22380\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2106 - mean_squared_error: 0.0876 - val_loss: 0.2287 - val_mean_squared_error: 0.0643\n",
            "Epoch 97/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2149 - mean_squared_error: 0.0833\n",
            "Epoch 97: val_loss did not improve from 0.22380\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2203 - mean_squared_error: 0.0906 - val_loss: 0.2355 - val_mean_squared_error: 0.0672\n",
            "Epoch 98/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2405 - mean_squared_error: 0.0982\n",
            "Epoch 98: val_loss improved from 0.22380 to 0.22370, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2064 - mean_squared_error: 0.0821 - val_loss: 0.2237 - val_mean_squared_error: 0.0649\n",
            "Epoch 99/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2040 - mean_squared_error: 0.0807\n",
            "Epoch 99: val_loss did not improve from 0.22370\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2096 - mean_squared_error: 0.0863 - val_loss: 0.2608 - val_mean_squared_error: 0.0731\n",
            "Epoch 100/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2473 - mean_squared_error: 0.1368\n",
            "Epoch 100: val_loss did not improve from 0.22370\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2287 - mean_squared_error: 0.0963 - val_loss: 0.2557 - val_mean_squared_error: 0.1059\n",
            "Epoch 101/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1709 - mean_squared_error: 0.0549\n",
            "Epoch 101: val_loss did not improve from 0.22370\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2200 - mean_squared_error: 0.0888 - val_loss: 0.2414 - val_mean_squared_error: 0.0707\n",
            "Epoch 102/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2019 - mean_squared_error: 0.0801\n",
            "Epoch 102: val_loss improved from 0.22370 to 0.21901, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2052 - mean_squared_error: 0.0816 - val_loss: 0.2190 - val_mean_squared_error: 0.0550\n",
            "Epoch 103/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2066 - mean_squared_error: 0.0664\n",
            "Epoch 103: val_loss did not improve from 0.21901\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2205 - mean_squared_error: 0.0947 - val_loss: 0.2519 - val_mean_squared_error: 0.0870\n",
            "Epoch 104/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2125 - mean_squared_error: 0.0854\n",
            "Epoch 104: val_loss improved from 0.21901 to 0.21574, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.2094 - mean_squared_error: 0.0830 - val_loss: 0.2157 - val_mean_squared_error: 0.0609\n",
            "Epoch 105/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2002 - mean_squared_error: 0.0752\n",
            "Epoch 105: val_loss improved from 0.21574 to 0.19753, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2083 - mean_squared_error: 0.0838 - val_loss: 0.1975 - val_mean_squared_error: 0.0487\n",
            "Epoch 106/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2208 - mean_squared_error: 0.0962\n",
            "Epoch 106: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2078 - mean_squared_error: 0.0817 - val_loss: 0.2487 - val_mean_squared_error: 0.0733\n",
            "Epoch 107/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2135 - mean_squared_error: 0.1267\n",
            "Epoch 107: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2047 - mean_squared_error: 0.0812 - val_loss: 0.1991 - val_mean_squared_error: 0.0539\n",
            "Epoch 108/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2097 - mean_squared_error: 0.0859\n",
            "Epoch 108: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2065 - mean_squared_error: 0.0832 - val_loss: 0.2211 - val_mean_squared_error: 0.0592\n",
            "Epoch 109/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2186 - mean_squared_error: 0.0937\n",
            "Epoch 109: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2040 - mean_squared_error: 0.0792 - val_loss: 0.2140 - val_mean_squared_error: 0.0561\n",
            "Epoch 110/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1788 - mean_squared_error: 0.0463\n",
            "Epoch 110: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2023 - mean_squared_error: 0.0798 - val_loss: 0.2097 - val_mean_squared_error: 0.0528\n",
            "Epoch 111/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1491 - mean_squared_error: 0.0375\n",
            "Epoch 111: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2032 - mean_squared_error: 0.0815 - val_loss: 0.2222 - val_mean_squared_error: 0.0610\n",
            "Epoch 112/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2052 - mean_squared_error: 0.0815\n",
            "Epoch 112: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2021 - mean_squared_error: 0.0802 - val_loss: 0.2182 - val_mean_squared_error: 0.0677\n",
            "Epoch 113/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2051 - mean_squared_error: 0.0924\n",
            "Epoch 113: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2114 - mean_squared_error: 0.0833 - val_loss: 0.1997 - val_mean_squared_error: 0.0499\n",
            "Epoch 114/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0802\n",
            "Epoch 114: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2000 - mean_squared_error: 0.0796 - val_loss: 0.2342 - val_mean_squared_error: 0.0709\n",
            "Epoch 115/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2006 - mean_squared_error: 0.0786\n",
            "Epoch 115: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1989 - mean_squared_error: 0.0791 - val_loss: 0.2035 - val_mean_squared_error: 0.0546\n",
            "Epoch 116/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1635 - mean_squared_error: 0.0438\n",
            "Epoch 116: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1959 - mean_squared_error: 0.0777 - val_loss: 0.2215 - val_mean_squared_error: 0.0685\n",
            "Epoch 117/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2078 - mean_squared_error: 0.0840\n",
            "Epoch 117: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2081 - mean_squared_error: 0.0830 - val_loss: 0.2094 - val_mean_squared_error: 0.0559\n",
            "Epoch 118/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2191 - mean_squared_error: 0.0927\n",
            "Epoch 118: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2111 - mean_squared_error: 0.0879 - val_loss: 0.2515 - val_mean_squared_error: 0.0679\n",
            "Epoch 119/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2339 - mean_squared_error: 0.0938\n",
            "Epoch 119: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2261 - mean_squared_error: 0.0917 - val_loss: 0.2846 - val_mean_squared_error: 0.1536\n",
            "Epoch 120/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2374 - mean_squared_error: 0.1041\n",
            "Epoch 120: val_loss did not improve from 0.19753\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2372 - mean_squared_error: 0.1016 - val_loss: 0.2550 - val_mean_squared_error: 0.0718\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2159 - mean_squared_error: 0.0833\n",
            "Epoch 121: val_loss improved from 0.19753 to 0.19661, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.2159 - mean_squared_error: 0.0833 - val_loss: 0.1966 - val_mean_squared_error: 0.0463\n",
            "Epoch 122/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2020 - mean_squared_error: 0.0790\n",
            "Epoch 122: val_loss did not improve from 0.19661\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2075 - mean_squared_error: 0.0815 - val_loss: 0.2368 - val_mean_squared_error: 0.0921\n",
            "Epoch 123/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2144 - mean_squared_error: 0.0918\n",
            "Epoch 123: val_loss improved from 0.19661 to 0.18915, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 53ms/step - loss: 0.2108 - mean_squared_error: 0.0887 - val_loss: 0.1891 - val_mean_squared_error: 0.0441\n",
            "Epoch 124/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1819 - mean_squared_error: 0.0637\n",
            "Epoch 124: val_loss did not improve from 0.18915\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2005 - mean_squared_error: 0.0771 - val_loss: 0.2044 - val_mean_squared_error: 0.0537\n",
            "Epoch 125/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2047 - mean_squared_error: 0.0802\n",
            "Epoch 125: val_loss improved from 0.18915 to 0.18868, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 59ms/step - loss: 0.1997 - mean_squared_error: 0.0768 - val_loss: 0.1887 - val_mean_squared_error: 0.0523\n",
            "Epoch 126/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2127 - mean_squared_error: 0.0868\n",
            "Epoch 126: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1986 - mean_squared_error: 0.0783 - val_loss: 0.1996 - val_mean_squared_error: 0.0488\n",
            "Epoch 127/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1813 - mean_squared_error: 0.0671\n",
            "Epoch 127: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1965 - mean_squared_error: 0.0757 - val_loss: 0.2037 - val_mean_squared_error: 0.0482\n",
            "Epoch 128/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2095 - mean_squared_error: 0.0851\n",
            "Epoch 128: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2036 - mean_squared_error: 0.0787 - val_loss: 0.2088 - val_mean_squared_error: 0.0605\n",
            "Epoch 129/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1909 - mean_squared_error: 0.0696\n",
            "Epoch 129: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1963 - mean_squared_error: 0.0755 - val_loss: 0.1920 - val_mean_squared_error: 0.0470\n",
            "Epoch 130/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1936 - mean_squared_error: 0.0734\n",
            "Epoch 130: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1950 - mean_squared_error: 0.0748 - val_loss: 0.1916 - val_mean_squared_error: 0.0571\n",
            "Epoch 131/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2003 - mean_squared_error: 0.0783\n",
            "Epoch 131: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2000 - mean_squared_error: 0.0776 - val_loss: 0.1983 - val_mean_squared_error: 0.0484\n",
            "Epoch 132/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2121 - mean_squared_error: 0.0880\n",
            "Epoch 132: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2105 - mean_squared_error: 0.0834 - val_loss: 0.2136 - val_mean_squared_error: 0.0525\n",
            "Epoch 133/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2055 - mean_squared_error: 0.0776\n",
            "Epoch 133: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2033 - mean_squared_error: 0.0772 - val_loss: 0.1921 - val_mean_squared_error: 0.0535\n",
            "Epoch 134/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2088 - mean_squared_error: 0.0795\n",
            "Epoch 134: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2088 - mean_squared_error: 0.0795 - val_loss: 0.2053 - val_mean_squared_error: 0.0491\n",
            "Epoch 135/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1946 - mean_squared_error: 0.0726\n",
            "Epoch 135: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2044 - mean_squared_error: 0.0810 - val_loss: 0.2454 - val_mean_squared_error: 0.0648\n",
            "Epoch 136/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2268 - mean_squared_error: 0.0909\n",
            "Epoch 136: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.2268 - mean_squared_error: 0.0909 - val_loss: 0.2174 - val_mean_squared_error: 0.0834\n",
            "Epoch 137/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1946 - mean_squared_error: 0.0711\n",
            "Epoch 137: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2065 - mean_squared_error: 0.0781 - val_loss: 0.2121 - val_mean_squared_error: 0.0526\n",
            "Epoch 138/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2114 - mean_squared_error: 0.0766\n",
            "Epoch 138: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2052 - mean_squared_error: 0.0770 - val_loss: 0.1978 - val_mean_squared_error: 0.0462\n",
            "Epoch 139/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1876 - mean_squared_error: 0.0674\n",
            "Epoch 139: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1938 - mean_squared_error: 0.0725 - val_loss: 0.2278 - val_mean_squared_error: 0.0891\n",
            "Epoch 140/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1995 - mean_squared_error: 0.0780\n",
            "Epoch 140: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2017 - mean_squared_error: 0.0778 - val_loss: 0.1896 - val_mean_squared_error: 0.0451\n",
            "Epoch 141/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2029 - mean_squared_error: 0.0798\n",
            "Epoch 141: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1963 - mean_squared_error: 0.0730 - val_loss: 0.1930 - val_mean_squared_error: 0.0477\n",
            "Epoch 142/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2093 - mean_squared_error: 0.0810\n",
            "Epoch 142: val_loss did not improve from 0.18868\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2093 - mean_squared_error: 0.0810 - val_loss: 0.1915 - val_mean_squared_error: 0.0498\n",
            "Epoch 143/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1923 - mean_squared_error: 0.0710\n",
            "Epoch 143: val_loss improved from 0.18868 to 0.18138, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 0.1920 - mean_squared_error: 0.0712 - val_loss: 0.1814 - val_mean_squared_error: 0.0496\n",
            "Epoch 144/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1812 - mean_squared_error: 0.0648\n",
            "Epoch 144: val_loss did not improve from 0.18138\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1953 - mean_squared_error: 0.0748 - val_loss: 0.1822 - val_mean_squared_error: 0.0437\n",
            "Epoch 145/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1965 - mean_squared_error: 0.0752\n",
            "Epoch 145: val_loss improved from 0.18138 to 0.17965, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.1922 - mean_squared_error: 0.0720 - val_loss: 0.1796 - val_mean_squared_error: 0.0413\n",
            "Epoch 146/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.1840 - mean_squared_error: 0.0775\n",
            "Epoch 146: val_loss did not improve from 0.17965\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1959 - mean_squared_error: 0.0727 - val_loss: 0.1950 - val_mean_squared_error: 0.0551\n",
            "Epoch 147/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1993 - mean_squared_error: 0.0733\n",
            "Epoch 147: val_loss improved from 0.17965 to 0.15722, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 0.2090 - mean_squared_error: 0.0774 - val_loss: 0.1572 - val_mean_squared_error: 0.0352\n",
            "Epoch 148/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1838 - mean_squared_error: 0.0677\n",
            "Epoch 148: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2004 - mean_squared_error: 0.0781 - val_loss: 0.2123 - val_mean_squared_error: 0.0550\n",
            "Epoch 149/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2187 - mean_squared_error: 0.0855\n",
            "Epoch 149: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2157 - mean_squared_error: 0.0838 - val_loss: 0.2166 - val_mean_squared_error: 0.0919\n",
            "Epoch 150/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2246 - mean_squared_error: 0.1015\n",
            "Epoch 150: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 32ms/step - loss: 0.2200 - mean_squared_error: 0.0949 - val_loss: 0.2825 - val_mean_squared_error: 0.0889\n",
            "Epoch 151/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2244 - mean_squared_error: 0.0890\n",
            "Epoch 151: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2284 - mean_squared_error: 0.0908 - val_loss: 0.1680 - val_mean_squared_error: 0.0543\n",
            "Epoch 152/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2116 - mean_squared_error: 0.0850\n",
            "Epoch 152: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2100 - mean_squared_error: 0.0821 - val_loss: 0.1829 - val_mean_squared_error: 0.0441\n",
            "Epoch 153/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2031 - mean_squared_error: 0.0731\n",
            "Epoch 153: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2027 - mean_squared_error: 0.0747 - val_loss: 0.1982 - val_mean_squared_error: 0.0478\n",
            "Epoch 154/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2057 - mean_squared_error: 0.0795\n",
            "Epoch 154: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.1975 - mean_squared_error: 0.0733 - val_loss: 0.1875 - val_mean_squared_error: 0.0585\n",
            "Epoch 155/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1844 - mean_squared_error: 0.0693\n",
            "Epoch 155: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.1997 - mean_squared_error: 0.0792 - val_loss: 0.1722 - val_mean_squared_error: 0.0370\n",
            "Epoch 156/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1875 - mean_squared_error: 0.0677\n",
            "Epoch 156: val_loss did not improve from 0.15722\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1959 - mean_squared_error: 0.0710 - val_loss: 0.2237 - val_mean_squared_error: 0.0668\n",
            "Epoch 157/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1909 - mean_squared_error: 0.0702\n",
            "Epoch 157: val_loss improved from 0.15722 to 0.15385, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1895 - mean_squared_error: 0.0692 - val_loss: 0.1539 - val_mean_squared_error: 0.0333\n",
            "Epoch 158/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2017 - mean_squared_error: 0.0748\n",
            "Epoch 158: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1971 - mean_squared_error: 0.0723 - val_loss: 0.2078 - val_mean_squared_error: 0.0537\n",
            "Epoch 159/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2041 - mean_squared_error: 0.0722\n",
            "Epoch 159: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2032 - mean_squared_error: 0.0732 - val_loss: 0.1757 - val_mean_squared_error: 0.0569\n",
            "Epoch 160/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2002 - mean_squared_error: 0.0736\n",
            "Epoch 160: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.2002 - mean_squared_error: 0.0736 - val_loss: 0.1784 - val_mean_squared_error: 0.0394\n",
            "Epoch 161/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2017 - mean_squared_error: 0.0767\n",
            "Epoch 161: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2010 - mean_squared_error: 0.0756 - val_loss: 0.2067 - val_mean_squared_error: 0.0547\n",
            "Epoch 162/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1770 - mean_squared_error: 0.0618\n",
            "Epoch 162: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1866 - mean_squared_error: 0.0670 - val_loss: 0.1788 - val_mean_squared_error: 0.0376\n",
            "Epoch 163/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1989 - mean_squared_error: 0.0772\n",
            "Epoch 163: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1935 - mean_squared_error: 0.0706 - val_loss: 0.2099 - val_mean_squared_error: 0.0633\n",
            "Epoch 164/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1872 - mean_squared_error: 0.0640\n",
            "Epoch 164: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1878 - mean_squared_error: 0.0654 - val_loss: 0.1650 - val_mean_squared_error: 0.0350\n",
            "Epoch 165/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1981 - mean_squared_error: 0.0733\n",
            "Epoch 165: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1866 - mean_squared_error: 0.0655 - val_loss: 0.1789 - val_mean_squared_error: 0.0398\n",
            "Epoch 166/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1872 - mean_squared_error: 0.0679\n",
            "Epoch 166: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.1866 - mean_squared_error: 0.0681 - val_loss: 0.1650 - val_mean_squared_error: 0.0401\n",
            "Epoch 167/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1856 - mean_squared_error: 0.0667\n",
            "Epoch 167: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.1856 - mean_squared_error: 0.0667 - val_loss: 0.1633 - val_mean_squared_error: 0.0380\n",
            "Epoch 168/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1842 - mean_squared_error: 0.0668\n",
            "Epoch 168: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1850 - mean_squared_error: 0.0669 - val_loss: 0.1694 - val_mean_squared_error: 0.0417\n",
            "Epoch 169/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1840 - mean_squared_error: 0.0669\n",
            "Epoch 169: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1850 - mean_squared_error: 0.0680 - val_loss: 0.1760 - val_mean_squared_error: 0.0419\n",
            "Epoch 170/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1790 - mean_squared_error: 0.0617\n",
            "Epoch 170: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1912 - mean_squared_error: 0.0693 - val_loss: 0.1615 - val_mean_squared_error: 0.0373\n",
            "Epoch 171/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.2053 - mean_squared_error: 0.0786\n",
            "Epoch 171: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2170 - mean_squared_error: 0.0909 - val_loss: 0.1540 - val_mean_squared_error: 0.0437\n",
            "Epoch 172/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1978 - mean_squared_error: 0.0758\n",
            "Epoch 172: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1898 - mean_squared_error: 0.0690 - val_loss: 0.2024 - val_mean_squared_error: 0.0501\n",
            "Epoch 173/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1960 - mean_squared_error: 0.0721\n",
            "Epoch 173: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.1909 - mean_squared_error: 0.0693 - val_loss: 0.1617 - val_mean_squared_error: 0.0380\n",
            "Epoch 174/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1929 - mean_squared_error: 0.0642\n",
            "Epoch 174: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1968 - mean_squared_error: 0.0717 - val_loss: 0.2001 - val_mean_squared_error: 0.0706\n",
            "Epoch 175/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.0848\n",
            "Epoch 175: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1964 - mean_squared_error: 0.0739 - val_loss: 0.1873 - val_mean_squared_error: 0.0429\n",
            "Epoch 176/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1947 - mean_squared_error: 0.0700\n",
            "Epoch 176: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1947 - mean_squared_error: 0.0700 - val_loss: 0.1909 - val_mean_squared_error: 0.0519\n",
            "Epoch 177/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2026 - mean_squared_error: 0.0721\n",
            "Epoch 177: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.1920 - mean_squared_error: 0.0687 - val_loss: 0.1696 - val_mean_squared_error: 0.0459\n",
            "Epoch 178/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1792 - mean_squared_error: 0.0626\n",
            "Epoch 178: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1844 - mean_squared_error: 0.0680 - val_loss: 0.1589 - val_mean_squared_error: 0.0389\n",
            "Epoch 179/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1878 - mean_squared_error: 0.0677\n",
            "Epoch 179: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1878 - mean_squared_error: 0.0677 - val_loss: 0.1666 - val_mean_squared_error: 0.0364\n",
            "Epoch 180/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1241 - mean_squared_error: 0.0285\n",
            "Epoch 180: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1935 - mean_squared_error: 0.0716 - val_loss: 0.1744 - val_mean_squared_error: 0.0387\n",
            "Epoch 181/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2671 - mean_squared_error: 0.1366\n",
            "Epoch 181: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.1985 - mean_squared_error: 0.0745 - val_loss: 0.1704 - val_mean_squared_error: 0.0411\n",
            "Epoch 182/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1804 - mean_squared_error: 0.0603\n",
            "Epoch 182: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1816 - mean_squared_error: 0.0639 - val_loss: 0.1693 - val_mean_squared_error: 0.0378\n",
            "Epoch 183/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1943 - mean_squared_error: 0.0603\n",
            "Epoch 183: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1867 - mean_squared_error: 0.0664 - val_loss: 0.1747 - val_mean_squared_error: 0.0395\n",
            "Epoch 184/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1825 - mean_squared_error: 0.0522\n",
            "Epoch 184: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1874 - mean_squared_error: 0.0648 - val_loss: 0.1768 - val_mean_squared_error: 0.0484\n",
            "Epoch 185/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1872 - mean_squared_error: 0.0660\n",
            "Epoch 185: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1872 - mean_squared_error: 0.0660 - val_loss: 0.1652 - val_mean_squared_error: 0.0356\n",
            "Epoch 186/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2162 - mean_squared_error: 0.0906\n",
            "Epoch 186: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1880 - mean_squared_error: 0.0665 - val_loss: 0.2072 - val_mean_squared_error: 0.0811\n",
            "Epoch 187/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1936 - mean_squared_error: 0.0699\n",
            "Epoch 187: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1936 - mean_squared_error: 0.0699 - val_loss: 0.1955 - val_mean_squared_error: 0.0450\n",
            "Epoch 188/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1772 - mean_squared_error: 0.0587\n",
            "Epoch 188: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2017 - mean_squared_error: 0.0758 - val_loss: 0.2025 - val_mean_squared_error: 0.0712\n",
            "Epoch 189/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1646 - mean_squared_error: 0.0697\n",
            "Epoch 189: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2024 - mean_squared_error: 0.0743 - val_loss: 0.1940 - val_mean_squared_error: 0.0570\n",
            "Epoch 190/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1923 - mean_squared_error: 0.0712\n",
            "Epoch 190: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1923 - mean_squared_error: 0.0712 - val_loss: 0.1970 - val_mean_squared_error: 0.0451\n",
            "Epoch 191/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1933 - mean_squared_error: 0.0667\n",
            "Epoch 191: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1933 - mean_squared_error: 0.0667 - val_loss: 0.1699 - val_mean_squared_error: 0.0410\n",
            "Epoch 192/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1800 - mean_squared_error: 0.0625\n",
            "Epoch 192: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1837 - mean_squared_error: 0.0642 - val_loss: 0.1574 - val_mean_squared_error: 0.0375\n",
            "Epoch 193/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1934 - mean_squared_error: 0.0728\n",
            "Epoch 193: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.1856 - mean_squared_error: 0.0650 - val_loss: 0.1722 - val_mean_squared_error: 0.0373\n",
            "Epoch 194/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1313 - mean_squared_error: 0.0276\n",
            "Epoch 194: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1828 - mean_squared_error: 0.0645 - val_loss: 0.1597 - val_mean_squared_error: 0.0340\n",
            "Epoch 195/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1840 - mean_squared_error: 0.0668\n",
            "Epoch 195: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1811 - mean_squared_error: 0.0634 - val_loss: 0.1639 - val_mean_squared_error: 0.0366\n",
            "Epoch 196/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1809 - mean_squared_error: 0.0643\n",
            "Epoch 196: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.1809 - mean_squared_error: 0.0643 - val_loss: 0.1760 - val_mean_squared_error: 0.0396\n",
            "Epoch 197/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1857 - mean_squared_error: 0.0649\n",
            "Epoch 197: val_loss did not improve from 0.15385\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.1854 - mean_squared_error: 0.0642 - val_loss: 0.1867 - val_mean_squared_error: 0.0568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 637 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cc1d1519090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 203ms/step\n",
            "Cycle:  5\n",
            "start compiling\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            30          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            273905      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            18          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1)            0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5)            0           ['dense_1[0][0]',                \n",
            "                                                                  'flatten[0][0]',                \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           384         ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 274,402\n",
            "Trainable params: 1,042\n",
            "Non-trainable params: 273,360\n",
            "__________________________________________________________________________________________________\n",
            "start fitting\n",
            "Epoch 1/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 8.0909 - mean_squared_error: 156.9922\n",
            "Epoch 1: val_loss improved from inf to 6.90021, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 3s 92ms/step - loss: 7.9834 - mean_squared_error: 144.3511 - val_loss: 6.9002 - val_mean_squared_error: 98.5969\n",
            "Epoch 2/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 4.2577 - mean_squared_error: 40.7785\n",
            "Epoch 2: val_loss improved from 6.90021 to 2.85119, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 4.1366 - mean_squared_error: 39.1481 - val_loss: 2.8512 - val_mean_squared_error: 14.8387\n",
            "Epoch 3/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.6883 - mean_squared_error: 4.8575\n",
            "Epoch 3: val_loss improved from 2.85119 to 1.01116, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 1.1512 - mean_squared_error: 3.1792 - val_loss: 1.0112 - val_mean_squared_error: 1.7441\n",
            "Epoch 4/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 1.4091 - mean_squared_error: 3.3327\n",
            "Epoch 4: val_loss improved from 1.01116 to 0.68368, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 1.3901 - mean_squared_error: 3.2457 - val_loss: 0.6837 - val_mean_squared_error: 0.8000\n",
            "Epoch 5/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 1.1367 - mean_squared_error: 2.5961\n",
            "Epoch 5: val_loss did not improve from 0.68368\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.8399 - mean_squared_error: 1.3473 - val_loss: 1.1910 - val_mean_squared_error: 1.7460\n",
            "Epoch 6/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.7703 - mean_squared_error: 0.8881\n",
            "Epoch 6: val_loss improved from 0.68368 to 0.64410, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.7715 - mean_squared_error: 1.0546 - val_loss: 0.6441 - val_mean_squared_error: 0.5941\n",
            "Epoch 7/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.6021 - mean_squared_error: 0.5996\n",
            "Epoch 7: val_loss improved from 0.64410 to 0.60239, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.6153 - mean_squared_error: 0.6756 - val_loss: 0.6024 - val_mean_squared_error: 0.5445\n",
            "Epoch 8/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.6611 - mean_squared_error: 0.9373\n",
            "Epoch 8: val_loss did not improve from 0.60239\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.5636 - mean_squared_error: 0.5225 - val_loss: 0.6265 - val_mean_squared_error: 0.5663\n",
            "Epoch 9/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5155 - mean_squared_error: 0.5050\n",
            "Epoch 9: val_loss improved from 0.60239 to 0.58853, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 0.5083 - mean_squared_error: 0.4783 - val_loss: 0.5885 - val_mean_squared_error: 0.5037\n",
            "Epoch 10/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.5035 - mean_squared_error: 0.4143\n",
            "Epoch 10: val_loss improved from 0.58853 to 0.58809, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.4807 - mean_squared_error: 0.3852 - val_loss: 0.5881 - val_mean_squared_error: 0.5824\n",
            "Epoch 11/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.4493 - mean_squared_error: 0.3546\n",
            "Epoch 11: val_loss improved from 0.58809 to 0.56487, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.4582 - mean_squared_error: 0.3880 - val_loss: 0.5649 - val_mean_squared_error: 0.4638\n",
            "Epoch 12/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.4388 - mean_squared_error: 0.3339\n",
            "Epoch 12: val_loss improved from 0.56487 to 0.52458, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.4255 - mean_squared_error: 0.3236 - val_loss: 0.5246 - val_mean_squared_error: 0.4420\n",
            "Epoch 13/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.4112 - mean_squared_error: 0.2902\n",
            "Epoch 13: val_loss improved from 0.52458 to 0.50958, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.4122 - mean_squared_error: 0.3009 - val_loss: 0.5096 - val_mean_squared_error: 0.4126\n",
            "Epoch 14/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3946 - mean_squared_error: 0.2851\n",
            "Epoch 14: val_loss improved from 0.50958 to 0.50559, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3946 - mean_squared_error: 0.2851 - val_loss: 0.5056 - val_mean_squared_error: 0.4024\n",
            "Epoch 15/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.3930 - mean_squared_error: 0.2969\n",
            "Epoch 15: val_loss improved from 0.50559 to 0.49136, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.4169 - mean_squared_error: 0.3114 - val_loss: 0.4914 - val_mean_squared_error: 0.3826\n",
            "Epoch 16/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.5493 - mean_squared_error: 0.5483\n",
            "Epoch 16: val_loss did not improve from 0.49136\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.3985 - mean_squared_error: 0.2820 - val_loss: 0.5710 - val_mean_squared_error: 0.5639\n",
            "Epoch 17/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4214 - mean_squared_error: 0.3232\n",
            "Epoch 17: val_loss improved from 0.49136 to 0.48301, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.4153 - mean_squared_error: 0.3340 - val_loss: 0.4830 - val_mean_squared_error: 0.3651\n",
            "Epoch 18/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3871 - mean_squared_error: 0.2826\n",
            "Epoch 18: val_loss improved from 0.48301 to 0.47442, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3871 - mean_squared_error: 0.2826 - val_loss: 0.4744 - val_mean_squared_error: 0.3399\n",
            "Epoch 19/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3608 - mean_squared_error: 0.2395\n",
            "Epoch 19: val_loss improved from 0.47442 to 0.46831, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3608 - mean_squared_error: 0.2395 - val_loss: 0.4683 - val_mean_squared_error: 0.3523\n",
            "Epoch 20/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3571 - mean_squared_error: 0.2398\n",
            "Epoch 20: val_loss improved from 0.46831 to 0.44168, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.3607 - mean_squared_error: 0.2405 - val_loss: 0.4417 - val_mean_squared_error: 0.3041\n",
            "Epoch 21/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.4161 - mean_squared_error: 0.2694\n",
            "Epoch 21: val_loss improved from 0.44168 to 0.43130, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.3527 - mean_squared_error: 0.2320 - val_loss: 0.4313 - val_mean_squared_error: 0.2857\n",
            "Epoch 22/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3504 - mean_squared_error: 0.2298\n",
            "Epoch 22: val_loss did not improve from 0.43130\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.3459 - mean_squared_error: 0.2212 - val_loss: 0.4509 - val_mean_squared_error: 0.3142\n",
            "Epoch 23/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2912 - mean_squared_error: 0.2570\n",
            "Epoch 23: val_loss did not improve from 0.43130\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.3422 - mean_squared_error: 0.2209 - val_loss: 0.4390 - val_mean_squared_error: 0.2945\n",
            "Epoch 24/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2919 - mean_squared_error: 0.1588\n",
            "Epoch 24: val_loss did not improve from 0.43130\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.3426 - mean_squared_error: 0.2127 - val_loss: 0.4440 - val_mean_squared_error: 0.3177\n",
            "Epoch 25/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3663 - mean_squared_error: 0.2558\n",
            "Epoch 25: val_loss did not improve from 0.43130\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3524 - mean_squared_error: 0.2383 - val_loss: 0.4557 - val_mean_squared_error: 0.2916\n",
            "Epoch 26/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3253 - mean_squared_error: 0.1946\n",
            "Epoch 26: val_loss improved from 0.43130 to 0.41876, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.3281 - mean_squared_error: 0.1991 - val_loss: 0.4188 - val_mean_squared_error: 0.2599\n",
            "Epoch 27/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3189 - mean_squared_error: 0.1804\n",
            "Epoch 27: val_loss improved from 0.41876 to 0.41379, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.3188 - mean_squared_error: 0.1896 - val_loss: 0.4138 - val_mean_squared_error: 0.2509\n",
            "Epoch 28/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3448 - mean_squared_error: 0.2297\n",
            "Epoch 28: val_loss did not improve from 0.41379\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.3453 - mean_squared_error: 0.2151 - val_loss: 0.4256 - val_mean_squared_error: 0.2685\n",
            "Epoch 29/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3439 - mean_squared_error: 0.2148\n",
            "Epoch 29: val_loss did not improve from 0.41379\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.3249 - mean_squared_error: 0.1932 - val_loss: 0.4326 - val_mean_squared_error: 0.2784\n",
            "Epoch 30/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3049 - mean_squared_error: 0.1757\n",
            "Epoch 30: val_loss did not improve from 0.41379\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.3049 - mean_squared_error: 0.1757 - val_loss: 0.4679 - val_mean_squared_error: 0.2795\n",
            "Epoch 31/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3769 - mean_squared_error: 0.2460\n",
            "Epoch 31: val_loss did not improve from 0.41379\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.3755 - mean_squared_error: 0.2368 - val_loss: 0.4671 - val_mean_squared_error: 0.3771\n",
            "Epoch 32/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.3505 - mean_squared_error: 0.2261\n",
            "Epoch 32: val_loss improved from 0.41379 to 0.41094, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.3485 - mean_squared_error: 0.2229 - val_loss: 0.4109 - val_mean_squared_error: 0.2492\n",
            "Epoch 33/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.3175 - mean_squared_error: 0.1783\n",
            "Epoch 33: val_loss did not improve from 0.41094\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.3212 - mean_squared_error: 0.1859 - val_loss: 0.4378 - val_mean_squared_error: 0.2720\n",
            "Epoch 34/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3050 - mean_squared_error: 0.1710\n",
            "Epoch 34: val_loss improved from 0.41094 to 0.39000, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.3026 - mean_squared_error: 0.1756 - val_loss: 0.3900 - val_mean_squared_error: 0.2136\n",
            "Epoch 35/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2965 - mean_squared_error: 0.1752\n",
            "Epoch 35: val_loss improved from 0.39000 to 0.38542, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2955 - mean_squared_error: 0.1646 - val_loss: 0.3854 - val_mean_squared_error: 0.2045\n",
            "Epoch 36/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2700 - mean_squared_error: 0.1384\n",
            "Epoch 36: val_loss did not improve from 0.38542\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2867 - mean_squared_error: 0.1575 - val_loss: 0.3969 - val_mean_squared_error: 0.2096\n",
            "Epoch 37/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2974 - mean_squared_error: 0.1721\n",
            "Epoch 37: val_loss did not improve from 0.38542\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2883 - mean_squared_error: 0.1568 - val_loss: 0.3963 - val_mean_squared_error: 0.2057\n",
            "Epoch 38/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2765 - mean_squared_error: 0.1409\n",
            "Epoch 38: val_loss improved from 0.38542 to 0.37555, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2894 - mean_squared_error: 0.1574 - val_loss: 0.3755 - val_mean_squared_error: 0.1919\n",
            "Epoch 39/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2548 - mean_squared_error: 0.1296\n",
            "Epoch 39: val_loss did not improve from 0.37555\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2781 - mean_squared_error: 0.1492 - val_loss: 0.3921 - val_mean_squared_error: 0.2087\n",
            "Epoch 40/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2890 - mean_squared_error: 0.1598\n",
            "Epoch 40: val_loss did not improve from 0.37555\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2786 - mean_squared_error: 0.1498 - val_loss: 0.3867 - val_mean_squared_error: 0.2054\n",
            "Epoch 41/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2819 - mean_squared_error: 0.1496\n",
            "Epoch 41: val_loss improved from 0.37555 to 0.37475, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2819 - mean_squared_error: 0.1496 - val_loss: 0.3748 - val_mean_squared_error: 0.2120\n",
            "Epoch 42/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2796 - mean_squared_error: 0.1319\n",
            "Epoch 42: val_loss did not improve from 0.37475\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2859 - mean_squared_error: 0.1538 - val_loss: 0.4370 - val_mean_squared_error: 0.3017\n",
            "Epoch 43/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.3408 - mean_squared_error: 0.2548\n",
            "Epoch 43: val_loss did not improve from 0.37475\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.3429 - mean_squared_error: 0.2421 - val_loss: 0.4646 - val_mean_squared_error: 0.2584\n",
            "Epoch 44/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2971 - mean_squared_error: 0.1553\n",
            "Epoch 44: val_loss improved from 0.37475 to 0.36044, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.2956 - mean_squared_error: 0.1708 - val_loss: 0.3604 - val_mean_squared_error: 0.1870\n",
            "Epoch 45/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2658 - mean_squared_error: 0.1301\n",
            "Epoch 45: val_loss did not improve from 0.36044\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2728 - mean_squared_error: 0.1394 - val_loss: 0.3658 - val_mean_squared_error: 0.1866\n",
            "Epoch 46/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2615 - mean_squared_error: 0.1262\n",
            "Epoch 46: val_loss did not improve from 0.36044\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2672 - mean_squared_error: 0.1364 - val_loss: 0.3804 - val_mean_squared_error: 0.1926\n",
            "Epoch 47/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2849 - mean_squared_error: 0.1507\n",
            "Epoch 47: val_loss did not improve from 0.36044\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2729 - mean_squared_error: 0.1387 - val_loss: 0.3933 - val_mean_squared_error: 0.2076\n",
            "Epoch 48/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2562 - mean_squared_error: 0.1180\n",
            "Epoch 48: val_loss improved from 0.36044 to 0.34793, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 56ms/step - loss: 0.2598 - mean_squared_error: 0.1289 - val_loss: 0.3479 - val_mean_squared_error: 0.1646\n",
            "Epoch 49/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2743 - mean_squared_error: 0.1403\n",
            "Epoch 49: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 35ms/step - loss: 0.2778 - mean_squared_error: 0.1390 - val_loss: 0.3757 - val_mean_squared_error: 0.1851\n",
            "Epoch 50/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2798 - mean_squared_error: 0.1453\n",
            "Epoch 50: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2801 - mean_squared_error: 0.1441 - val_loss: 0.3483 - val_mean_squared_error: 0.1720\n",
            "Epoch 51/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.1473\n",
            "Epoch 51: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2882 - mean_squared_error: 0.1527 - val_loss: 0.3852 - val_mean_squared_error: 0.1970\n",
            "Epoch 52/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2638 - mean_squared_error: 0.1213\n",
            "Epoch 52: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.2778 - mean_squared_error: 0.1471 - val_loss: 0.3630 - val_mean_squared_error: 0.1741\n",
            "Epoch 53/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2557 - mean_squared_error: 0.1261\n",
            "Epoch 53: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2596 - mean_squared_error: 0.1286 - val_loss: 0.3853 - val_mean_squared_error: 0.2284\n",
            "Epoch 54/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2667 - mean_squared_error: 0.1314\n",
            "Epoch 54: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.2667 - mean_squared_error: 0.1314 - val_loss: 0.4255 - val_mean_squared_error: 0.2872\n",
            "Epoch 55/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2649 - mean_squared_error: 0.1327\n",
            "Epoch 55: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.2690 - mean_squared_error: 0.1352 - val_loss: 0.3866 - val_mean_squared_error: 0.2140\n",
            "Epoch 56/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2550 - mean_squared_error: 0.1258\n",
            "Epoch 56: val_loss did not improve from 0.34793\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.2548 - mean_squared_error: 0.1238 - val_loss: 0.3571 - val_mean_squared_error: 0.1780\n",
            "Epoch 57/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2465 - mean_squared_error: 0.1158\n",
            "Epoch 57: val_loss improved from 0.34793 to 0.34761, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 1s 65ms/step - loss: 0.2465 - mean_squared_error: 0.1158 - val_loss: 0.3476 - val_mean_squared_error: 0.1649\n",
            "Epoch 58/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.1187\n",
            "Epoch 58: val_loss improved from 0.34761 to 0.34559, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 52ms/step - loss: 0.2493 - mean_squared_error: 0.1187 - val_loss: 0.3456 - val_mean_squared_error: 0.1819\n",
            "Epoch 59/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2605 - mean_squared_error: 0.1231\n",
            "Epoch 59: val_loss did not improve from 0.34559\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2664 - mean_squared_error: 0.1369 - val_loss: 0.4323 - val_mean_squared_error: 0.3007\n",
            "Epoch 60/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2646 - mean_squared_error: 0.1332\n",
            "Epoch 60: val_loss did not improve from 0.34559\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.2646 - mean_squared_error: 0.1317 - val_loss: 0.4124 - val_mean_squared_error: 0.2653\n",
            "Epoch 61/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2551 - mean_squared_error: 0.1236\n",
            "Epoch 61: val_loss improved from 0.34559 to 0.34157, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2505 - mean_squared_error: 0.1183 - val_loss: 0.3416 - val_mean_squared_error: 0.1594\n",
            "Epoch 62/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2412 - mean_squared_error: 0.1168\n",
            "Epoch 62: val_loss improved from 0.34157 to 0.33285, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2443 - mean_squared_error: 0.1119 - val_loss: 0.3328 - val_mean_squared_error: 0.1543\n",
            "Epoch 63/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1906 - mean_squared_error: 0.0819\n",
            "Epoch 63: val_loss did not improve from 0.33285\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2424 - mean_squared_error: 0.1087 - val_loss: 0.3400 - val_mean_squared_error: 0.1602\n",
            "Epoch 64/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1970 - mean_squared_error: 0.0726\n",
            "Epoch 64: val_loss did not improve from 0.33285\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2447 - mean_squared_error: 0.1104 - val_loss: 0.3810 - val_mean_squared_error: 0.2081\n",
            "Epoch 65/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2933 - mean_squared_error: 0.1230\n",
            "Epoch 65: val_loss improved from 0.33285 to 0.31030, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2544 - mean_squared_error: 0.1173 - val_loss: 0.3103 - val_mean_squared_error: 0.1445\n",
            "Epoch 66/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2501 - mean_squared_error: 0.1172\n",
            "Epoch 66: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2501 - mean_squared_error: 0.1172 - val_loss: 0.3672 - val_mean_squared_error: 0.1736\n",
            "Epoch 67/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2473 - mean_squared_error: 0.1131\n",
            "Epoch 67: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2473 - mean_squared_error: 0.1131 - val_loss: 0.4463 - val_mean_squared_error: 0.2319\n",
            "Epoch 68/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3510 - mean_squared_error: 0.2436\n",
            "Epoch 68: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2831 - mean_squared_error: 0.1641 - val_loss: 0.3364 - val_mean_squared_error: 0.1681\n",
            "Epoch 69/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2366 - mean_squared_error: 0.1008\n",
            "Epoch 69: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2366 - mean_squared_error: 0.1008 - val_loss: 0.3486 - val_mean_squared_error: 0.1741\n",
            "Epoch 70/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2617 - mean_squared_error: 0.1297\n",
            "Epoch 70: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2396 - mean_squared_error: 0.1056 - val_loss: 0.3411 - val_mean_squared_error: 0.1589\n",
            "Epoch 71/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2295 - mean_squared_error: 0.0988\n",
            "Epoch 71: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2295 - mean_squared_error: 0.0988 - val_loss: 0.3281 - val_mean_squared_error: 0.1480\n",
            "Epoch 72/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1835 - mean_squared_error: 0.0585\n",
            "Epoch 72: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2272 - mean_squared_error: 0.0970 - val_loss: 0.3744 - val_mean_squared_error: 0.2112\n",
            "Epoch 73/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2365 - mean_squared_error: 0.1034\n",
            "Epoch 73: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2372 - mean_squared_error: 0.1027 - val_loss: 0.3510 - val_mean_squared_error: 0.1734\n",
            "Epoch 74/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2716 - mean_squared_error: 0.1668\n",
            "Epoch 74: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2283 - mean_squared_error: 0.0955 - val_loss: 0.3317 - val_mean_squared_error: 0.1566\n",
            "Epoch 75/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2289 - mean_squared_error: 0.0996\n",
            "Epoch 75: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2293 - mean_squared_error: 0.0983 - val_loss: 0.3538 - val_mean_squared_error: 0.1709\n",
            "Epoch 76/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1787 - mean_squared_error: 0.0621\n",
            "Epoch 76: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2285 - mean_squared_error: 0.0981 - val_loss: 0.3112 - val_mean_squared_error: 0.1387\n",
            "Epoch 77/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1468 - mean_squared_error: 0.0460\n",
            "Epoch 77: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2416 - mean_squared_error: 0.1110 - val_loss: 0.3780 - val_mean_squared_error: 0.1754\n",
            "Epoch 78/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2642 - mean_squared_error: 0.1311\n",
            "Epoch 78: val_loss did not improve from 0.31030\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2642 - mean_squared_error: 0.1311 - val_loss: 0.3497 - val_mean_squared_error: 0.1690\n",
            "Epoch 79/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2572 - mean_squared_error: 0.1211\n",
            "Epoch 79: val_loss improved from 0.31030 to 0.30884, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2572 - mean_squared_error: 0.1211 - val_loss: 0.3088 - val_mean_squared_error: 0.1380\n",
            "Epoch 80/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2094 - mean_squared_error: 0.0697\n",
            "Epoch 80: val_loss did not improve from 0.30884\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2309 - mean_squared_error: 0.0995 - val_loss: 0.3421 - val_mean_squared_error: 0.1582\n",
            "Epoch 81/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2284 - mean_squared_error: 0.0864\n",
            "Epoch 81: val_loss did not improve from 0.30884\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2412 - mean_squared_error: 0.1093 - val_loss: 0.3455 - val_mean_squared_error: 0.1751\n",
            "Epoch 82/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3719 - mean_squared_error: 0.2344\n",
            "Epoch 82: val_loss did not improve from 0.30884\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2259 - mean_squared_error: 0.0954 - val_loss: 0.3103 - val_mean_squared_error: 0.1381\n",
            "Epoch 83/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2078 - mean_squared_error: 0.0944\n",
            "Epoch 83: val_loss improved from 0.30884 to 0.30526, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2255 - mean_squared_error: 0.0952 - val_loss: 0.3053 - val_mean_squared_error: 0.1326\n",
            "Epoch 84/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2357 - mean_squared_error: 0.1029\n",
            "Epoch 84: val_loss did not improve from 0.30526\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2370 - mean_squared_error: 0.1040 - val_loss: 0.3173 - val_mean_squared_error: 0.1450\n",
            "Epoch 85/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2199 - mean_squared_error: 0.0968\n",
            "Epoch 85: val_loss improved from 0.30526 to 0.29943, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2232 - mean_squared_error: 0.0952 - val_loss: 0.2994 - val_mean_squared_error: 0.1292\n",
            "Epoch 86/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2230 - mean_squared_error: 0.0898\n",
            "Epoch 86: val_loss did not improve from 0.29943\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2230 - mean_squared_error: 0.0898 - val_loss: 0.3162 - val_mean_squared_error: 0.1410\n",
            "Epoch 87/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1721 - mean_squared_error: 0.0462\n",
            "Epoch 87: val_loss did not improve from 0.29943\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2344 - mean_squared_error: 0.1020 - val_loss: 0.3174 - val_mean_squared_error: 0.1299\n",
            "Epoch 88/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2244 - mean_squared_error: 0.0926\n",
            "Epoch 88: val_loss improved from 0.29943 to 0.28710, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2331 - mean_squared_error: 0.1064 - val_loss: 0.2871 - val_mean_squared_error: 0.1185\n",
            "Epoch 89/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2232 - mean_squared_error: 0.0929\n",
            "Epoch 89: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2232 - mean_squared_error: 0.0929 - val_loss: 0.3631 - val_mean_squared_error: 0.1549\n",
            "Epoch 90/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2967 - mean_squared_error: 0.1472\n",
            "Epoch 90: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2496 - mean_squared_error: 0.1158 - val_loss: 0.3187 - val_mean_squared_error: 0.1294\n",
            "Epoch 91/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2196 - mean_squared_error: 0.0815\n",
            "Epoch 91: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.2552 - mean_squared_error: 0.1357 - val_loss: 0.3780 - val_mean_squared_error: 0.2468\n",
            "Epoch 92/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2326 - mean_squared_error: 0.1028\n",
            "Epoch 92: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2291 - mean_squared_error: 0.0997 - val_loss: 0.3056 - val_mean_squared_error: 0.1279\n",
            "Epoch 93/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2337 - mean_squared_error: 0.1034\n",
            "Epoch 93: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2337 - mean_squared_error: 0.1034 - val_loss: 0.2877 - val_mean_squared_error: 0.1173\n",
            "Epoch 94/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2275 - mean_squared_error: 0.0989\n",
            "Epoch 94: val_loss did not improve from 0.28710\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2275 - mean_squared_error: 0.0989 - val_loss: 0.3361 - val_mean_squared_error: 0.1362\n",
            "Epoch 95/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2450 - mean_squared_error: 0.0971\n",
            "Epoch 95: val_loss improved from 0.28710 to 0.26628, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2211 - mean_squared_error: 0.0908 - val_loss: 0.2663 - val_mean_squared_error: 0.1083\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2208 - mean_squared_error: 0.0919\n",
            "Epoch 96: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2208 - mean_squared_error: 0.0919 - val_loss: 0.2956 - val_mean_squared_error: 0.1212\n",
            "Epoch 97/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1988 - mean_squared_error: 0.0768\n",
            "Epoch 97: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2317 - mean_squared_error: 0.0978 - val_loss: 0.2669 - val_mean_squared_error: 0.1050\n",
            "Epoch 98/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2129 - mean_squared_error: 0.0864\n",
            "Epoch 98: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2129 - mean_squared_error: 0.0864 - val_loss: 0.2929 - val_mean_squared_error: 0.1157\n",
            "Epoch 99/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2608 - mean_squared_error: 0.1264\n",
            "Epoch 99: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2113 - mean_squared_error: 0.0845 - val_loss: 0.3218 - val_mean_squared_error: 0.1276\n",
            "Epoch 100/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2188 - mean_squared_error: 0.1305\n",
            "Epoch 100: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2333 - mean_squared_error: 0.1093 - val_loss: 0.3075 - val_mean_squared_error: 0.1334\n",
            "Epoch 101/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1815 - mean_squared_error: 0.0627\n",
            "Epoch 101: val_loss did not improve from 0.26628\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2167 - mean_squared_error: 0.0838 - val_loss: 0.2928 - val_mean_squared_error: 0.1170\n",
            "Epoch 102/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2239 - mean_squared_error: 0.0942\n",
            "Epoch 102: val_loss improved from 0.26628 to 0.26308, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2239 - mean_squared_error: 0.0942 - val_loss: 0.2631 - val_mean_squared_error: 0.1008\n",
            "Epoch 103/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2131 - mean_squared_error: 0.0864\n",
            "Epoch 103: val_loss did not improve from 0.26308\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2131 - mean_squared_error: 0.0864 - val_loss: 0.3014 - val_mean_squared_error: 0.1239\n",
            "Epoch 104/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1959 - mean_squared_error: 0.0783\n",
            "Epoch 104: val_loss improved from 0.26308 to 0.25417, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2273 - mean_squared_error: 0.0925 - val_loss: 0.2542 - val_mean_squared_error: 0.1025\n",
            "Epoch 105/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2052 - mean_squared_error: 0.0933\n",
            "Epoch 105: val_loss did not improve from 0.25417\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2171 - mean_squared_error: 0.0888 - val_loss: 0.2692 - val_mean_squared_error: 0.1056\n",
            "Epoch 106/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.1173\n",
            "Epoch 106: val_loss did not improve from 0.25417\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2269 - mean_squared_error: 0.0987 - val_loss: 0.2802 - val_mean_squared_error: 0.1127\n",
            "Epoch 107/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2213 - mean_squared_error: 0.1351\n",
            "Epoch 107: val_loss did not improve from 0.25417\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2294 - mean_squared_error: 0.1081 - val_loss: 0.2687 - val_mean_squared_error: 0.1158\n",
            "Epoch 108/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1991 - mean_squared_error: 0.0736\n",
            "Epoch 108: val_loss did not improve from 0.25417\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2306 - mean_squared_error: 0.0999 - val_loss: 0.2809 - val_mean_squared_error: 0.1095\n",
            "Epoch 109/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2139 - mean_squared_error: 0.0882\n",
            "Epoch 109: val_loss did not improve from 0.25417\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2139 - mean_squared_error: 0.0882 - val_loss: 0.3114 - val_mean_squared_error: 0.1683\n",
            "Epoch 110/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2948 - mean_squared_error: 0.1398\n",
            "Epoch 110: val_loss improved from 0.25417 to 0.24556, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2321 - mean_squared_error: 0.0981 - val_loss: 0.2456 - val_mean_squared_error: 0.0915\n",
            "Epoch 111/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.0955\n",
            "Epoch 111: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.2258 - mean_squared_error: 0.0955 - val_loss: 0.3351 - val_mean_squared_error: 0.1324\n",
            "Epoch 112/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2589 - mean_squared_error: 0.1295\n",
            "Epoch 112: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2297 - mean_squared_error: 0.0981 - val_loss: 0.2484 - val_mean_squared_error: 0.0928\n",
            "Epoch 113/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1896 - mean_squared_error: 0.0598\n",
            "Epoch 113: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2251 - mean_squared_error: 0.1016 - val_loss: 0.3279 - val_mean_squared_error: 0.1836\n",
            "Epoch 114/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3487 - mean_squared_error: 0.1955\n",
            "Epoch 114: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2514 - mean_squared_error: 0.1107 - val_loss: 0.3492 - val_mean_squared_error: 0.2278\n",
            "Epoch 115/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2529 - mean_squared_error: 0.1227\n",
            "Epoch 115: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2529 - mean_squared_error: 0.1227 - val_loss: 0.2707 - val_mean_squared_error: 0.0985\n",
            "Epoch 116/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1530 - mean_squared_error: 0.0374\n",
            "Epoch 116: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2120 - mean_squared_error: 0.0835 - val_loss: 0.2513 - val_mean_squared_error: 0.0940\n",
            "Epoch 117/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1695 - mean_squared_error: 0.0497\n",
            "Epoch 117: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2355 - mean_squared_error: 0.1034 - val_loss: 0.2989 - val_mean_squared_error: 0.1676\n",
            "Epoch 118/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.3403 - mean_squared_error: 0.2045\n",
            "Epoch 118: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2723 - mean_squared_error: 0.1448 - val_loss: 0.2488 - val_mean_squared_error: 0.1063\n",
            "Epoch 119/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1713 - mean_squared_error: 0.0523\n",
            "Epoch 119: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2344 - mean_squared_error: 0.0994 - val_loss: 0.2465 - val_mean_squared_error: 0.0928\n",
            "Epoch 120/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2313 - mean_squared_error: 0.1272\n",
            "Epoch 120: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2068 - mean_squared_error: 0.0775 - val_loss: 0.2911 - val_mean_squared_error: 0.1341\n",
            "Epoch 121/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2250 - mean_squared_error: 0.1045\n",
            "Epoch 121: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2287 - mean_squared_error: 0.1056 - val_loss: 0.2834 - val_mean_squared_error: 0.1032\n",
            "Epoch 122/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2146 - mean_squared_error: 0.0869\n",
            "Epoch 122: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2146 - mean_squared_error: 0.0869 - val_loss: 0.2872 - val_mean_squared_error: 0.1053\n",
            "Epoch 123/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2373 - mean_squared_error: 0.1198\n",
            "Epoch 123: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2177 - mean_squared_error: 0.0914 - val_loss: 0.2750 - val_mean_squared_error: 0.0990\n",
            "Epoch 124/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2195 - mean_squared_error: 0.0949\n",
            "Epoch 124: val_loss did not improve from 0.24556\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2268 - mean_squared_error: 0.1025 - val_loss: 0.2535 - val_mean_squared_error: 0.0973\n",
            "Epoch 125/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1966 - mean_squared_error: 0.0620\n",
            "Epoch 125: val_loss improved from 0.24556 to 0.23925, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2122 - mean_squared_error: 0.0820 - val_loss: 0.2392 - val_mean_squared_error: 0.0913\n",
            "Epoch 126/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2915 - mean_squared_error: 0.1272\n",
            "Epoch 126: val_loss did not improve from 0.23925\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2180 - mean_squared_error: 0.0900 - val_loss: 0.3070 - val_mean_squared_error: 0.1143\n",
            "Epoch 127/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1688 - mean_squared_error: 0.0572\n",
            "Epoch 127: val_loss did not improve from 0.23925\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2192 - mean_squared_error: 0.0936 - val_loss: 0.2580 - val_mean_squared_error: 0.0923\n",
            "Epoch 128/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.0820\n",
            "Epoch 128: val_loss did not improve from 0.23925\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2092 - mean_squared_error: 0.0787 - val_loss: 0.2597 - val_mean_squared_error: 0.0955\n",
            "Epoch 129/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2023 - mean_squared_error: 0.0717\n",
            "Epoch 129: val_loss improved from 0.23925 to 0.22871, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2074 - mean_squared_error: 0.0792 - val_loss: 0.2287 - val_mean_squared_error: 0.0826\n",
            "Epoch 130/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2008 - mean_squared_error: 0.0749\n",
            "Epoch 130: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2008 - mean_squared_error: 0.0749 - val_loss: 0.2920 - val_mean_squared_error: 0.1359\n",
            "Epoch 131/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2096 - mean_squared_error: 0.0813\n",
            "Epoch 131: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2096 - mean_squared_error: 0.0813 - val_loss: 0.2426 - val_mean_squared_error: 0.0902\n",
            "Epoch 132/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2984 - mean_squared_error: 0.1721\n",
            "Epoch 132: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2083 - mean_squared_error: 0.0772 - val_loss: 0.2377 - val_mean_squared_error: 0.0865\n",
            "Epoch 133/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2112 - mean_squared_error: 0.0800\n",
            "Epoch 133: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2112 - mean_squared_error: 0.0800 - val_loss: 0.2334 - val_mean_squared_error: 0.0828\n",
            "Epoch 134/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2286 - mean_squared_error: 0.0970\n",
            "Epoch 134: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2286 - mean_squared_error: 0.0970 - val_loss: 0.2621 - val_mean_squared_error: 0.0922\n",
            "Epoch 135/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2383 - mean_squared_error: 0.1001\n",
            "Epoch 135: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2277 - mean_squared_error: 0.0963 - val_loss: 0.3450 - val_mean_squared_error: 0.1454\n",
            "Epoch 136/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2929 - mean_squared_error: 0.2593\n",
            "Epoch 136: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2290 - mean_squared_error: 0.1077 - val_loss: 0.2956 - val_mean_squared_error: 0.1083\n",
            "Epoch 137/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2263 - mean_squared_error: 0.1007\n",
            "Epoch 137: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2263 - mean_squared_error: 0.1007 - val_loss: 0.2534 - val_mean_squared_error: 0.0965\n",
            "Epoch 138/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2049 - mean_squared_error: 0.0770\n",
            "Epoch 138: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2049 - mean_squared_error: 0.0770 - val_loss: 0.2626 - val_mean_squared_error: 0.0957\n",
            "Epoch 139/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2016 - mean_squared_error: 0.0749\n",
            "Epoch 139: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2016 - mean_squared_error: 0.0749 - val_loss: 0.2360 - val_mean_squared_error: 0.0823\n",
            "Epoch 140/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2592 - mean_squared_error: 0.1100\n",
            "Epoch 140: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2032 - mean_squared_error: 0.0741 - val_loss: 0.2693 - val_mean_squared_error: 0.1283\n",
            "Epoch 141/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2111 - mean_squared_error: 0.0821\n",
            "Epoch 141: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2111 - mean_squared_error: 0.0821 - val_loss: 0.2489 - val_mean_squared_error: 0.0852\n",
            "Epoch 142/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2093 - mean_squared_error: 0.0769\n",
            "Epoch 142: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2093 - mean_squared_error: 0.0769 - val_loss: 0.2560 - val_mean_squared_error: 0.0919\n",
            "Epoch 143/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1479 - mean_squared_error: 0.0405\n",
            "Epoch 143: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2011 - mean_squared_error: 0.0738 - val_loss: 0.2616 - val_mean_squared_error: 0.0936\n",
            "Epoch 144/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.1005\n",
            "Epoch 144: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2047 - mean_squared_error: 0.0784 - val_loss: 0.2730 - val_mean_squared_error: 0.1178\n",
            "Epoch 145/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2178 - mean_squared_error: 0.0856\n",
            "Epoch 145: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2184 - mean_squared_error: 0.0871 - val_loss: 0.2296 - val_mean_squared_error: 0.0831\n",
            "Epoch 146/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2068 - mean_squared_error: 0.0793\n",
            "Epoch 146: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2034 - mean_squared_error: 0.0767 - val_loss: 0.2311 - val_mean_squared_error: 0.0846\n",
            "Epoch 147/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2183 - mean_squared_error: 0.0977\n",
            "Epoch 147: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2234 - mean_squared_error: 0.1008 - val_loss: 0.2909 - val_mean_squared_error: 0.1401\n",
            "Epoch 148/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2126 - mean_squared_error: 0.0837\n",
            "Epoch 148: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2107 - mean_squared_error: 0.0818 - val_loss: 0.2425 - val_mean_squared_error: 0.0844\n",
            "Epoch 149/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2061 - mean_squared_error: 0.0773\n",
            "Epoch 149: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2027 - mean_squared_error: 0.0751 - val_loss: 0.2629 - val_mean_squared_error: 0.0955\n",
            "Epoch 150/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2053 - mean_squared_error: 0.0727\n",
            "Epoch 150: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2065 - mean_squared_error: 0.0753 - val_loss: 0.2292 - val_mean_squared_error: 0.0825\n",
            "Epoch 151/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1947 - mean_squared_error: 0.0709\n",
            "Epoch 151: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2015 - mean_squared_error: 0.0739 - val_loss: 0.2443 - val_mean_squared_error: 0.0865\n",
            "Epoch 152/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2009 - mean_squared_error: 0.0732\n",
            "Epoch 152: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1997 - mean_squared_error: 0.0719 - val_loss: 0.2403 - val_mean_squared_error: 0.0867\n",
            "Epoch 153/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2106 - mean_squared_error: 0.0822\n",
            "Epoch 153: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2123 - mean_squared_error: 0.0847 - val_loss: 0.2336 - val_mean_squared_error: 0.0799\n",
            "Epoch 154/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2048 - mean_squared_error: 0.0802\n",
            "Epoch 154: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2017 - mean_squared_error: 0.0736 - val_loss: 0.2412 - val_mean_squared_error: 0.0839\n",
            "Epoch 155/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2245 - mean_squared_error: 0.0959\n",
            "Epoch 155: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2265 - mean_squared_error: 0.0965 - val_loss: 0.2700 - val_mean_squared_error: 0.0961\n",
            "Epoch 156/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1995 - mean_squared_error: 0.0766\n",
            "Epoch 156: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2116 - mean_squared_error: 0.0842 - val_loss: 0.2907 - val_mean_squared_error: 0.1317\n",
            "Epoch 157/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2020 - mean_squared_error: 0.0738\n",
            "Epoch 157: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2067 - mean_squared_error: 0.0788 - val_loss: 0.2358 - val_mean_squared_error: 0.0850\n",
            "Epoch 158/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0724\n",
            "Epoch 158: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.1971 - mean_squared_error: 0.0700 - val_loss: 0.2337 - val_mean_squared_error: 0.0856\n",
            "Epoch 159/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2061 - mean_squared_error: 0.0740\n",
            "Epoch 159: val_loss did not improve from 0.22871\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2061 - mean_squared_error: 0.0740 - val_loss: 0.2780 - val_mean_squared_error: 0.1185\n",
            "Epoch 160/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2264 - mean_squared_error: 0.0869\n",
            "Epoch 160: val_loss improved from 0.22871 to 0.22739, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2235 - mean_squared_error: 0.0850 - val_loss: 0.2274 - val_mean_squared_error: 0.0845\n",
            "Epoch 161/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1942 - mean_squared_error: 0.0673\n",
            "Epoch 161: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1972 - mean_squared_error: 0.0714 - val_loss: 0.2300 - val_mean_squared_error: 0.0823\n",
            "Epoch 162/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2007 - mean_squared_error: 0.0751\n",
            "Epoch 162: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2007 - mean_squared_error: 0.0751 - val_loss: 0.2281 - val_mean_squared_error: 0.0789\n",
            "Epoch 163/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1932 - mean_squared_error: 0.0758\n",
            "Epoch 163: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1937 - mean_squared_error: 0.0756 - val_loss: 0.2632 - val_mean_squared_error: 0.1133\n",
            "Epoch 164/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2063 - mean_squared_error: 0.0754\n",
            "Epoch 164: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2065 - mean_squared_error: 0.0757 - val_loss: 0.2703 - val_mean_squared_error: 0.1090\n",
            "Epoch 165/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2325 - mean_squared_error: 0.0926\n",
            "Epoch 165: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2263 - mean_squared_error: 0.0928 - val_loss: 0.2281 - val_mean_squared_error: 0.0777\n",
            "Epoch 166/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1988 - mean_squared_error: 0.0743\n",
            "Epoch 166: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2030 - mean_squared_error: 0.0762 - val_loss: 0.2909 - val_mean_squared_error: 0.1048\n",
            "Epoch 167/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2250 - mean_squared_error: 0.0941\n",
            "Epoch 167: val_loss did not improve from 0.22739\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2261 - mean_squared_error: 0.0939 - val_loss: 0.2349 - val_mean_squared_error: 0.0868\n",
            "Epoch 168/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2056 - mean_squared_error: 0.0764\n",
            "Epoch 168: val_loss improved from 0.22739 to 0.21231, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2090 - mean_squared_error: 0.0807 - val_loss: 0.2123 - val_mean_squared_error: 0.0739\n",
            "Epoch 169/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1994 - mean_squared_error: 0.0729\n",
            "Epoch 169: val_loss did not improve from 0.21231\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1994 - mean_squared_error: 0.0729 - val_loss: 0.2752 - val_mean_squared_error: 0.0992\n",
            "Epoch 170/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2065 - mean_squared_error: 0.0779\n",
            "Epoch 170: val_loss did not improve from 0.21231\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2064 - mean_squared_error: 0.0772 - val_loss: 0.2323 - val_mean_squared_error: 0.0772\n",
            "Epoch 171/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2287 - mean_squared_error: 0.1062\n",
            "Epoch 171: val_loss did not improve from 0.21231\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2303 - mean_squared_error: 0.1058 - val_loss: 0.3030 - val_mean_squared_error: 0.2028\n",
            "Epoch 172/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2253 - mean_squared_error: 0.0896\n",
            "Epoch 172: val_loss improved from 0.21231 to 0.20962, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.2238 - mean_squared_error: 0.0891 - val_loss: 0.2096 - val_mean_squared_error: 0.0713\n",
            "Epoch 173/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2064 - mean_squared_error: 0.0781\n",
            "Epoch 173: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2001 - mean_squared_error: 0.0736 - val_loss: 0.2127 - val_mean_squared_error: 0.0718\n",
            "Epoch 174/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1877 - mean_squared_error: 0.0613\n",
            "Epoch 174: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1947 - mean_squared_error: 0.0689 - val_loss: 0.2923 - val_mean_squared_error: 0.1618\n",
            "Epoch 175/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2280 - mean_squared_error: 0.0956\n",
            "Epoch 175: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2152 - mean_squared_error: 0.0877 - val_loss: 0.2333 - val_mean_squared_error: 0.0804\n",
            "Epoch 176/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2250 - mean_squared_error: 0.0963\n",
            "Epoch 176: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2163 - mean_squared_error: 0.0878 - val_loss: 0.2575 - val_mean_squared_error: 0.0878\n",
            "Epoch 177/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2115 - mean_squared_error: 0.0793\n",
            "Epoch 177: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2121 - mean_squared_error: 0.0804 - val_loss: 0.2762 - val_mean_squared_error: 0.1469\n",
            "Epoch 178/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2194 - mean_squared_error: 0.0876\n",
            "Epoch 178: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2178 - mean_squared_error: 0.0858 - val_loss: 0.2184 - val_mean_squared_error: 0.0806\n",
            "Epoch 179/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2159 - mean_squared_error: 0.0839\n",
            "Epoch 179: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.2146 - mean_squared_error: 0.0827 - val_loss: 0.2134 - val_mean_squared_error: 0.0759\n",
            "Epoch 180/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2257 - mean_squared_error: 0.0934\n",
            "Epoch 180: val_loss did not improve from 0.20962\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.2133 - mean_squared_error: 0.0838 - val_loss: 0.2950 - val_mean_squared_error: 0.1082\n",
            "Epoch 181/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2540 - mean_squared_error: 0.1216\n",
            "Epoch 181: val_loss improved from 0.20962 to 0.20957, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 0.2365 - mean_squared_error: 0.1090 - val_loss: 0.2096 - val_mean_squared_error: 0.0692\n",
            "Epoch 182/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1989 - mean_squared_error: 0.0666\n",
            "Epoch 182: val_loss did not improve from 0.20957\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2016 - mean_squared_error: 0.0717 - val_loss: 0.2155 - val_mean_squared_error: 0.0712\n",
            "Epoch 183/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2027 - mean_squared_error: 0.0734\n",
            "Epoch 183: val_loss did not improve from 0.20957\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1936 - mean_squared_error: 0.0677 - val_loss: 0.2572 - val_mean_squared_error: 0.0898\n",
            "Epoch 184/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2261 - mean_squared_error: 0.1065\n",
            "Epoch 184: val_loss did not improve from 0.20957\n",
            "9/9 [==============================] - 0s 21ms/step - loss: 0.2218 - mean_squared_error: 0.0998 - val_loss: 0.2578 - val_mean_squared_error: 0.1135\n",
            "Epoch 185/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2028 - mean_squared_error: 0.0752\n",
            "Epoch 185: val_loss improved from 0.20957 to 0.20644, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2028 - mean_squared_error: 0.0752 - val_loss: 0.2064 - val_mean_squared_error: 0.0697\n",
            "Epoch 186/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1932 - mean_squared_error: 0.0689\n",
            "Epoch 186: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.1978 - mean_squared_error: 0.0725 - val_loss: 0.2487 - val_mean_squared_error: 0.1135\n",
            "Epoch 187/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2167 - mean_squared_error: 0.0913\n",
            "Epoch 187: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2213 - mean_squared_error: 0.0925 - val_loss: 0.2316 - val_mean_squared_error: 0.0851\n",
            "Epoch 188/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2181 - mean_squared_error: 0.0926\n",
            "Epoch 188: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.2131 - mean_squared_error: 0.0867 - val_loss: 0.2877 - val_mean_squared_error: 0.1071\n",
            "Epoch 189/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2084 - mean_squared_error: 0.0898\n",
            "Epoch 189: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.2050 - mean_squared_error: 0.0839 - val_loss: 0.2451 - val_mean_squared_error: 0.1347\n",
            "Epoch 190/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1972 - mean_squared_error: 0.0703\n",
            "Epoch 190: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2130 - mean_squared_error: 0.0812 - val_loss: 0.2289 - val_mean_squared_error: 0.0780\n",
            "Epoch 191/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2017 - mean_squared_error: 0.0772\n",
            "Epoch 191: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2017 - mean_squared_error: 0.0772 - val_loss: 0.2743 - val_mean_squared_error: 0.1025\n",
            "Epoch 192/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2122 - mean_squared_error: 0.0945\n",
            "Epoch 192: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2100 - mean_squared_error: 0.0908 - val_loss: 0.2275 - val_mean_squared_error: 0.0945\n",
            "Epoch 193/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1812 - mean_squared_error: 0.0560\n",
            "Epoch 193: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1998 - mean_squared_error: 0.0703 - val_loss: 0.2115 - val_mean_squared_error: 0.0707\n",
            "Epoch 194/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1518 - mean_squared_error: 0.0346\n",
            "Epoch 194: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1931 - mean_squared_error: 0.0654 - val_loss: 0.2083 - val_mean_squared_error: 0.0709\n",
            "Epoch 195/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2141 - mean_squared_error: 0.0851\n",
            "Epoch 195: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2131 - mean_squared_error: 0.0841 - val_loss: 0.2786 - val_mean_squared_error: 0.1066\n",
            "Epoch 196/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2269 - mean_squared_error: 0.0899\n",
            "Epoch 196: val_loss did not improve from 0.20644\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2269 - mean_squared_error: 0.0899 - val_loss: 0.2295 - val_mean_squared_error: 0.1159\n",
            "Epoch 197/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.2386 - mean_squared_error: 0.1117\n",
            "Epoch 197: val_loss improved from 0.20644 to 0.18265, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2265 - mean_squared_error: 0.1012 - val_loss: 0.1826 - val_mean_squared_error: 0.0743\n",
            "Epoch 198/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2262 - mean_squared_error: 0.0920\n",
            "Epoch 198: val_loss did not improve from 0.18265\n",
            "9/9 [==============================] - 0s 34ms/step - loss: 0.2262 - mean_squared_error: 0.0920 - val_loss: 0.2930 - val_mean_squared_error: 0.1145\n",
            "Epoch 199/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2208 - mean_squared_error: 0.0984\n",
            "Epoch 199: val_loss did not improve from 0.18265\n",
            "9/9 [==============================] - 0s 39ms/step - loss: 0.2208 - mean_squared_error: 0.0984 - val_loss: 0.2058 - val_mean_squared_error: 0.0803\n",
            "Epoch 200/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1945 - mean_squared_error: 0.0669\n",
            "Epoch 200: val_loss improved from 0.18265 to 0.17709, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 0.2053 - mean_squared_error: 0.0764 - val_loss: 0.1771 - val_mean_squared_error: 0.0610\n",
            "Epoch 201/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2010 - mean_squared_error: 0.0715\n",
            "Epoch 201: val_loss improved from 0.17709 to 0.17375, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 33ms/step - loss: 0.1928 - mean_squared_error: 0.0667 - val_loss: 0.1737 - val_mean_squared_error: 0.0604\n",
            "Epoch 202/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1870 - mean_squared_error: 0.0631\n",
            "Epoch 202: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.1946 - mean_squared_error: 0.0673 - val_loss: 0.1873 - val_mean_squared_error: 0.0664\n",
            "Epoch 203/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1735 - mean_squared_error: 0.0543\n",
            "Epoch 203: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.1896 - mean_squared_error: 0.0651 - val_loss: 0.1912 - val_mean_squared_error: 0.0692\n",
            "Epoch 204/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1943 - mean_squared_error: 0.0669\n",
            "Epoch 204: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 0.1992 - mean_squared_error: 0.0696 - val_loss: 0.2248 - val_mean_squared_error: 0.0944\n",
            "Epoch 205/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2202 - mean_squared_error: 0.0883\n",
            "Epoch 205: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.2148 - mean_squared_error: 0.0842 - val_loss: 0.2208 - val_mean_squared_error: 0.0769\n",
            "Epoch 206/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1968 - mean_squared_error: 0.0710\n",
            "Epoch 206: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1968 - mean_squared_error: 0.0710 - val_loss: 0.1761 - val_mean_squared_error: 0.0625\n",
            "Epoch 207/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2028 - mean_squared_error: 0.0723\n",
            "Epoch 207: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2085 - mean_squared_error: 0.0766 - val_loss: 0.2424 - val_mean_squared_error: 0.1147\n",
            "Epoch 208/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1895 - mean_squared_error: 0.0642\n",
            "Epoch 208: val_loss did not improve from 0.17375\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2091 - mean_squared_error: 0.0774 - val_loss: 0.2110 - val_mean_squared_error: 0.0764\n",
            "Epoch 209/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2073 - mean_squared_error: 0.0714\n",
            "Epoch 209: val_loss improved from 0.17375 to 0.16758, saving model to best_cnn_gap_tf.h5\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.2073 - mean_squared_error: 0.0714 - val_loss: 0.1676 - val_mean_squared_error: 0.0609\n",
            "Epoch 210/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1733 - mean_squared_error: 0.0524\n",
            "Epoch 210: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.1903 - mean_squared_error: 0.0647 - val_loss: 0.1931 - val_mean_squared_error: 0.0643\n",
            "Epoch 211/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1548 - mean_squared_error: 0.0408\n",
            "Epoch 211: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1957 - mean_squared_error: 0.0686 - val_loss: 0.1964 - val_mean_squared_error: 0.0748\n",
            "Epoch 212/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1961 - mean_squared_error: 0.0694\n",
            "Epoch 212: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.1961 - mean_squared_error: 0.0694 - val_loss: 0.1951 - val_mean_squared_error: 0.0659\n",
            "Epoch 213/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2037 - mean_squared_error: 0.0786\n",
            "Epoch 213: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.2010 - mean_squared_error: 0.0766 - val_loss: 0.2997 - val_mean_squared_error: 0.1279\n",
            "Epoch 214/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2421 - mean_squared_error: 0.1187\n",
            "Epoch 214: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.2421 - mean_squared_error: 0.1187 - val_loss: 0.2765 - val_mean_squared_error: 0.1865\n",
            "Epoch 215/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2359 - mean_squared_error: 0.1016\n",
            "Epoch 215: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.2346 - mean_squared_error: 0.1014 - val_loss: 0.2506 - val_mean_squared_error: 0.0913\n",
            "Epoch 216/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.2175 - mean_squared_error: 0.0882\n",
            "Epoch 216: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2205 - mean_squared_error: 0.0893 - val_loss: 0.2157 - val_mean_squared_error: 0.0725\n",
            "Epoch 217/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1867 - mean_squared_error: 0.0607\n",
            "Epoch 217: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.1920 - mean_squared_error: 0.0653 - val_loss: 0.2116 - val_mean_squared_error: 0.0758\n",
            "Epoch 218/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1855 - mean_squared_error: 0.0599\n",
            "Epoch 218: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.1874 - mean_squared_error: 0.0610 - val_loss: 0.2047 - val_mean_squared_error: 0.0740\n",
            "Epoch 219/500\n",
            "4/9 [============>.................] - ETA: 0s - loss: 0.1897 - mean_squared_error: 0.0664\n",
            "Epoch 219: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1946 - mean_squared_error: 0.0659 - val_loss: 0.1991 - val_mean_squared_error: 0.0698\n",
            "Epoch 220/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1942 - mean_squared_error: 0.0642\n",
            "Epoch 220: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 23ms/step - loss: 0.1942 - mean_squared_error: 0.0642 - val_loss: 0.2019 - val_mean_squared_error: 0.0723\n",
            "Epoch 221/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.1823 - mean_squared_error: 0.0584\n",
            "Epoch 221: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1867 - mean_squared_error: 0.0611 - val_loss: 0.1982 - val_mean_squared_error: 0.0675\n",
            "Epoch 222/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1930 - mean_squared_error: 0.0665\n",
            "Epoch 222: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.1939 - mean_squared_error: 0.0659 - val_loss: 0.2182 - val_mean_squared_error: 0.0787\n",
            "Epoch 223/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1941 - mean_squared_error: 0.0661\n",
            "Epoch 223: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.1941 - mean_squared_error: 0.0661 - val_loss: 0.2456 - val_mean_squared_error: 0.1315\n",
            "Epoch 224/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.2038 - mean_squared_error: 0.0722\n",
            "Epoch 224: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.2080 - mean_squared_error: 0.0760 - val_loss: 0.2650 - val_mean_squared_error: 0.1048\n",
            "Epoch 225/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2337 - mean_squared_error: 0.0952\n",
            "Epoch 225: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.2337 - mean_squared_error: 0.0952 - val_loss: 0.2085 - val_mean_squared_error: 0.0734\n",
            "Epoch 226/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1897 - mean_squared_error: 0.0646\n",
            "Epoch 226: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.1897 - mean_squared_error: 0.0646 - val_loss: 0.2518 - val_mean_squared_error: 0.1278\n",
            "Epoch 227/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2164 - mean_squared_error: 0.0779\n",
            "Epoch 227: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2164 - mean_squared_error: 0.0779 - val_loss: 0.2438 - val_mean_squared_error: 0.0883\n",
            "Epoch 228/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2096 - mean_squared_error: 0.0748\n",
            "Epoch 228: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.2096 - mean_squared_error: 0.0748 - val_loss: 0.2190 - val_mean_squared_error: 0.0884\n",
            "Epoch 229/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1902 - mean_squared_error: 0.0612\n",
            "Epoch 229: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.2010 - mean_squared_error: 0.0726 - val_loss: 0.1918 - val_mean_squared_error: 0.0671\n",
            "Epoch 230/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1904 - mean_squared_error: 0.0651\n",
            "Epoch 230: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1880 - mean_squared_error: 0.0636 - val_loss: 0.1725 - val_mean_squared_error: 0.0643\n",
            "Epoch 231/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1815 - mean_squared_error: 0.0587\n",
            "Epoch 231: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.1815 - mean_squared_error: 0.0587 - val_loss: 0.1966 - val_mean_squared_error: 0.0678\n",
            "Epoch 232/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1906 - mean_squared_error: 0.0660\n",
            "Epoch 232: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.1836 - mean_squared_error: 0.0606 - val_loss: 0.1813 - val_mean_squared_error: 0.0648\n",
            "Epoch 233/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.1904 - mean_squared_error: 0.0617\n",
            "Epoch 233: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.1861 - mean_squared_error: 0.0603 - val_loss: 0.2089 - val_mean_squared_error: 0.0778\n",
            "Epoch 234/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1978 - mean_squared_error: 0.0647\n",
            "Epoch 234: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.1978 - mean_squared_error: 0.0647 - val_loss: 0.2213 - val_mean_squared_error: 0.0831\n",
            "Epoch 235/500\n",
            "1/9 [==>...........................] - ETA: 0s - loss: 0.2132 - mean_squared_error: 0.0843\n",
            "Epoch 235: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.2024 - mean_squared_error: 0.0705 - val_loss: 0.1841 - val_mean_squared_error: 0.0655\n",
            "Epoch 236/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1854 - mean_squared_error: 0.0610\n",
            "Epoch 236: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.1862 - mean_squared_error: 0.0613 - val_loss: 0.1919 - val_mean_squared_error: 0.0682\n",
            "Epoch 237/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2036 - mean_squared_error: 0.0740\n",
            "Epoch 237: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.2036 - mean_squared_error: 0.0740 - val_loss: 0.2333 - val_mean_squared_error: 0.1056\n",
            "Epoch 238/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1989 - mean_squared_error: 0.0708\n",
            "Epoch 238: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.1989 - mean_squared_error: 0.0708 - val_loss: 0.2187 - val_mean_squared_error: 0.0778\n",
            "Epoch 239/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1892 - mean_squared_error: 0.0633\n",
            "Epoch 239: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.1979 - mean_squared_error: 0.0712 - val_loss: 0.2070 - val_mean_squared_error: 0.0781\n",
            "Epoch 240/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.1861 - mean_squared_error: 0.0611\n",
            "Epoch 240: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 37ms/step - loss: 0.1861 - mean_squared_error: 0.0611 - val_loss: 0.2002 - val_mean_squared_error: 0.0684\n",
            "Epoch 241/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1896 - mean_squared_error: 0.0613\n",
            "Epoch 241: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 36ms/step - loss: 0.1917 - mean_squared_error: 0.0626 - val_loss: 0.1846 - val_mean_squared_error: 0.0667\n",
            "Epoch 242/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1848 - mean_squared_error: 0.0588\n",
            "Epoch 242: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 29ms/step - loss: 0.1852 - mean_squared_error: 0.0600 - val_loss: 0.2165 - val_mean_squared_error: 0.0845\n",
            "Epoch 243/500\n",
            "8/9 [=========================>....] - ETA: 0s - loss: 0.1842 - mean_squared_error: 0.0592\n",
            "Epoch 243: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 22ms/step - loss: 0.1927 - mean_squared_error: 0.0671 - val_loss: 0.1868 - val_mean_squared_error: 0.0692\n",
            "Epoch 244/500\n",
            "5/9 [===============>..............] - ETA: 0s - loss: 0.1659 - mean_squared_error: 0.0478\n",
            "Epoch 244: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 30ms/step - loss: 0.1893 - mean_squared_error: 0.0625 - val_loss: 0.1748 - val_mean_squared_error: 0.0629\n",
            "Epoch 245/500\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.2174 - mean_squared_error: 0.0905\n",
            "Epoch 245: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 31ms/step - loss: 0.2174 - mean_squared_error: 0.0905 - val_loss: 0.2177 - val_mean_squared_error: 0.0805\n",
            "Epoch 246/500\n",
            "6/9 [===================>..........] - ETA: 0s - loss: 0.2011 - mean_squared_error: 0.0738\n",
            "Epoch 246: val_loss did not improve from 0.16758\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.2013 - mean_squared_error: 0.0699 - val_loss: 0.1939 - val_mean_squared_error: 0.0659\n",
            "Epoch 247/500\n",
            "7/9 [======================>.......] - ETA: 0s - loss: 0.1865 - mean_squared_error: 0.0586"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9c3d9e59648f>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'target_variable'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m predicted_values, true_values, duration = run_cnn_leave_one_out(Dataset=DatasetAuNC,\n\u001b[0m\u001b[1;32m     19\u001b[0m                                                         \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetAuNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                         \u001b[0mtarget_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-973e0da16e04>\u001b[0m in \u001b[0;36mrun_cnn_leave_one_out\u001b[0;34m(Dataset, sample_size, target_variable, batch_size, learning_rate, patience, model_save_name, pre_trained_model, use_simplex, use_charge, random_state)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start the timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start fitting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         model.fit([x1_train, x2_train, x3_train],\n\u001b[0m\u001b[1;32m    128\u001b[0m                   \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PGcP4hneFh24",
        "2J82C1fy_PFD",
        "Cth4q9IM6NDl",
        "KHBecwB0_2_I",
        "khEOzNYf40co"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}