# -*- coding: utf-8 -*-
"""PI_KRR_RF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GNBXhuZaWz-IFQONq_t2AIO9ER5MCN-

# Generating Persistence Images and Simplical complex features to predict AuNC properties

1. This notebook focuses on generating persistence images and simplicial complex count and using them to build a Kernel Ridge Regression and Random Forest Regression model to predict the HOMO-LUMO gap, internal energies, and dipole moment of AuNCs.
2. Persistence Images were generated using the code in Townsend *et. al.* 2020 publication, publicly available at: https://gitlab.com/voglab/PersistentImages_Chemistry
3. Simplex analysis were done using GUDHI package
4. Please make sure that this notebook is in the same directory as the data folder and the scripts:
> 1. ElementsInfo.py
> 2. PersistentImageCode.py
> 3. VariancePersistCode.py
> 4. GenerateImagePI.py

# Installations
"""

!pip install gudhi
!pip install ripser
!pip install elements
!pip install qml
!pip install dscribe

import os
import pandas as pd
import numpy as np
from tqdm import tqdm

import qml
from qml.representations import *
from natsort import natsorted
from dscribe.descriptors import SOAP
from ase.io import read

from ElementsInfo import *
from PersistentImageCode import *
from VariancePersistCode import *
from GenerateImagePI import *

from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import mean_squared_error
import time

"""# Process data and store them to pickle files

Note: CM and SOAP features are too large to be stored within pickle files. The current pickle file uploaded in the "data" directory does not contain CM and SOAP feature vectors, and they must be calculated in the corresponding code blocks below.

"""

# import dataframe
DatasetAuNC = pd.read_excel("./data/AuNC/DatasetAuNC.xlsx")

# Appending xyz

os.chdir('./data/AuNC/AuNC_xyz_files')

xyz_list = []
files = []
for i in tqdm(range(len(DatasetAuNC))):
    try:
        xyz_filename = DatasetAuNC["Filename"][i] + ".xyz"
        with open(xyz_filename) as f:
            lines = f.readlines()
            files.append(xyz_filename)
            xyz_list.append(lines)
    except FileNotFoundError as e:
        print(f"Error: File '{xyz_filename}' not found.")
    except Exception as e:
        print(f"Error at index {i}: {str(e)}")
        continue

DatasetAuNC["xyz"] = xyz_list

# Calculating Betti feature count for overview

Error = []
B0_list = []
B1_list = []
B2_list = []

for i in range(len(DatasetAuNC)):
  try:
    D, elements = Makexyzdistance(DatasetAuNC["xyz"][i])
    persistent_homology_features = ripser(D,distance_matrix=True, maxdim = 2)
    B0_list.append(len(persistent_homology_features['dgms'][0])-1)
    B1_list.append(len(persistent_homology_features['dgms'][1]))
    B2_list.append(len(persistent_homology_features['dgms'][2]))

  except:
    Error.append(DatasetAuNC["Number"][i])
    continue

DatasetAuNC["Betti0count"] = B0_list
DatasetAuNC["Betti1count"] = B1_list
DatasetAuNC["Betti2count"] = B2_list

print("Finished appending Betti feature count!")

"""
Calculating and appending VariancePersist arrays to pandas dataframe
VariancePersistv1 is a modified PI function with added buffer values
(0.5, 0.05) for Betti 1 and 2 features.
Except block is added due to some xyz file having values written in different notations (e.g. 1e-4)
and cannot be read as a float. (current AuNC database does not have this issue)
"""


def append_persistence_image(Dataframe):
  Error = []
  PersImgArr = []

  for idx in range(len(Dataframe)):
    try:
        persistent_image_matrix = VariancePersistv1(
                              Dataframe["xyz"][idx],
                              pixelx=resolution,
                              pixely=resolution,
                              myspread=myspread ,
                              myspecs={"maxBD": max_bound, "minBD":min_bound},
                              electroneg_addition=electroneg_addition,
                              electroneg_division=electroneg_division,
                              B1_buffer=B1_buffer,
                              B2_buffer=B2_buffer,
                              showplot = False
                              )

        PersImgArr.append(persistent_image_matrix)
    except:
        Error.append(idx)
        print("Error at index: ", idx)
        PersImgArr.append(np.zeros(resolution*resolution,))

  Dataframe["PersImg"] = PersImgArr
  print("Finished appending Persistence Images!")

#Persistence Image Parameters
resolution=100
myspread=0.3
min_bound=-0.3
max_bound=7
electroneg_addition=+0.4
electroneg_division=10
B1_buffer=0.5
B2_buffer=0.05

append_persistence_image(DatasetAuNC)

# Simplcial complex analysis
def Simplex_analyze(Core_coordinates, bond_length_limit):
  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)
  simplex_tree = rips.create_simplex_tree(max_dimension=3)
  simplex_generator = simplex_tree.get_skeleton(3)

  Tetrahedra_list = []
  Vertices_of_tetrahedra = []
  Overlapping_triangles = []
  Unconnected_triangles = []
  Triangles_with_1_shared_vertex = []
  Triangles_with_2_shared_vertices = []

  for simplex in simplex_generator:
    if simplex[1] >= bond_length_limit: continue
    if len(simplex[0]) == 4: #simplex[0]: list of vertices, simplex[1]: birth filtration
      Tetrahedra_list.append(simplex)
      for vertex in simplex[0]:
        if vertex not in Vertices_of_tetrahedra:
          Vertices_of_tetrahedra.append(vertex)

  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)
  simplex_tree = rips.create_simplex_tree(max_dimension=3)
  simplex_generator = simplex_tree.get_skeleton(3)

  for simplex in simplex_generator:
    Shared_count_arr = []

    if simplex[1] >= bond_length_limit: continue
    if len(simplex[0]) != 3: continue

    if len(Tetrahedra_list) == 0: Unconnected_triangles.append(simplex[0])
    else:
      for tetrahedra in Tetrahedra_list:
        Shared_count_arr.append(len(set(simplex[0]) & set(tetrahedra[0])))

      if max(Shared_count_arr) == 3: Overlapping_triangles.append(simplex)
      elif max(Shared_count_arr) == 2: Triangles_with_2_shared_vertices.append(simplex)
      elif max(Shared_count_arr) == 1: Triangles_with_1_shared_vertex.append(simplex)
      else: Unconnected_triangles.append(simplex)

  return {"Tetrahedral_count": len(Tetrahedra_list), "Unconnected_triangles_count": len(Unconnected_triangles),
          "Triangles_with_1_shared_vertex_count":len(Triangles_with_1_shared_vertex),
          "Triangles_with_2_shared_vertices_count": len(Triangles_with_2_shared_vertices)}

"""
Simplex analysis
"""
os.chdir("./data/AuNC/AuNC_xyz_files")

Tetrahedral_count_arr = []
Unconnected_triangles_count_arr = []
Triangles_with_1_shared_vertex_count_arr = []
Triangles_with_2_shared_vertices_count_arr = []
files = []
for idx in range(len(DatasetAuNC)):
  try:
    filename = DatasetAuNC["Filename"][idx] + ".xyz"
    df=pd.read_table(filename, delim_whitespace=True, names=['a','b','c','d'],skiprows = 2) # skip the first 2 lines of xyz files
    mat = df[['b','c','d']].to_numpy()
    ElementArr = df['a'].to_numpy()

    Core_coordinates = []

    for index in range(len(ElementArr)):
      if ElementArr[index] != "Au": continue
      Core_coordinates.append(mat[index])

    Dict = Simplex_analyze(Core_coordinates, bond_length_limit = 4.0)

    Tetrahedral_count_arr.append(Dict["Tetrahedral_count"])
    Unconnected_triangles_count_arr.append(Dict["Unconnected_triangles_count"])
    Triangles_with_1_shared_vertex_count_arr.append(Dict["Triangles_with_1_shared_vertex_count"])
    Triangles_with_2_shared_vertices_count_arr.append(Dict["Triangles_with_2_shared_vertices_count"])

    print("Done: ", idx)
    print(len(Tetrahedral_count_arr))


  except:
    print("error at: ", idx)
    continue

DatasetAuNC["Tetrahedral_count"] = Tetrahedral_count_arr
DatasetAuNC["Unconnected_triangles_count"] = Unconnected_triangles_count_arr
DatasetAuNC["Triangles_with_1_shared_vertex_count"] = Triangles_with_1_shared_vertex_count_arr
DatasetAuNC["Triangles_with_2_shared_vertices_count"] = Triangles_with_2_shared_vertices_count_arr

print("Finished appending Simplex count!")

"""
Calculating and appending Coulomb Matrix arrays to pandas dataframe
"""
def append_cm(dataframe, xyz_root):
  CoulMatArr = []
  for f in natsorted(os.listdir(xyz_root)):
    try:
      mol = qml.Compound(xyz=f)
      mol.generate_coulomb_matrix(size=2000, sorting="row-norm")
      A = mol.representation
      CoulMatArr.append([f,A])
    except:
      print("Error at: ", f)

  # Sample array containing the format ["Filename.xyz", property array]
  property_arrays = np.empty(len(dataframe), dtype=object)

  for filename, property_array in CoulMatArr:
      filename_without_extension = filename.split('.xyz')[0]
      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]

      if len(row_index) == 1:
          property_arrays[row_index[0]] = property_array
      else:
          print(f"Filename '{filename_without_extension}' not found in the DataFrame.")

  dataframe['Coulomb_Matrix'] = property_arrays
  print("Finished appending Coulomb Matrices!")

append_cm(DatasetAuNC, "./data/AuNC/AuNC_xyz_files")

"""
Calculating and appending SOAP arrays to pandas dataframe
"""
def append_soap(dataframe, xyz_root):
  species = ["Au", "P", "S", "Sb", "Se", "Cl", "Br", "F", "C", "N", "O", "H", "I", "Fe"]
  r_cut = 6.0
  n_max = 8
  l_max = 6

  soap = SOAP(
      species=species,
      periodic=False,
      r_cut=r_cut,
      n_max=n_max,
      l_max=l_max,
  )

  SOAPArr = []
  for f in natsorted(os.listdir(xyz_root)):
    try:
      mol = read(f)
      a = soap.create(system=mol,centers=[0])
      A = a.flatten()
      SOAPArr.append([f,A])
    except:
      print("error at: ", f)

  property_arrays = np.empty(len(dataframe), dtype=object)

  for filename, property_array in SOAPArr:
      filename_without_extension = filename.split('.xyz')[0]
      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]

      if len(row_index) == 1:
          property_arrays[row_index[0]] = property_array
      else:
          print(f"Filename '{filename_without_extension}' not found in the DataFrame.")

  dataframe['SOAP'] = property_arrays
  print("Finished appending SOAP!")

append_soap(DatasetAuNC, "./data/AuNC/AuNC_xyz_files")

"""
DatabaseTM16Jan includes the XTB calculated properties for the AuTM molecules, along with their simplex counts already calculated at 4.0 filter via the function above
"""

DatasetTM = pd.read_csv("./data/AuTM/DatasetAuTM.csv",
                  sep=',',
                  header = 0)

"""
Appending xyz texts of AuTMs to dataframe
"""
os.chdir("./data/AuTM/AuTM_xyz_files")
xyz_list = []
files = []
for i in range(len(DatasetTM)):
  print("Datapoint: ",i)
  filename = "AuTM-" + str(i+1) + ".xyz"
  with open(filename) as f:
      lines = f.readlines()
      files.append(filename)
      xyz_list.append(lines)

DatasetTM["xyz"]=xyz_list

#Persistence Image Parameters
resolution=100
myspread=0.3
min_bound=-0.3
max_bound=7
electroneg_addition=+0.4
electroneg_division=10
B1_buffer=0.5
B2_buffer=0.05

append_persistence_image(DatasetAuTM)

"""# Read from pre-calculated dataset"""

DatasetAuNC = pd.read_pickle('./data/AuNC/DatasetAuNC.pkl')
DatasetTM = pd.read_pickle('./data/AuTM/DatasetAuTM.pkl')

"""## Result plotting functions"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def analyze_regression_performance(true_values, predicted_values, parameters, duration, text_position='top_left', save_image=False, image_name="regression_performance.png"):
    # Calculate evaluation metrics
    mae = mean_absolute_error(true_values, predicted_values)
    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
    r2 = r2_score(true_values, predicted_values)

    # Create a blank figure for text-only display
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.axis('off')  # Turn off axis for text display only

    # Determine text position
    if text_position == 'top_left':
        text_x, text_y = 0.02, 0.98
        ha, va = 'left', 'top'
    elif text_position == 'top_right':
        text_x, text_y = 0.98, 0.98
        ha, va = 'right', 'top'
    else:
        raise ValueError("Invalid text_position. Choose 'top_left' or 'top_right'.")

    # Add text with model parameters and evaluation metrics
    parameters_text = '\n'.join(f'{key}: {value}' for key, value in parameters.items())
    metrics_text = f"Mean Absolute Error (MAE): {mae:.4f}\n" \
                   f"Root Mean Squared Error (RMSE): {rmse:.4f}\n" \
                   f"R-squared (R²): {r2:.4f}\n" \
                   f"Training duration (s): {duration:.1f}\n"
    text = f"{parameters_text}\n\n{metrics_text}"
    ax.text(text_x, text_y, text, transform=ax.transAxes,
            bbox=dict(facecolor='white', edgecolor='black', alpha=0.8),
            horizontalalignment=ha, verticalalignment=va)

    if save_image:
        plt.savefig(image_name, bbox_inches='tight')

    plt.show()

def plot_ML_results(true_values, predicted_values, x_tick_sep=1.0, y_tick_sep=1.0, dpi=900, save_image=False, image_name="plot_ML_results.png"):
    """
    Plot the true values against predicted values for machine learning model evaluation.

    Parameters:
        true_values (array-like): True target values.
        predicted_values (array-like): Predicted target values.
        x_tick_sep (float): Separation between x-axis tick marks. Default is 1.0.
        y_tick_sep (float): Separation between y-axis tick marks. Default is 1.0.
        dpi (int): Dots per inch for the image resolution. Default is 900.
        save_image (bool): If True, save the image. Default is False.
        image_name (str): Name of the saved image file. Default is "plot_ML_results.png".
    """
    plt.figure(figsize=(10, 10))
    plt.scatter(true_values, predicted_values, c='teal', s=40)

    max_value = max(max(predicted_values), max(true_values))
    min_value = min(min(predicted_values), min(true_values))

    # Ensure the axes always start at 0.0
    max_value = max(max_value, 0.0)
    min_value = min(min_value, 0.0)

    plt.plot([min_value, max_value], [min_value, max_value], 'black')

    plt.xlabel('Calculated HOMO-LUMO gap (eV)', fontsize=25)
    plt.ylabel('Predicted HOMO-LUMO gap (eV)', fontsize=25)

    plt.tick_params(axis='both', labelsize=20, pad=8)
    x_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, x_tick_sep)
    y_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, y_tick_sep)

    plt.xticks(x_ticks)
    plt.yticks(y_ticks)

    plt.axis('equal')

    if save_image:
        plt.savefig(image_name, dpi=dpi)

    plt.show()

# Example usage:
# Assuming you have 'true_values' and 'predicted_values' arrays containing the data.
# plot_ML_results(true_values, predicted_values, x_tick_sep=0.5, y_tick_sep=0.5, dpi=900, save_image=True, image_name="my_plot.png")

"""## Kernel Ridge Regression"""

def run_krr_leave_one_out(Dataset, sample_size, target_variable, alpha, gamma, kernel, random_state=None, simplex=True, core=True, check_overfit=True):
    Dataset_current = Dataset.sample(n=sample_size, random_state=random_state)

    X = np.array(Dataset_current["PersImg"].tolist())
    if simplex == True:
      X = np.hstack((X, np.array(Dataset_current[["Tetrahedral_count",
                                                  "Unconnected_triangles_count",
                                                  "Triangles_with_1_shared_vertex_count",
                                                  "Triangles_with_2_shared_vertices_count"]])))
    if core == True:
      X = np.hstack((X, np.array(Dataset_current[["core"]])))
    y = np.array(Dataset_current[target_variable])

    loo = LeaveOneOut()
    predicted_arr = []
    true_arr = []
    train_error_arr = []
    loocv_error_arr = []

    np.random.seed(random_state)

    start_time = time.time()

    for train_index, test_index in loo.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        regr = KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel)
        regr.fit(X_train, y_train)

        X_pred = regr.predict(X_test)

        predicted_arr.append(X_pred[0])
        true_arr.append(float(y_test[0]))

        if check_overfit == True:
          # Calculate training error
          y_train_pred = regr.predict(X_train)
          train_error = mean_squared_error(y_train, y_train_pred)
          train_error_arr.append(train_error)

          # Calculate LOOCV error
          loocv_error = mean_squared_error(y_test, X_pred)
          loocv_error_arr.append(loocv_error)
        else:
          continue

    end_time = time.time()
    duration = end_time - start_time

    return predicted_arr, true_arr, train_error_arr, loocv_error_arr, duration

# os.chdir("/content/drive/MyDrive/AuNC database/fig4_plots")

kernel = "laplacian"
target_variable = "gap"
alpha=0.000316
gamma=5e-3
seed = 42
Filename = "KRR_all3" + "_" + kernel + "_" + target_variable

if target_variable == "u298": tick_sep=150
elif target_variable == "gap": tick_sep=0.5
else: tick_sep=50

parameters = {'alpha': alpha, 'kernel': kernel, "seed": seed}

predicted_values, true_values, train_error, loocv_error, duration = run_krr_leave_one_out(Dataset=DatasetAuNC,
                                                                                          sample_size=len(DatasetAuNC),
                                                                                          target_variable=target_variable,
                                                                                          alpha=alpha, gamma=gamma, kernel=kernel,
                                                                                          random_state=seed,
                                                                                          simplex=True, core=True,
                                                                                          check_overfit=False)
analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=True, image_name=Filename+"_data.png")
plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=True, image_name=Filename+".pdf")

"""## Random Forest Regression"""

def run_rf_leave_one_out(Dataset, sample_size, target_variable, max_features, max_depth, min_sample_split, n_estimators, max_samples, random_state=None, simplex=True, core=True, check_overfit=True):
    Dataset_current = Dataset.sample(n=sample_size, random_state=random_state)

    X = np.array(Dataset_current["PersImg"].tolist())
    if simplex == True:
      X = np.hstack((X, np.array(Dataset_current[["Tetrahedral_count",
                                                  "Unconnected_triangles_count",
                                                  "Triangles_with_1_shared_vertex_count",
                                                  "Triangles_with_2_shared_vertices_count"]])))
    if core == True:
      X = np.hstack((X, np.array(Dataset_current[["core"]])))
    y = np.array(Dataset_current[target_variable])

    loo = LeaveOneOut()
    predicted_arr = []
    true_arr = []
    train_error_arr = []
    loocv_error_arr = []

    np.random.seed(random_state)

    start_time = time.time()

    for train_index, test_index in loo.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        regr = RandomForestRegressor(max_features = max_features,
                                     max_depth = max_depth,
                                     min_samples_split=min_samples_split,
                                     n_estimators = n_estimators,
                                     max_samples = max_samples)
        regr.fit(X_train, y_train)

        X_pred = regr.predict(X_test)

        predicted_arr.append(X_pred[0])
        true_arr.append(float(y_test[0]))
        if check_overfit == True:
          # Calculate training error
          y_train_pred = regr.predict(X_train)
          train_error = mean_squared_error(y_train, y_train_pred)
          train_error_arr.append(train_error)

          # Calculate LOOCV error
          loocv_error = mean_squared_error(y_test, X_pred)
          loocv_error_arr.append(loocv_error)
        else:
          continue

    end_time = time.time()
    duration = end_time - start_time

    return predicted_arr, true_arr, train_error_arr, loocv_error_arr, duration

# os.chdir("/content/drive/MyDrive/AuNC database/fig4_plots")

target_variable = "gap"
max_features = 0.3
max_depth = None
min_samples_split = 2
n_estimators = 100
max_samples = None
seed = 42
Filename = "RF_all3_" + target_variable

if target_variable == "u298": tick_sep=150
elif target_variable == "gap": tick_sep=0.5
else: tick_sep=50

parameters = {'no. max features': max_features,
              "seed": seed}

predicted_values, true_values, train_error, loocv_error, duration = run_rf_leave_one_out(DatasetAuNC, sample_size=len(DatasetAuNC), target_variable,
                                                                                          max_features, max_depth, min_samples_split, n_estimators, max_samples,
                                                                                          seed,
                                                                                          simplex=False, core=False,
                                                                                          check_overfit=False)
analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=True, image_name=Filename+"_data.png")
plot_ML_results(true_values, predicted_values, x_tick_sep=0.5, y_tick_sep=0.5, save_image=True, image_name=Filename+".pdf")