# -*- coding: utf-8 -*-
"""PI_TF_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jDAJYqT9AvHMLt71XEHIdjBXTuZMv4aB

# Generating Persistence Images and Simplical complex features to predict AuNC properties

1. This notebook focuses on generating persistence images and simplicial complex count and using them to build a Kernel Ridge Regression and Random Forest Regression model to predict the HOMO-LUMO gap, internal energies, and dipole moment of AuNCs.
2. Persistence Images were generated using the code in Townsend *et. al.* 2020 publication, publicly available at: https://gitlab.com/voglab/PersistentImages_Chemistry
3. Simplex analysis were done using GUDHI package
4. Please make sure that this notebook is in the same directory as the data folder and the scripts:
> 1. ElementsInfo.py
> 2. PersistentImageCode.py
> 3. VariancePersistCode.py
> 4. GenerateImagePI.py

# Installations
"""

!pip install gudhi
!pip install ripser
!pip install elements
!pip install qml
!pip install dscribe

import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import random

import qml
from qml.representations import *
from natsort import natsorted
from dscribe.descriptors import SOAP
from ase.io import read
from copy import deepcopy
from ElementsInfo import *
from PersistentImageCode import *
from VariancePersistCode import *
from GenerateImagePI import *

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
import time

"""# Process data and store them to pickle files

"""

# import dataframe
DatasetAuNC = pd.read_excel("./data/AuNC/DatasetAuNC.xlsx")

# Appending xyz

os.chdir('./data/AuNC/AuNC_xyz_files')

xyz_list = []
files = []
for i in tqdm(range(len(DatasetAuNC))):
    try:
        xyz_filename = DatasetAuNC["Filename"][i] + ".xyz"
        with open(xyz_filename) as f:
            lines = f.readlines()
            files.append(xyz_filename)
            xyz_list.append(lines)
    except FileNotFoundError as e:
        print(f"Error: File '{xyz_filename}' not found.")
    except Exception as e:
        print(f"Error at index {i}: {str(e)}")
        continue

DatasetAuNC["xyz"] = xyz_list

# Calculating Betti feature count for overview

Error = []
B0_list = []
B1_list = []
B2_list = []

for i in range(len(DatasetAuNC)):
  try:
    D, elements = Makexyzdistance(DatasetAuNC["xyz"][i])
    persistent_homology_features = ripser(D,distance_matrix=True, maxdim = 2)
    B0_list.append(len(persistent_homology_features['dgms'][0])-1)
    B1_list.append(len(persistent_homology_features['dgms'][1]))
    B2_list.append(len(persistent_homology_features['dgms'][2]))

  except:
    Error.append(DatasetAuNC["Number"][i])
    continue

DatasetAuNC["Betti0count"] = B0_list
DatasetAuNC["Betti1count"] = B1_list
DatasetAuNC["Betti2count"] = B2_list

print("Finished appending Betti feature count!")

"""
Calculating and appending VariancePersist arrays to pandas dataframe
VariancePersistv1 is a modified PI function with added buffer values
(0.5, 0.05) for Betti 1 and 2 features.
Except block is added due to some xyz file having values written in different notations (e.g. 1e-4)
and cannot be read as a float. (current AuNC database does not have this issue)
"""


def append_persistence_image(Dataframe):
  Error = []
  PersImgArr = []

  for idx in range(len(Dataframe)):
    try:
        persistent_image_matrix = VariancePersistv1(
                              Dataframe["xyz"][idx],
                              pixelx=resolution,
                              pixely=resolution,
                              myspread=myspread ,
                              myspecs={"maxBD": max_bound, "minBD":min_bound},
                              electroneg_addition=electroneg_addition,
                              electroneg_division=electroneg_division,
                              B1_buffer=B1_buffer,
                              B2_buffer=B2_buffer,
                              showplot = False
                              )

        PersImgArr.append(persistent_image_matrix)
    except:
        Error.append(idx)
        print("Error at index: ", idx)
        PersImgArr.append(np.zeros(resolution*resolution,))

  Dataframe["PersImg"] = PersImgArr
  print("Finished appending Persistence Images!")

#Persistence Image Parameters
resolution=100
myspread=0.3
min_bound=-0.3
max_bound=7
electroneg_addition=+0.4
electroneg_division=10
B1_buffer=0.5
B2_buffer=0.05

append_persistence_image(DatasetAuNC)

# Simplcial complex analysis
def Simplex_analyze(Core_coordinates, bond_length_limit):
  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)
  simplex_tree = rips.create_simplex_tree(max_dimension=3)
  simplex_generator = simplex_tree.get_skeleton(3)

  Tetrahedra_list = []
  Vertices_of_tetrahedra = []
  Overlapping_triangles = []
  Unconnected_triangles = []
  Triangles_with_1_shared_vertex = []
  Triangles_with_2_shared_vertices = []

  for simplex in simplex_generator:
    if simplex[1] >= bond_length_limit: continue
    if len(simplex[0]) == 4: #simplex[0]: list of vertices, simplex[1]: birth filtration
      Tetrahedra_list.append(simplex)
      for vertex in simplex[0]:
        if vertex not in Vertices_of_tetrahedra:
          Vertices_of_tetrahedra.append(vertex)

  rips= gudhi.RipsComplex(points=list(Core_coordinates), max_edge_length=10.0)
  simplex_tree = rips.create_simplex_tree(max_dimension=3)
  simplex_generator = simplex_tree.get_skeleton(3)

  for simplex in simplex_generator:
    Shared_count_arr = []

    if simplex[1] >= bond_length_limit: continue
    if len(simplex[0]) != 3: continue

    if len(Tetrahedra_list) == 0: Unconnected_triangles.append(simplex[0])
    else:
      for tetrahedra in Tetrahedra_list:
        Shared_count_arr.append(len(set(simplex[0]) & set(tetrahedra[0])))

      if max(Shared_count_arr) == 3: Overlapping_triangles.append(simplex)
      elif max(Shared_count_arr) == 2: Triangles_with_2_shared_vertices.append(simplex)
      elif max(Shared_count_arr) == 1: Triangles_with_1_shared_vertex.append(simplex)
      else: Unconnected_triangles.append(simplex)

  return {"Tetrahedral_count": len(Tetrahedra_list), "Unconnected_triangles_count": len(Unconnected_triangles),
          "Triangles_with_1_shared_vertex_count":len(Triangles_with_1_shared_vertex),
          "Triangles_with_2_shared_vertices_count": len(Triangles_with_2_shared_vertices)}

"""
Simplex analysis
"""

Tetrahedral_count_arr = []
Unconnected_triangles_count_arr = []
Triangles_with_1_shared_vertex_count_arr = []
Triangles_with_2_shared_vertices_count_arr = []
files = []

for idx in range(len(DatasetAuNC)):
  try:
    filename = DatasetAuNC["Filename"][idx] + ".xyz"
    df=pd.read_table(filename, delim_whitespace=True, names=['a','b','c','d'],skiprows = 2) # skip the first 2 lines of xyz files
    mat = df[['b','c','d']].to_numpy()
    ElementArr = df['a'].to_numpy()

    Core_coordinates = []

    for index in range(len(ElementArr)):
      if ElementArr[index] != "Au": continue
      Core_coordinates.append(mat[index])

    Dict = Simplex_analyze(Core_coordinates, bond_length_limit = 4.0)

    Tetrahedral_count_arr.append(Dict["Tetrahedral_count"])
    Unconnected_triangles_count_arr.append(Dict["Unconnected_triangles_count"])
    Triangles_with_1_shared_vertex_count_arr.append(Dict["Triangles_with_1_shared_vertex_count"])
    Triangles_with_2_shared_vertices_count_arr.append(Dict["Triangles_with_2_shared_vertices_count"])

    print("Done: ", idx)
    print(len(Tetrahedral_count_arr))


  except:
    print("error at: ", idx)
    continue

DatasetAuNC["Tetrahedral_count"] = Tetrahedral_count_arr
DatasetAuNC["Unconnected_triangles_count"] = Unconnected_triangles_count_arr
DatasetAuNC["Triangles_with_1_shared_vertex_count"] = Triangles_with_1_shared_vertex_count_arr
DatasetAuNC["Triangles_with_2_shared_vertices_count"] = Triangles_with_2_shared_vertices_count_arr

print("Finished appending Simplex count!")

"""
Calculating and appending Coulomb Matrix arrays to pandas dataframe
"""
def append_cm(dataframe, xyz_root):
  CoulMatArr = []
  for f in natsorted(os.listdir(xyz_root)):
    try:
      mol = qml.Compound(xyz=f)
      mol.generate_coulomb_matrix(size=2000, sorting="row-norm")
      A = mol.representation
      CoulMatArr.append([f,A])
    except:
      print("Error at: ", f)

  # Sample array containing the format ["Filename.xyz", property array]
  property_arrays = np.empty(len(dataframe), dtype=object)

  for filename, property_array in CoulMatArr:
      filename_without_extension = filename.split('.xyz')[0]
      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]

      if len(row_index) == 1:
          property_arrays[row_index[0]] = property_array
      else:
          print(f"Filename '{filename_without_extension}' not found in the DataFrame.")

  dataframe['Coulomb_Matrix'] = property_arrays
  print("Finished appending Coulomb Matrices!")

append_cm(DatasetAuNC, "./data/AuNC/AuNC_xyz_files")

"""
Calculating and appending SOAP arrays to pandas dataframe
"""
def append_soap(dataframe, xyz_root):
  species = ["Au", "P", "S", "Sb", "Se", "Cl", "Br", "F", "C", "N", "O", "H", "I", "Fe"]
  r_cut = 6.0
  n_max = 8
  l_max = 6

  soap = SOAP(
      species=species,
      periodic=False,
      r_cut=r_cut,
      n_max=n_max,
      l_max=l_max,
  )

  SOAPArr = []
  for f in natsorted(os.listdir(xyz_root)):
    try:
      mol = read(f)
      a = soap.create(system=mol,centers=[0])
      A = a.flatten()
      SOAPArr.append([f,A])
    except:
      print("error at: ", f)

  property_arrays = np.empty(len(dataframe), dtype=object)

  for filename, property_array in SOAPArr:
      filename_without_extension = filename.split('.xyz')[0]
      row_index = dataframe.index[dataframe['Filename'] == filename_without_extension]

      if len(row_index) == 1:
          property_arrays[row_index[0]] = property_array
      else:
          print(f"Filename '{filename_without_extension}' not found in the DataFrame.")

  dataframe['SOAP'] = property_arrays
  print("Finished appending SOAP!")

append_soap(DatasetAuNC, "./data/AuNC/AuNC_xyz_files")

"""
DatabaseTM16Jan includes the XTB calculated properties for the AuTM molecules, along with their simplex counts already calculated at 4.0 filter via the function above
"""

DatasetTM = pd.read_csv("./data/AuTM/DatasetAuTM.csv",
                  sep=',',
                  header = 0)

"""
Appending xyz texts of AuTMs to dataframe
"""
os.chdir("./data/AuTM/AuTM_xyz_files")
xyz_list = []
files = []
for i in range(len(DatasetTM)):
  print("Datapoint: ",i)
  filename = "AuTM-" + str(i+1) + ".xyz"
  with open(filename) as f:
      lines = f.readlines()
      files.append(filename)
      xyz_list.append(lines)

DatasetTM["xyz"]=xyz_list

#Persistence Image Parameters
resolution=100
myspread=0.3
min_bound=-0.3
max_bound=7
electroneg_addition=+0.4
electroneg_division=10
B1_buffer=0.5
B2_buffer=0.05

append_persistence_image(DatasetAuTM)

"""# Read from pre-calculated dataset"""

DatasetAuNC = pd.read_pickle('./data/AuNC/DatasetAuNC.pkl')
DatasetTM = pd.read_pickle('./data/AuTM/DatasetAuTM.pkl')

"""## Result plotting functions"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def analyze_regression_performance(true_values, predicted_values, parameters, duration, text_position='top_left', save_image=False, image_name="regression_performance.png"):
    # Calculate evaluation metrics
    mae = mean_absolute_error(true_values, predicted_values)
    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))
    r2 = r2_score(true_values, predicted_values)

    # Create a blank figure for text-only display
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.axis('off')  # Turn off axis for text display only

    # Determine text position
    if text_position == 'top_left':
        text_x, text_y = 0.02, 0.98
        ha, va = 'left', 'top'
    elif text_position == 'top_right':
        text_x, text_y = 0.98, 0.98
        ha, va = 'right', 'top'
    else:
        raise ValueError("Invalid text_position. Choose 'top_left' or 'top_right'.")

    # Add text with model parameters and evaluation metrics
    parameters_text = '\n'.join(f'{key}: {value}' for key, value in parameters.items())
    metrics_text = f"Mean Absolute Error (MAE): {mae:.4f}\n" \
                   f"Root Mean Squared Error (RMSE): {rmse:.4f}\n" \
                   f"R-squared (R²): {r2:.4f}\n" \
                   f"Training duration (s): {duration:.1f}\n"
    text = f"{parameters_text}\n\n{metrics_text}"
    ax.text(text_x, text_y, text, transform=ax.transAxes,
            bbox=dict(facecolor='white', edgecolor='black', alpha=0.8),
            horizontalalignment=ha, verticalalignment=va)

    if save_image:
        plt.savefig(image_name, bbox_inches='tight')

    plt.show()

def plot_ML_results(true_values, predicted_values, x_tick_sep=1.0, y_tick_sep=1.0, dpi=900, save_image=False, image_name="plot_ML_results.png"):
    """
    Plot the true values against predicted values for machine learning model evaluation.

    Parameters:
        true_values (array-like): True target values.
        predicted_values (array-like): Predicted target values.
        x_tick_sep (float): Separation between x-axis tick marks. Default is 1.0.
        y_tick_sep (float): Separation between y-axis tick marks. Default is 1.0.
        dpi (int): Dots per inch for the image resolution. Default is 900.
        save_image (bool): If True, save the image. Default is False.
        image_name (str): Name of the saved image file. Default is "plot_ML_results.png".
    """
    plt.figure(figsize=(10, 10))
    plt.scatter(true_values, predicted_values, c='teal', s=40)

    max_value = max(max(predicted_values), max(true_values))
    min_value = min(min(predicted_values), min(true_values))

    # Ensure the axes always start at 0.0
    max_value = max(max_value, 0.0)
    min_value = min(min_value, 0.0)

    plt.plot([min_value, max_value], [min_value, max_value], 'black')

    plt.xlabel('Calculated HOMO-LUMO gap (eV)', fontsize=25)
    plt.ylabel('Predicted HOMO-LUMO gap (eV)', fontsize=25)

    plt.tick_params(axis='both', labelsize=20, pad=8)
    x_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, x_tick_sep)
    y_ticks = np.arange(int(np.floor(min_value)), int(np.ceil(max_value)) + 1, y_tick_sep)

    plt.xticks(x_ticks)
    plt.yticks(y_ticks)

    plt.axis('equal')

    if save_image:
        plt.savefig(image_name, dpi=dpi)

    plt.show()

"""## Pre-training, Convolutional Neural Network

Pre-training is done using the AuTM dataset, where the structures are obtained from the tmQM dataset.

Pre training using the same PI model structure as non-pretrained CNN models. Afterwards, CNN models will replace the PI processing block with the pre-trained model, with its layer frozen except for the final 2 layers.
"""

def pretrain_cnn_model(dataset, target_variable, learning_rate, patience, batch_size, model_save_name):
    x, y = [], []

    for i in range(len(dataset)):
        x.append(np.asarray(dataset["PersImg"][i]).reshape(100, 100, 1))
        y.append(float(dataset[target_variable][i]))

    y = np.array(y)
    x = np.array(x)

    predicted_arr = []
    true_arr = []

    x_train = []
    y_train = []
    x_train_full = []
    y_train_full = []
    x_test = []
    y_test = []

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    tf.keras.backend.clear_session()

    print("Running model...")
    input2 = Input(shape=(100, 100, 1))
    conv_1 = Conv2D(16, (3, 3), activation='relu')(input2)
    maxpool_1 = MaxPooling2D((2, 2))(conv_1)
    conv_2 = Conv2D(16, (3, 3), activation='relu')(maxpool_1)
    maxpool_2 = MaxPooling2D((2, 2))(conv_2)
    flatten = Flatten()(maxpool_2)
    dense_2 = Dense(32, activation='relu', name="dense_a")(flatten)
    dense_3 = Dense(16, activation='relu', name="dense_b")(dense_2)
    dense_4 = Dense(1, name="dense_c")(dense_3)

    model = Model(inputs=input2, outputs=dense_4)
    model.summary()
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='mean_absolute_error',
        metrics=['mean_squared_error']
    )

    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)
    model_checkpoint = ModelCheckpoint(filepath=model_save_name,
                                       save_best_only=True,
                                       save_weights_only=False,
                                       monitor='val_loss',
                                       mode='min',
                                       verbose=1)
    start_time = time.time()  # Start the timer
    model.fit([x_train],
              y_train,
              batch_size=batch_size,
              epochs=500,
              verbose=1,
              validation_data=([x_test], y_test),
              callbacks=[early_stopping, model_checkpoint])
    end_time = time.time()
    duration = end_time - start_time

    print("fitting...")
    model = load_model(model_save_name)

    for i in range(len(x_test)):
        y_pred = model.predict(np.asarray([x_test[i]]))
        predicted_arr.append(y_pred)
        true_arr.append(float(y_test[i]))

    return predicted_arr, true_arr, duration

os.chdir("/content/drive/MyDrive/AuNC database/fig4_plots")

# Note: dipole moment variable for AuTM dataset is named "dipTotal" instead of "mu"
target_variable = "gap"
batch_size = 128
learning_rate=0.0005
patience = 30
model_save_name = "best_tf_gap.h5"
Filename = "TF_CNN_pretraining" + "_" + target_variable
seed = 42

parameters = {'target_variable': target_variable, 'batch_size': batch_size, "lr": learning_rate, "seed": seed}


if target_variable == "u298": tick_sep=150
elif target_variable == "gap": tick_sep=0.5
else: tick_sep=50
model_name = "transferTM_" + target_variable

predicted_values, true_values, duration = pretrain_cnn_model(DatasetTM, target_variable=target_variable,
                                          learning_rate=learning_rate, patience=patience,
                                          batch_size=batch_size, model_save_name=model_save_name)

predicted_values = [prediction[0][0] for prediction in predicted_values]

analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=False, image_name=Filename+"_data.png")
plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=False, image_name=Filename+".pdf")

"""# CNN model training"""

def create_tf_model(use_simplex=True, use_charge=True, pre_trained_model=None):
    tf.keras.backend.clear_session()

    # Define the input layers
    input_1 = Input(shape=(5,))
    input_2 = Input(shape=(100, 100, 1))
    input_3 = Input(shape=(1,))

    # Model 1 - Simplexes
    if use_simplex:
        dense_1 = Dense(5, activation='relu')(input_1)
        dense_1_extra = Dense(3, activation='relu')(dense_1)
        input_concat = [dense_1_extra]
    else:
        input_concat = []

    # Model 2 - Convolutional Neural Network
    base_model = load_model(pre_trained_model)
    for layer in base_model.layers[:-2]:
        layer.trainable = False
    new_layer = base_model(input_2)
    flatten = Flatten()(new_layer)
    input_concat.append(flatten)

    # Model 3 - Charge
    if use_charge:
        input_concat.append(input_3)

    # Concatenate input layers based on whether simplex and/or charge are used
    if len(input_concat) > 1:
        x = Concatenate()(input_concat)
    else:
        x = input_concat[0]

    x = Dense(64, activation='relu')(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_1, input_2, input_3], outputs=output)
    model.summary()

    return model


def run_cnn_leave_one_out(Dataset, sample_size, target_variable, batch_size, learning_rate, patience, model_save_name, pre_trained_model, use_simplex=True, use_charge=True, random_state=42):
    random.seed(random_state)
    np.random.seed(random_state)
    tf.random.set_seed(random_state)

    # Initialize the lists for input data and target variable
    x1, x2, x3, y = [], [], [], []

    # Append data to the lists
    for i in range(len(Dataset)):
        x1.append(np.asarray([Dataset["core"][i],
                              Dataset["Tetrahedral_count"][i],
                              Dataset["Unconnected_triangles_count"][i],
                              Dataset["Triangles_with_1_shared_vertex_count"][i],
                              Dataset["Triangles_with_2_shared_vertices_count"][i]]))

        x2.append(np.asarray(Dataset["PersImg"][i]).reshape(100, 100, 1))
        x3.append([Dataset["Charge"][i]])
        y.append(float(Dataset["gap"][i]))

    y = np.array(y)
    x1 = np.array(x1)
    x2 = np.array(x2)
    x3 = np.array(x3)

    predicted_arr = []
    true_arr = []
    MAE_arr = []
    RMSE_arr = []
    TotalAccuracy = 0
    TotalError = 0
    MSE = 0

    x1_train_full = x1.tolist()
    x2_train_full = x2.tolist()
    x3_train_full = x3.tolist()
    y_train_full = y.tolist()

    for test_index in range(len(x1)):
        print("Cycle: ", test_index)

        x1_train = deepcopy(x1_train_full)
        x2_train = deepcopy(x2_train_full)
        x3_train = deepcopy(x3_train_full)
        y_train = deepcopy(y_train_full)

        x1_test = x1[test_index]
        x2_test = x2[test_index]
        x3_test = x3[test_index]
        y_test = y[test_index]

        x1_train.pop(test_index)
        x2_train.pop(test_index)
        x3_train.pop(test_index)
        y_train.pop(test_index)

        x1_train = np.asarray(x1_train)
        x2_train = np.asarray(x2_train)
        x3_train = np.asarray(x3_train)
        y_train = np.asarray(y_train)


        # Split training data into train and validation sets
        x1_train, x1_val, x2_train, x2_val, x3_train, x3_val, y_train, y_val = train_test_split(
            x1_train, x2_train, x3_train, y_train, test_size=10, random_state=random_state)

        print("start compiling")

        model = create_tf_model(use_simplex=use_simplex, use_charge=use_charge, pre_trained_model=pre_trained_model)
        model.compile(optimizer=Adam(learning_rate=learning_rate),
                      loss='mean_absolute_error',
                      metrics=['mean_squared_error'])

        early_stopping = EarlyStopping(monitor='val_loss', patience=patience)
        model_checkpoint = ModelCheckpoint(filepath=model_save_name,
                                           save_best_only=True,
                                           save_weights_only=False,
                                           monitor='val_loss',
                                           mode='min',
                                           verbose=1)

        start_time = time.time()  # Start the timer
        print("start fitting")
        model.fit([x1_train, x2_train, x3_train],
                  y_train,
                  batch_size=batch_size,
                  epochs=500,
                  verbose=1,
                  validation_data=([x1_val, x2_val, x3_val], y_val),
                  callbacks=[early_stopping, model_checkpoint])

        end_time = time.time()  # Stop the timer
        duration = end_time - start_time

        model = load_model(model_save_name)

        y_pred = model.predict([np.asarray([x1_test]), np.asarray([x2_test]), np.asarray([x3_test])])
        predicted_arr.append(y_pred)
        true_arr.append(float(y_test))

    return predicted_arr, true_arr, duration

os.chdir("/content/drive/MyDrive/AuNC database/fig4_plots")

pre_trained_model = "best_tf_gap.h5" #set pre-trained model variable

target_variable = "gap"
batch_size = 24
learning_rate=0.001
patience = 40
Filename = "CNN" + "_" + target_variable
seed = 42
model_save_name = "best_cnn_gap_tf.h5"
if target_variable == "u298": tick_sep=150
elif target_variable == "gap": tick_sep=0.5
else: tick_sep=50

parameters = {'target_variable': target_variable, 'batch_size': batch_size, "lr": learning_rate, "seed": seed}

predicted_values, true_values, duration = run_cnn_leave_one_out(Dataset=DatasetAuNC,
                                                        sample_size=len(DatasetAuNC),
                                                        target_variable=target_variable,
                                                        batch_size=batch_size, learning_rate=learning_rate, patience=patience,
                                                        model_save_name=model_save_name, pre_trained_model = pre_trained_model,
                                                        random_state=seed,
                                                        use_simplex=True, use_charge=True,
                                                        )
analyze_regression_performance(true_values, predicted_values, parameters, duration, save_image=False, image_name=Filename+"_data.png")
plot_ML_results(true_values, predicted_values, x_tick_sep=tick_sep, y_tick_sep=tick_sep, save_image=False, image_name=Filename+".pdf")